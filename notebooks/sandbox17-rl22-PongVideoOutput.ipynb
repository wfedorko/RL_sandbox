{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../baselines')\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id=\"PongNoFrameskip-v4\"\n",
    "env=make_atari(env_id)\n",
    "env=wrap_deepmind(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, episode,step, info=\"\"):\n",
    "    plt.figure(99999,figsize=[8,6])\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"episode: {} step: {} \".format(episode,step))\n",
    "    #plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device=torch.device(\"cuda:4\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PARAMS ######\n",
    "learning_rate = 0.0001\n",
    "num_episodes = 10000\n",
    "gamma=0.99\n",
    "#gamma=0.85\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 10000\n",
    "\n",
    "report_interval=10\n",
    "\n",
    "score_to_solve = 18.0\n",
    "\n",
    "hidden_layer_size=512\n",
    "\n",
    "replay_memory_size=100000\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "update_target_frequency = 5000\n",
    "\n",
    "clip_error=True\n",
    "normalize_image=True\n",
    "\n",
    "double_dqn=True\n",
    "\n",
    "file2save = 'pong_save.pth'\n",
    "\n",
    "save_model_frequency=10000\n",
    "\n",
    "resume_previous_training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.wrappers.Monitor(env, '../mp4/sandbox10',video_callable=lambda episode_id: True,force=True)\n",
    "env = gym.wrappers.Monitor(env, '../mp4/PongVideos',video_callable=lambda episode_id: episode_id%20==0, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs=env.observation_space.shape[0]\n",
    "number_of_outputs=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Discrete.contains of Discrete(6)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    return torch.load(file2save)\n",
    "\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), file2save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = frame.transpose((2,0,1))\n",
    "    frame = torch.from_numpy(frame)\n",
    "    frame = frame.to(device, dtype=torch.float32)\n",
    "    frame = frame.unsqueeze(0)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "    plt.figure(figsize=[12,5])\n",
    "    plt.title(\"Score at end of episode\")\n",
    "    plt.plot(reward_total[:i_episode],color='red')\n",
    "    plt.savefig(\"Pong-results.png\")\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19.]])\n"
     ]
    }
   ],
   "source": [
    "o=torch.arange(20,dtype=torch.float32).view(-1,5)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity=capacity\n",
    "        self.memory=[]\n",
    "        self.position=0\n",
    "        \n",
    "    \n",
    "    def push(self, state,\n",
    "             action, new_state,\n",
    "             reward, done):\n",
    "        \n",
    "            transition=(state,action,new_state,reward,done)\n",
    "            \n",
    "            if self.position>=len(self.memory):\n",
    "                self.memory.append(transition)\n",
    "            else:\n",
    "                self.memory[self.position]=transition\n",
    "                \n",
    "            self.position=(self.position+1)%self.capacity\n",
    "        \n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkDueling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkDueling, self).__init__()\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8 ,stride=4)\n",
    "        self.conv2=nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4 ,stride=2)\n",
    "        self.conv3=nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3 ,stride=1)\n",
    "        \n",
    "        self.advantage1 = nn.Linear(7*7*64, hidden_layer_size)\n",
    "        self.advantage2 = nn.Linear(hidden_layer_size, number_of_outputs)\n",
    "        \n",
    "        self.value1 = nn.Linear(7*7*64,hidden_layer_size)\n",
    "        self.value2 = nn.Linear(hidden_layer_size,1)\n",
    "        \n",
    "        #self.activation=nn.Tanh()\n",
    "        self.activation=nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print('x shape {} and value:'.format(x.shape))\n",
    "        #print(x.detach().cpu())\n",
    "        \n",
    "        if normalize_image:\n",
    "            x=x/255.0\n",
    "        \n",
    "        output_conv = self.conv1(x)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv2(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv3(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        \n",
    "        output_conv = output_conv.view(output_conv.shape[0],-1)\n",
    "        \n",
    "        output_advantage=self.advantage1(output_conv)\n",
    "        output_advantage=self.activation(output_advantage)\n",
    "        output_advantage=self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value=self.value1(output_conv)\n",
    "        output_value=self.activation(output_value)\n",
    "        output_value=self.value2(output_value)\n",
    "        \n",
    "        #print('output_advantage shape {} and value:'.format(output_advantage.shape))\n",
    "        #print(output_advantage.detach().cpu())\n",
    "        \n",
    "        #print('output_value shape {} and value:'.format(output_value.shape))\n",
    "        #print(output_value.detach().cpu())\n",
    "        \n",
    "        #print('output_advantage.mean shape {} and value:'.format(output_advantage.mean(dim=1,keepdim=True).shape))\n",
    "        #print(output_advantage.mean(dim=1,keepdim=True).detach().cpu())\n",
    "        \n",
    "        output_final=output_value+output_advantage-output_advantage.mean(dim=1,keepdim=True)\n",
    "        \n",
    "        \n",
    "        return output_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer_size)\n",
    "        self.linear2 = nn.Linear(hidden_layer_size,number_of_outputs)\n",
    "        \n",
    "        self.activation=nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_Agent():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.nn = NeuralNetworkDueling().to(device)\n",
    "        self.target_nn = NeuralNetworkDueling().to(device)\n",
    "        \n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.number_of_frames = 0\n",
    "        \n",
    "        if resume_previous_training and os.path.exists(file2save):\n",
    "            print('loading previous model')\n",
    "            self.nn.load_state_dict(load_model())\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy=torch.rand(1).item()\n",
    "        \n",
    "        if random_for_egreedy>epsilon:\n",
    "            self.nn.eval()\n",
    "            with torch.no_grad():\n",
    "                state=preprocess_frame(state)\n",
    "                #state=torch.Tensor(state).to(device)\n",
    "                predicted_value_from_nn=self.nn(state).squeeze()\n",
    "                #print('predicted value from nn:')\n",
    "                #print(predicted_value_from_nn)\n",
    "                action=torch.argmax(predicted_value_from_nn).item()\n",
    "                #print('action: {}'.format(action))\n",
    "        else:\n",
    "            action=env.action_space.sample()\n",
    "                \n",
    "                \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if len(memory)<batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state=[preprocess_frame(frame) for frame in state] \n",
    "        state=torch.cat(state)\n",
    "        \n",
    "        new_state=[preprocess_frame(frame) for frame in new_state] \n",
    "        new_state=torch.cat(new_state)\n",
    "        \n",
    "        #print('state batch shape {}'.format(state.shape))\n",
    "        #print(state)\n",
    "        \n",
    "        #state=torch.Tensor(state).to(device)\n",
    "        #new_state=torch.Tensor(new_state).to(device)\n",
    "        \n",
    "        \n",
    "        reward=torch.Tensor(reward).to(device)\n",
    "        \n",
    "        #the view call below is to transform into column vector\n",
    "        #so that it can be used in the gather call\n",
    "        #i.e. we will use it to pick out from the computed value\n",
    "        #tensor only values indexed by selected action\n",
    "        action=(torch.Tensor(action).view(-1,1).long()).to(device)\n",
    "        #print('action: ')\n",
    "        #print(action)\n",
    "        #print('contiguous?', action.is_contiguous())\n",
    "        done=torch.Tensor(done).to(device)\n",
    "        \n",
    "        #print('shape of: state, new state, reward, action, done:')\n",
    "        #print(state.shape)\n",
    "        #print(new_state.shape)\n",
    "        #print(reward.shape)\n",
    "        #print(action.shape)\n",
    "        #print(done.shape)\n",
    "        \n",
    "        \n",
    "        self.nn.eval()\n",
    "        self.target_nn.eval()\n",
    "            \n",
    "            \n",
    "        if double_dqn:\n",
    "            #print('in double DQN')\n",
    "            new_state_values_from_nn=self.nn(new_state).detach()\n",
    "            #print('new_state_values_from_nn shape {} and value:'.format(new_state_values_from_nn.shape))\n",
    "            #print(new_state_values_from_nn)\n",
    "            max_new_state_indexes=torch.max(new_state_values_from_nn,dim=1)[1].view(-1,1)\n",
    "            #print('max_new_state_indexes shape {} and value:'.format(max_new_state_indexes.shape))\n",
    "            #print(max_new_state_indexes)\n",
    "            new_state_values=self.target_nn(new_state).detach()\n",
    "            #print('new_state_values shape {} and value:'.format(new_state_values.shape))\n",
    "            #print(new_state_values)\n",
    "            max_new_state_values=torch.gather(new_state_values,1,max_new_state_indexes).squeeze()\n",
    "            #print('max_new_state_values shape {} and value:'.format(max_new_state_values.shape))\n",
    "            #print(max_new_state_values)\n",
    "        else:\n",
    "            #print('in regular DQN')\n",
    "            new_state_values=self.target_nn(new_state).detach()\n",
    "            #print('new_state_values shape {} and value'.format(new_state_values.shape))\n",
    "            #print(new_state_values)\n",
    "        \n",
    "            max_new_state_values=torch.max(new_state_values,dim=1)[0]\n",
    "            #print('max_new_state_values shape {} and value'.format(max_new_state_values.shape))\n",
    "            #print(max_new_state_values)\n",
    "            \n",
    "        target_value=(reward + (1-done)*gamma*max_new_state_values).view(-1,1)\n",
    "        \n",
    "        #print('shape of: target_value')\n",
    "        #print(target_value.shape)\n",
    "        self.nn.train()\n",
    "        \n",
    "        #this will select only the values of the desired actions\n",
    "        predicted_value=torch.gather(self.nn(state),1,action)\n",
    "        #print('shape of: predicted_value')\n",
    "        #print(predicted_value.shape)\n",
    "        \n",
    "        \n",
    "        loss=self.loss_function(predicted_value,target_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.clamp_(-1.0,1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.number_of_frames % update_target_frequency == 0:\n",
    "            #print(\"***********************\")\n",
    "            #print(\"UPDATING TARGET NETWORK\")\n",
    "            #print(\"update counter: {}\".format(self.update_target_counter))\n",
    "            #print(\"***********************\")\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "            \n",
    "        if self.number_of_frames % save_model_frequency ==0:\n",
    "            save_model(self.nn)\n",
    "        \n",
    "        self.number_of_frames+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ExperienceReplay(replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_agent=QNet_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value=23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  10 **** \n",
      "Recent average reward: -20.6\n",
      "Average over all episodes so far: -20.6\n",
      "epsilon: 0.35063838590878144\n",
      "**** Episode  20 **** \n",
      "Recent average reward: -20.4\n",
      "Average over all episodes so far: -20.5\n",
      "epsilon: 0.15364162218858307\n",
      "**** Episode  30 **** \n",
      "Recent average reward: -20.8\n",
      "Average over all episodes so far: -20.6\n",
      "epsilon: 0.07116783397633487\n",
      "**** Episode  40 **** \n",
      "Recent average reward: -20.4\n",
      "Average over all episodes so far: -20.55\n",
      "epsilon: 0.036674857703615525\n",
      "**** Episode  50 **** \n",
      "Recent average reward: -20.1\n",
      "Average over all episodes so far: -20.46\n",
      "epsilon: 0.02042098396955367\n",
      "**** Episode  60 **** \n",
      "Recent average reward: -19.7\n",
      "Average over all episodes so far: -20.333333333333332\n",
      "epsilon: 0.013597076609767493\n",
      "**** Episode  70 **** \n",
      "Recent average reward: -19.5\n",
      "Average over all episodes so far: -20.214285714285715\n",
      "epsilon: 0.011188772738447638\n",
      "**** Episode  80 **** \n",
      "Recent average reward: -18.5\n",
      "Average over all episodes so far: -20.0\n",
      "epsilon: 0.010273794203401578\n",
      "**** Episode  90 **** \n",
      "Recent average reward: -17.6\n",
      "Average over all episodes so far: -19.733333333333334\n",
      "epsilon: 0.010066358823871403\n",
      "**** Episode  100 **** \n",
      "Recent average reward: -18.2\n",
      "Average over all episodes so far: -19.58\n",
      "epsilon: 0.010008224164753006\n",
      "**** Episode  110 **** \n",
      "Recent average reward: -20.1\n",
      "Reward over last 100: -19.53\n",
      "Average over all episodes so far: -19.62727272727273\n",
      "epsilon: 0.010001093492542983\n",
      "**** Episode  120 **** \n",
      "Recent average reward: -17.2\n",
      "Reward over last 100: -19.21\n",
      "Average over all episodes so far: -19.425\n",
      "epsilon: 0.010000109610373712\n",
      "**** Episode  130 **** \n",
      "Recent average reward: -18.1\n",
      "Reward over last 100: -18.94\n",
      "Average over all episodes so far: -19.323076923076922\n",
      "epsilon: 0.010000012762765938\n",
      "**** Episode  140 **** \n",
      "Recent average reward: -17.4\n",
      "Reward over last 100: -18.64\n",
      "Average over all episodes so far: -19.185714285714287\n",
      "epsilon: 0.01000000109114982\n",
      "**** Episode  150 **** \n",
      "Recent average reward: -16.2\n",
      "Reward over last 100: -18.25\n",
      "Average over all episodes so far: -18.986666666666668\n",
      "epsilon: 0.010000000076331622\n",
      "**** Episode  160 **** \n",
      "Recent average reward: -16.0\n",
      "Reward over last 100: -17.88\n",
      "Average over all episodes so far: -18.8\n",
      "epsilon: 0.0100000000083302\n",
      "**** Episode  170 **** \n",
      "Recent average reward: -14.6\n",
      "Reward over last 100: -17.39\n",
      "Average over all episodes so far: -18.55294117647059\n",
      "epsilon: 0.010000000000704116\n",
      "**** Episode  180 **** \n",
      "Recent average reward: -16.6\n",
      "Reward over last 100: -17.2\n",
      "Average over all episodes so far: -18.444444444444443\n",
      "epsilon: 0.010000000000055916\n",
      "**** Episode  190 **** \n",
      "Recent average reward: -17.1\n",
      "Reward over last 100: -17.15\n",
      "Average over all episodes so far: -18.373684210526317\n",
      "epsilon: 0.010000000000005812\n",
      "**** Episode  200 **** \n",
      "Recent average reward: -15.3\n",
      "Reward over last 100: -16.86\n",
      "Average over all episodes so far: -18.22\n",
      "epsilon: 0.01000000000000044\n",
      "**** Episode  210 **** \n",
      "Recent average reward: -15.0\n",
      "Reward over last 100: -16.35\n",
      "Average over all episodes so far: -18.066666666666666\n",
      "epsilon: 0.010000000000000037\n",
      "**** Episode  220 **** \n",
      "Recent average reward: -14.3\n",
      "Reward over last 100: -16.06\n",
      "Average over all episodes so far: -17.895454545454545\n",
      "epsilon: 0.010000000000000004\n",
      "**** Episode  230 **** \n",
      "Recent average reward: -13.5\n",
      "Reward over last 100: -15.6\n",
      "Average over all episodes so far: -17.704347826086956\n",
      "epsilon: 0.01\n",
      "**** Episode  240 **** \n",
      "Recent average reward: -13.5\n",
      "Reward over last 100: -15.21\n",
      "Average over all episodes so far: -17.529166666666665\n",
      "epsilon: 0.01\n",
      "**** Episode  250 **** \n",
      "Recent average reward: -13.0\n",
      "Reward over last 100: -14.89\n",
      "Average over all episodes so far: -17.348\n",
      "epsilon: 0.01\n",
      "**** Episode  260 **** \n",
      "Recent average reward: -14.2\n",
      "Reward over last 100: -14.71\n",
      "Average over all episodes so far: -17.226923076923075\n",
      "epsilon: 0.01\n",
      "**** Episode  270 **** \n",
      "Recent average reward: -11.6\n",
      "Reward over last 100: -14.41\n",
      "Average over all episodes so far: -17.01851851851852\n",
      "epsilon: 0.01\n",
      "**** Episode  280 **** \n",
      "Recent average reward: -13.3\n",
      "Reward over last 100: -14.08\n",
      "Average over all episodes so far: -16.885714285714286\n",
      "epsilon: 0.01\n",
      "**** Episode  290 **** \n",
      "Recent average reward: -10.9\n",
      "Reward over last 100: -13.46\n",
      "Average over all episodes so far: -16.679310344827588\n",
      "epsilon: 0.01\n",
      "**** Episode  300 **** \n",
      "Recent average reward: -9.9\n",
      "Reward over last 100: -12.92\n",
      "Average over all episodes so far: -16.453333333333333\n",
      "epsilon: 0.01\n",
      "**** Episode  310 **** \n",
      "Recent average reward: -6.1\n",
      "Reward over last 100: -12.03\n",
      "Average over all episodes so far: -16.11935483870968\n",
      "epsilon: 0.01\n",
      "**** Episode  320 **** \n",
      "Recent average reward: -9.2\n",
      "Reward over last 100: -11.52\n",
      "Average over all episodes so far: -15.903125\n",
      "epsilon: 0.01\n",
      "**** Episode  330 **** \n",
      "Recent average reward: -3.8\n",
      "Reward over last 100: -10.55\n",
      "Average over all episodes so far: -15.536363636363637\n",
      "epsilon: 0.01\n",
      "**** Episode  340 **** \n",
      "Recent average reward: 3.5\n",
      "Reward over last 100: -8.85\n",
      "Average over all episodes so far: -14.976470588235294\n",
      "epsilon: 0.01\n",
      "**** Episode  350 **** \n",
      "Recent average reward: 7.1\n",
      "Reward over last 100: -6.84\n",
      "Average over all episodes so far: -14.345714285714285\n",
      "epsilon: 0.01\n",
      "**** Episode  360 **** \n",
      "Recent average reward: 8.9\n",
      "Reward over last 100: -4.53\n",
      "Average over all episodes so far: -13.7\n",
      "epsilon: 0.01\n",
      "**** Episode  370 **** \n",
      "Recent average reward: 10.3\n",
      "Reward over last 100: -2.34\n",
      "Average over all episodes so far: -13.051351351351352\n",
      "epsilon: 0.01\n",
      "**** Episode  380 **** \n",
      "Recent average reward: 10.1\n",
      "Reward over last 100: 0.0\n",
      "Average over all episodes so far: -12.442105263157895\n",
      "epsilon: 0.01\n",
      "**** Episode  390 **** \n",
      "Recent average reward: 13.0\n",
      "Reward over last 100: 2.39\n",
      "Average over all episodes so far: -11.78974358974359\n",
      "epsilon: 0.01\n",
      "**** Episode  400 **** \n",
      "Recent average reward: 18.3\n",
      "Reward over last 100: 5.21\n",
      "Average over all episodes so far: -11.0375\n",
      "epsilon: 0.01\n",
      "**** Episode  410 **** \n",
      "Recent average reward: 16.7\n",
      "Reward over last 100: 7.49\n",
      "Average over all episodes so far: -10.360975609756098\n",
      "epsilon: 0.01\n",
      "**** Episode  420 **** \n",
      "Recent average reward: 18.8\n",
      "Reward over last 100: 10.29\n",
      "Average over all episodes so far: -9.666666666666666\n",
      "epsilon: 0.01\n",
      "**** Episode  430 **** \n",
      "Recent average reward: 17.9\n",
      "Reward over last 100: 12.46\n",
      "Average over all episodes so far: -9.025581395348837\n",
      "epsilon: 0.01\n",
      "**** Episode  440 **** \n",
      "Recent average reward: 16.5\n",
      "Reward over last 100: 13.76\n",
      "Average over all episodes so far: -8.445454545454545\n",
      "epsilon: 0.01\n",
      "**** Episode  450 **** \n",
      "Recent average reward: 17.1\n",
      "Reward over last 100: 14.76\n",
      "Average over all episodes so far: -7.877777777777778\n",
      "epsilon: 0.01\n",
      "**** Episode  470 **** \n",
      "Recent average reward: 17.5\n",
      "Reward over last 100: 16.44\n",
      "Average over all episodes so far: -6.776595744680851\n",
      "epsilon: 0.01\n",
      "**** Episode  480 **** \n",
      "Recent average reward: 18.2\n",
      "Reward over last 100: 17.25\n",
      "Average over all episodes so far: -6.25625\n",
      "epsilon: 0.01\n",
      "**** Episode  490 **** \n",
      "Recent average reward: 17.2\n",
      "Reward over last 100: 17.67\n",
      "Average over all episodes so far: -5.777551020408163\n",
      "epsilon: 0.01\n",
      "**** Episode  500 **** \n",
      "Recent average reward: 18.2\n",
      "Reward over last 100: 17.66\n",
      "Average over all episodes so far: -5.298\n",
      "epsilon: 0.01\n",
      "**** Episode  510 **** \n",
      "Recent average reward: 17.6\n",
      "Reward over last 100: 17.75\n",
      "Average over all episodes so far: -4.849019607843137\n",
      "epsilon: 0.01\n",
      "**** Episode  520 **** \n",
      "Recent average reward: 16.5\n",
      "Reward over last 100: 17.52\n",
      "Average over all episodes so far: -4.438461538461539\n",
      "epsilon: 0.01\n",
      "**** Episode  530 **** \n",
      "Recent average reward: 17.6\n",
      "Reward over last 100: 17.49\n",
      "Average over all episodes so far: -4.022641509433963\n",
      "epsilon: 0.01\n",
      "**** Episode  540 **** \n",
      "Recent average reward: 17.8\n",
      "Reward over last 100: 17.62\n",
      "Average over all episodes so far: -3.6185185185185187\n",
      "epsilon: 0.01\n",
      "**** Episode  550 **** \n",
      "Recent average reward: 18.3\n",
      "Reward over last 100: 17.74\n",
      "Average over all episodes so far: -3.22\n",
      "epsilon: 0.01\n",
      "**** Episode  560 **** \n",
      "Recent average reward: 16.0\n",
      "Reward over last 100: 17.49\n",
      "Average over all episodes so far: -2.8767857142857145\n",
      "epsilon: 0.01\n",
      "**** Episode  570 **** \n",
      "Recent average reward: 16.9\n",
      "Reward over last 100: 17.43\n",
      "Average over all episodes so far: -2.5298245614035086\n",
      "epsilon: 0.01\n",
      "**** Episode  580 **** \n",
      "Recent average reward: 17.6\n",
      "Reward over last 100: 17.37\n",
      "Average over all episodes so far: -2.182758620689655\n",
      "epsilon: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  590 **** \n",
      "Recent average reward: 16.8\n",
      "Reward over last 100: 17.33\n",
      "Average over all episodes so far: -1.8610169491525423\n",
      "epsilon: 0.01\n",
      "**** Episode  600 **** \n",
      "Recent average reward: 18.4\n",
      "Reward over last 100: 17.35\n",
      "Average over all episodes so far: -1.5233333333333334\n",
      "epsilon: 0.01\n",
      "**** Episode  610 **** \n",
      "Recent average reward: 18.3\n",
      "Reward over last 100: 17.42\n",
      "Average over all episodes so far: -1.198360655737705\n",
      "epsilon: 0.01\n",
      "**** Episode  620 **** \n",
      "Recent average reward: 18.4\n",
      "Reward over last 100: 17.61\n",
      "Average over all episodes so far: -0.882258064516129\n",
      "epsilon: 0.01\n",
      "**** Episode  630 **** \n",
      "Recent average reward: 17.4\n",
      "Reward over last 100: 17.59\n",
      "Average over all episodes so far: -0.5920634920634921\n",
      "epsilon: 0.01\n",
      "**** Episode  640 **** \n",
      "Recent average reward: 17.2\n",
      "Reward over last 100: 17.53\n",
      "Average over all episodes so far: -0.3140625\n",
      "epsilon: 0.01\n",
      "**** Episode  650 **** \n",
      "Recent average reward: 18.8\n",
      "Reward over last 100: 17.58\n",
      "Average over all episodes so far: -0.02\n",
      "epsilon: 0.01\n",
      "**** Episode  660 **** \n",
      "Recent average reward: 19.1\n",
      "Reward over last 100: 17.89\n",
      "Average over all episodes so far: 0.2696969696969697\n",
      "epsilon: 0.01\n",
      "SOLVED! After 668 episodes \n",
      "**** Episode  670 **** \n",
      "Recent average reward: 18.4\n",
      "Reward over last 100: 18.04\n",
      "Average over all episodes so far: 0.5402985074626866\n",
      "epsilon: 0.01\n",
      "**** Episode  680 **** \n",
      "Recent average reward: 19.6\n",
      "Reward over last 100: 18.24\n",
      "Average over all episodes so far: 0.8205882352941176\n",
      "epsilon: 0.01\n",
      "**** Episode  690 **** \n",
      "Recent average reward: 18.2\n",
      "Reward over last 100: 18.38\n",
      "Average over all episodes so far: 1.0724637681159421\n",
      "epsilon: 0.01\n",
      "**** Episode  700 **** \n",
      "Recent average reward: 18.4\n",
      "Reward over last 100: 18.38\n",
      "Average over all episodes so far: 1.32\n",
      "epsilon: 0.01\n",
      "**** Episode  710 **** \n",
      "Recent average reward: 18.9\n",
      "Reward over last 100: 18.44\n",
      "Average over all episodes so far: 1.5676056338028168\n",
      "epsilon: 0.01\n",
      "**** Episode  720 **** \n",
      "Recent average reward: 19.1\n",
      "Reward over last 100: 18.51\n",
      "Average over all episodes so far: 1.8111111111111111\n",
      "epsilon: 0.01\n",
      "**** Episode  730 **** \n",
      "Recent average reward: 18.9\n",
      "Reward over last 100: 18.66\n",
      "Average over all episodes so far: 2.0452054794520547\n",
      "epsilon: 0.01\n",
      "**** Episode  740 **** \n",
      "Recent average reward: 18.7\n",
      "Reward over last 100: 18.81\n",
      "Average over all episodes so far: 2.27027027027027\n",
      "epsilon: 0.01\n",
      "**** Episode  750 **** \n",
      "Recent average reward: 18.9\n",
      "Reward over last 100: 18.82\n",
      "Average over all episodes so far: 2.492\n",
      "epsilon: 0.01\n",
      "**** Episode  760 **** \n",
      "Recent average reward: 18.2\n",
      "Reward over last 100: 18.73\n",
      "Average over all episodes so far: 2.6986842105263156\n",
      "epsilon: 0.01\n",
      "**** Episode  770 **** \n",
      "Recent average reward: 19.4\n",
      "Reward over last 100: 18.83\n",
      "Average over all episodes so far: 2.9155844155844157\n",
      "epsilon: 0.01\n",
      "**** Episode  780 **** \n",
      "Recent average reward: 18.6\n",
      "Reward over last 100: 18.73\n",
      "Average over all episodes so far: 3.1166666666666667\n",
      "epsilon: 0.01\n",
      "**** Episode  790 **** \n",
      "Recent average reward: 19.2\n",
      "Reward over last 100: 18.83\n",
      "Average over all episodes so far: 3.320253164556962\n",
      "epsilon: 0.01\n",
      "**** Episode  800 **** \n",
      "Recent average reward: 19.0\n",
      "Reward over last 100: 18.89\n",
      "Average over all episodes so far: 3.51625\n",
      "epsilon: 0.01\n",
      "**** Episode  810 **** \n",
      "Recent average reward: 19.1\n",
      "Reward over last 100: 18.91\n",
      "Average over all episodes so far: 3.708641975308642\n",
      "epsilon: 0.01\n",
      "**** Episode  820 **** \n",
      "Recent average reward: 18.4\n",
      "Reward over last 100: 18.84\n",
      "Average over all episodes so far: 3.8878048780487804\n",
      "epsilon: 0.01\n",
      "**** Episode  830 **** \n",
      "Recent average reward: 19.2\n",
      "Reward over last 100: 18.87\n",
      "Average over all episodes so far: 4.072289156626506\n",
      "epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "steps_total=np.full([num_episodes],-999,dtype=np.int32)\n",
    "reward_total=np.full([num_episodes],-999,dtype=np.int32)\n",
    "\n",
    "frames_total=0\n",
    "\n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    #for step in range(100):\n",
    "    step=0\n",
    "    reward_total[i_episode]=0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        step+=1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon=calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action=env.action_space.sample()\n",
    "        action=qnet_agent.select_action(state,epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        memory.push(state, action, new_state,\n",
    "                     reward, done)\n",
    "        \n",
    "        reward_total[i_episode]+=reward\n",
    "        \n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state=new_state\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            steps_total[i_episode]=step\n",
    "            \n",
    "            if i_episode>100:\n",
    "                mean_reward_100 = np.sum(reward_total[i_episode-100:i_episode])/100\n",
    "                \n",
    "            \n",
    "                if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                    print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                    solved_after = i_episode\n",
    "                    solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0 and i_episode>1):\n",
    "                \n",
    "                plot_results()\n",
    "                \n",
    "                print(\"**** Episode  {} **** \".format(i_episode))\n",
    "                recent_avg_reward=np.average(reward_total[i_episode-report_interval:i_episode])\n",
    "                print(\"Recent average reward: {}\".format(recent_avg_reward))\n",
    "                if i_episode>100:\n",
    "                    print(\"Reward over last 100: {}\".format(mean_reward_100))\n",
    "                full_avg_so_far=np.average(reward_total[:i_episode])\n",
    "                print(\"Average over all episodes so far: {}\".format(full_avg_so_far))\n",
    "                print(\"epsilon: {}\".format(epsilon))\n",
    "            \n",
    "                #print(\"Episode {} finished after: {}\".format(i_episode,step))\n",
    "            break\n",
    "            \n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(2,figsize=[12,5])\n",
    "plt.title(\"Score at end of episode\")\n",
    "plt.plot(reward_total[:i_episode])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of steps: {}\". format(np.average(steps_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of steps: {}\". format(np.average(steps_total)))\n",
    "print(\"Average number of steps in last 100 episodes: {}\". format(np.average(steps_total[-100:])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(1,figsize=[12,5])\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total,alpha=0.6, color='green')\n",
    "#plt.plot(rewards_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2,figsize=[12,5])\n",
    "plt.title(\"Steps to finish episode\")\n",
    "plt.plot(:i_episode)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
