test_tensor_ExpRep-gpu
commit 3de6042..c08b65d 

Timer unit: 1e-06 s

Total time: 17424.2 s
File: ../agents/QNet_Agent.py
Function: optimize at line 70

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    70                                               def optimize(self):
    71                                                   
    72   1907375    9904284.0      5.2      0.1          if len(self.memory)<self.config.batch_size:
    73        31         33.0      1.1      0.0              return
    74                                                   
    75   1907344  345195785.0    181.0      2.0          state, action, new_state, reward, done = self.memory.sample(self.config.batch_size)
    76                                                   
    77   1907344   10020684.0      5.3      0.1          state=state.to(self.device)
    78   1907344    5215109.0      2.7      0.0          new_state=new_state.to(self.device)
    79   1907344    9215613.0      4.8      0.1          action=action.to(self.device, dtype=torch.long)
    80   1907344    5044158.0      2.6      0.0          reward=reward.to(self.device)
    81   1907344    4653282.0      2.4      0.0          done=done.to(self.device)
    82                                                   
    83                                                   #state=[preprocess_frame(frame, self.device) for frame in state] 
    84                                                   #state=torch.cat(state)
    85                                                   
    86                                                   #new_state=[preprocess_frame(frame, self.device) for frame in new_state] 
    87                                                   #new_state=torch.cat(new_state)
    88                                                   
    89                                                   #print('state batch shape {}'.format(state.shape))
    90                                                   #print(state)
    91                                                   
    92                                                   #state=torch.Tensor(state).to(device)
    93                                                   #new_state=torch.Tensor(new_state).to(device)
    94                                                   
    95                                                   
    96                                                   #reward=torch.Tensor(reward).to(self.device)
    97                                                   
    98                                                   #the view call below is to transform into column vector
    99                                                   #so that it can be used in the gather call
   100                                                   #i.e. we will use it to pick out from the computed value
   101                                                   #tensor only values indexed by selected action
   102                                                   #action=(torch.Tensor(action).view(-1,1).long()).to(self.device)
   103                                                   #print('action: ')
   104                                                   #print(action)
   105                                                   #print('contiguous?', action.is_contiguous())
   106                                                   #done=torch.Tensor(done).to(self.device)
   107                                                   
   108                                                   #print('shape of: state, new state, reward, action, done:')
   109                                                   #print(state.shape)
   110                                                   #print(new_state.shape)
   111                                                   #print(reward.shape)
   112                                                   #print(action.shape)
   113                                                   #print(done.shape)
   114                                                   
   115                                                   
   116   1907344  280614890.0    147.1      1.6          self.nn.eval()
   117   1907344  234399853.0    122.9      1.3          self.target_nn.eval()
   118                                                       
   119   1907344   12506173.0      6.6      0.1          with torch.no_grad():
   120   1907344    3408293.0      1.8      0.0              if self.config.double_dqn:
   121                                                           #print('in double DQN')
   122   1907344 2195471951.0   1151.1     12.6                  new_state_values_from_nn=self.nn(new_state).detach()
   123                                                           #print('new_state_values_from_nn shape {} and value:'.format(new_state_values_from_nn.shape))
   124                                                           #print(new_state_values_from_nn)
   125   1907344   82205318.0     43.1      0.5                  max_new_state_indexes=torch.max(new_state_values_from_nn,dim=1)[1].view(-1,1)
   126                                                           #print('max_new_state_indexes shape {} and value:'.format(max_new_state_indexes.shape))
   127                                                           #print(max_new_state_indexes)
   128   1907344 2089571360.0   1095.5     12.0                  new_state_values=self.target_nn(new_state).detach()
   129                                                           #print('new_state_values shape {} and value:'.format(new_state_values.shape))
   130                                                           #print(new_state_values)
   131   1907344   66920375.0     35.1      0.4                  max_new_state_values=torch.gather(new_state_values,1,max_new_state_indexes).squeeze()
   132                                                           #print('max_new_state_values shape {} and value:'.format(max_new_state_values.shape))
   133                                                           #print(max_new_state_values)
   134                                                       else:
   135                                                           #print('in regular DQN')
   136                                                           new_state_values=self.target_nn(new_state).detach()
   137                                                           #print('new_state_values shape {} and value'.format(new_state_values.shape))
   138                                                           #print(new_state_values)
   139                                                       
   140                                                           max_new_state_values=torch.max(new_state_values,dim=1)[0]
   141                                                           #print('max_new_state_values shape {} and value'.format(max_new_state_values.shape))
   142                                                           #print(max_new_state_values)
   143                                                           
   144   1907344  205679609.0    107.8      1.2              target_value=(reward + (1-done)*self.config.gamma*max_new_state_values).view(-1,1)
   145                                                   
   146                                                   #end no grad
   147                                                   
   148                                                   #print('shape of: target_value')
   149                                                   #print(target_value.shape)
   150   1907344  271416807.0    142.3      1.6          self.nn.train()
   151                                                   
   152                                                   #this will select only the values of the desired actions
   153   1907344 2477346609.0   1298.8     14.2          predicted_value=torch.gather(self.nn(state),1,action)
   154                                                   #print('shape of: predicted_value')
   155                                                   #print(predicted_value.shape)
   156                                                   
   157                                                   
   158   1907344  187949360.0     98.5      1.1          loss=self.loss_function(predicted_value,target_value)
   159   1907344  528800786.0    277.2      3.0          self.optimizer.zero_grad()
   160   1907344 3141630809.0   1647.1     18.0          loss.backward()
   161                                                   
   162   1907344    7496642.0      3.9      0.0          if self.config.clip_error:
   163  28610160  409924315.0     14.3      2.4              for param in self.nn.parameters():
   164  26702816  511361165.0     19.2      2.9                  param.grad.clamp_(-1.0,1.0)
   165                                                   
   166   1907344 4306023405.0   2257.6     24.7          self.optimizer.step()
   167                                                   
   168   1907344    7019191.0      3.7      0.0          if self.number_of_frames % self.config.update_target_frequency == 0:
   169                                                       #print("***********************")
   170                                                       #print("UPDATING TARGET NETWORK")
   171                                                       #print("update counter: {}".format(self.update_target_counter))
   172                                                       #print("***********************")
   173       382     405627.0   1061.9      0.0              self.target_nn.load_state_dict(self.nn.state_dict())
   174                                                       
   175   1907344    3422974.0      1.8      0.0          if self.number_of_frames % self.config.save_model_frequency ==0:
   176       191    7234787.0  37878.5      0.0              self.save_model()
   177                                                   
   178   1907344    4094840.0      2.1      0.0          self.number_of_frames+=1

Total time: 8560.43 s
File: ../models/models.py
Function: forward at line 84

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    84                                               def forward(self, x):
    85                                                   
    86                                                   #print('in model forward')
    87                                                   
    88                                                   #print('x shape {} and value:'.format(x.shape))
    89                                                   #print(x)
    90   7601754   12429060.0      1.6      0.1          if self.config.normalize_image:
    91   7601754  252804292.0     33.3      3.0              x=x/255.0
    92                                                   
    93                                                   
    94   7601754 1171108857.0    154.1     13.7          output_conv = self.conv1(x)
    95   7601754  362887600.0     47.7      4.2          output_conv = self.activation(output_conv)
    96   7601754  963798931.0    126.8     11.3          output_conv = self.conv2(output_conv)
    97   7601754  328294653.0     43.2      3.8          output_conv = self.activation(output_conv)
    98   7601754  951026423.0    125.1     11.1          output_conv = self.conv3(output_conv)
    99   7601754  322191991.0     42.4      3.8          output_conv = self.activation(output_conv)
   100                                                   
   101                                                   #print('conv part done; reshape')
   102   7601754   83904163.0     11.0      1.0          output_conv = output_conv.view(output_conv.shape[0],-1)
   103                                                   #print('output_conv shape: {}'.format(output_conv.shape))
   104                                                   
   105                                                   #print('reshape done; advantage value')
   106   7601754  882504488.0    116.1     10.3          output_advantage=self.advantage1(output_conv)
   107                                                   #print('a1 done')
   108   7601754  333818553.0     43.9      3.9          output_advantage=self.activation(output_advantage)
   109                                                   #print('a1relu done')
   110   7601754  713517245.0     93.9      8.3          output_advantage=self.advantage2(output_advantage)
   111                                                   #print('a2 done')
   112                                                   
   113   7601754  682840672.0     89.8      8.0          output_value=self.value1(output_conv)
   114                                                   #print('v1 done')
   115   7601754  319799208.0     42.1      3.7          output_value=self.activation(output_value)
   116                                                   #print('v1relu done')
   117   7601754  611374978.0     80.4      7.1          output_value=self.value2(output_value)
   118                                                   #print('v2 done')
   119                                                   
   120                                                   #print('output_advantage shape {} and value:'.format(output_advantage.shape))
   121                                                   #print(output_advantage.detach().cpu())
   122                                                   
   123                                                   #print('output_value shape {} and value:'.format(output_value.shape))
   124                                                   #print(output_value.detach().cpu())
   125                                                   
   126                                                   #print('output_advantage.mean shape {} and value:'.format(output_advantage.mean(dim=1,keepdim=True).shape))
   127                                                   #print(output_advantage.mean(dim=1,keepdim=True).detach().cpu())
   128                                                   
   129                                                   #print('advantage value done; computing final output')
   130   7601754  559125816.0     73.6      6.5          output_final = output_value + output_advantage - output_advantage.mean()
   131                                                   
   132                                                   #output_final=output_value+output_advantage-output_advantage.mean(dim=1,keepdim=True)
   133                                                   
   134                                                   #print('returning')
   135   7601754    9007754.0      1.2      0.1          return output_final

Total time: 485.369 s
File: ../utils/ExperienceReplay.py
Function: push at line 42

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    42                                               def push(self, state,
    43                                                        action, new_state,
    44                                                        reward, done):
    45                                                   
    46                                                       #print('pushing')
    47                                                       #print(self.memory_state.is_contiguous())
    48                                                       #print(self.memory_new_state.is_contiguous())
    49                                                       #print(self.memory_state.shape)
    50                                                       #print(self.memory_new_state.shape)
    51                                                       #print(self.memory_state)
    52                                                       #print(self.memory_state[self.position,:])
    53                                                       
    54                                                       
    55   1907375  132731896.0     69.6     27.3              self.memory_state[self.position,:]=state[0,:]
    56   1907375   84723236.0     44.4     17.5              self.memory_new_state[self.position,:]=new_state[0,:]
    57   1907375  111955346.0     58.7     23.1              self.memory_action[self.position,0]=action
    58   1907375   78025603.0     40.9     16.1              self.memory_reward[self.position]=reward
    59   1907375   67986390.0     35.6     14.0              self.memory_done[self.position]=done
    60                                                       
    61                                                         
    62   1907375    4445914.0      2.3      0.9              self.position=(self.position+1)%self.capacity
    63   1907375    5500414.0      2.9      1.1              self.filled_to=min(self.capacity,self.filled_to+1)

Total time: 320.504 s
File: ../utils/ExperienceReplay.py
Function: sample at line 66

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    66                                               def sample(self,batch_size):
    67                                                   
    68                                                   #idx=torch.multinomial(self.wgt,batch_size)
    69   1907344    2610730.0      1.4      0.8          idx=torch.randint(0,self.filled_to,
    70   1907344    1464410.0      0.8      0.5                            (batch_size,),
    71   1907344    1570599.0      0.8      0.5                            dtype=torch.long,
    72   1907344   57990624.0     30.4     18.1                            device=self.memory_device)
    73   1907344   75813866.0     39.7     23.7          return (self.memory_state[idx],
    74   1907344   50644017.0     26.6     15.8                  self.memory_action[idx],
    75   1907344   47974312.0     25.2     15.0                  self.memory_new_state[idx],
    76   1907344   42956449.0     22.5     13.4                  self.memory_reward[idx],
    77   1907344   39479125.0     20.7     12.3                  self.memory_done[idx])

Total time: 25410.5 s
File: ../utils/engine.py
Function: train_agent at line 79

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    79                                               def train_agent(self):
    80                                                   
    81         1         98.0     98.0      0.0          print('training the agent')
    82                                                   
    83                                                   
    84         1        104.0    104.0      0.0          steps_total=np.full([self.config.num_episodes],-999,dtype=np.int32)
    85         1         15.0     15.0      0.0          reward_total=np.full([self.config.num_episodes],-999,dtype=np.int32)
    86                                           
    87         1          1.0      1.0      0.0          frames_total=0
    88                                           
    89         1          1.0      1.0      0.0          solved_after = 0
    90         1          1.0      1.0      0.0          solved = False
    91                                           
    92         1          2.0      2.0      0.0          start_time = time.time()
    93                                           
    94      1001       2185.0      2.2      0.0          for i_episode in range(self.config.num_episodes):
    95                                                       
    96                                                       #print('reset environment')
    97      1000   47600078.0  47600.1      0.2              state = self.env.reset()
    98      1000      74495.0     74.5      0.0              state=preprocess_frame(state)
    99                                                       #for step in range(100):
   100      1000       2007.0      2.0      0.0              step=0
   101      1000       3066.0      3.1      0.0              reward_total[i_episode]=0
   102                                           
   103      1000       1461.0      1.5      0.0              while True:
   104                                           
   105   1907375    2974063.0      1.6      0.0                  step+=1
   106   1907375    2772008.0      1.5      0.0                  frames_total += 1
   107                                           
   108   1907375   14206588.0      7.4      0.1                  epsilon=calculate_epsilon(frames_total,self.config)
   109                                           
   110                                                           #action=env.action_space.sample()
   111                                                           #print('select action')
   112   1907375 2751030381.0   1442.3     10.8                  action=self.qnet_agent.select_action(state,epsilon)
   113                                                           
   114                                                           #print('step env')
   115   1907375 4217636775.0   2211.2     16.6                  new_state, reward, done, info = self.env.step(action)
   116   1907375  106257549.0     55.7      0.4                  new_state=preprocess_frame(new_state)
   117                                                           
   118                                                           #print('push mem')
   119   1907375    4538325.0      2.4      0.0                  self.memory.push(state, action, new_state,
   120   1907375  504952356.0    264.7      2.0                                   reward, done)
   121                                           
   122   1907375   40075265.0     21.0      0.2                  reward_total[i_episode]+=reward
   123                                           
   124                                                           #print('agent optimize')
   125   1907375 17680935902.0   9269.8     69.6                  self.qnet_agent.optimize()
   126                                           
   127                                                           #print('assign state')
   128   1907375    7604499.0      4.0      0.0                  state=new_state
   129                                           
   130                                           
   131   1907375    2745377.0      1.4      0.0                  if done:
   132      1000       6972.0      7.0      0.0                      steps_total[i_episode]=step
   133                                           
   134      1000       1708.0      1.7      0.0                      if i_episode>100:
   135       899      78630.0     87.5      0.0                          mean_reward_100 = np.sum(reward_total[i_episode-100:i_episode])/100
   136                                           
   137                                           
   138       899       3888.0      4.3      0.0                          if (mean_reward_100 > self.config.score_to_solve and solved == False):
   139         1        681.0    681.0      0.0                              print("SOLVED! After %i episodes " % i_episode)
   140         1          3.0      3.0      0.0                              solved_after = i_episode
   141         1          2.0      2.0      0.0                              solved = True
   142                                           
   143      1000       2060.0      2.1      0.0                      if (i_episode % self.config.report_interval == 0 and i_episode>1):
   144                                           
   145        99   26857079.0 271283.6      0.1                          plot_results(reward_total, i_episode,self.config)
   146                                           
   147        99      45467.0    459.3      0.0                          print("**** Episode  {} **** ".format(i_episode))
   148        99      10456.0    105.6      0.0                          recent_avg_reward=np.average(reward_total[i_episode-self.config.report_interval:i_episode])
   149        99       8819.0     89.1      0.0                          print("Recent average reward: {}".format(recent_avg_reward))
   150        99        195.0      2.0      0.0                          if i_episode>100:
   151        89       6923.0     77.8      0.0                              print("Reward over last 100: {}".format(mean_reward_100))
   152        99       4860.0     49.1      0.0                          full_avg_so_far=np.average(reward_total[:i_episode])
   153        99       7031.0     71.0      0.0                          print("Average over all episodes so far: {}".format(full_avg_so_far))
   154        99       5627.0     56.8      0.0                          print("epsilon: {}".format(epsilon))
   155        99        385.0      3.9      0.0                          elapsed_time = time.time() - start_time
   156        99      15644.0    158.0      0.0                          print("Elapsed time: ", time.strftime("%H:%M:%S", time.gmtime(elapsed_time)))
   157                                           
   158                                                                   #print("Episode {} finished after: {}".format(i_episode,step))
   159      1000       1595.0      1.6      0.0                      break
   160                                           
   161         1          2.0      2.0      0.0          if solved:
   162         1        533.0    533.0      0.0              print("Solved after %i episodes" % solved_after)