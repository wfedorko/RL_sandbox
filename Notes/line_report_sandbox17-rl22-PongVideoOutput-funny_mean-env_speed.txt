Timer unit: 1e-06 s

Total time: 21.1793 s
File: <ipython-input-25-c200d54e7269>
Function: push at line 8

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     8                                               def push(self, state,
     9                                                        action, new_state,
    10                                                        reward, done):
    11                                                   
    12   2038754    1931047.0      0.9      9.1              transition=(state,action,new_state,reward,done)
    13                                                       
    14   2038754    3070517.0      1.5     14.5              if self.position>=len(self.memory):
    15    100000     118066.0      1.2      0.6                  self.memory.append(transition)
    16                                                       else:
    17   1938754   12967392.0      6.7     61.2                  self.memory[self.position]=transition
    18                                                           
    19   2038754    3092246.0      1.5     14.6              self.position=(self.position+1)%self.capacity

Total time: 453.764 s
File: <ipython-input-25-c200d54e7269>
Function: sample at line 22

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    22                                               def sample(self,batch_size):
    23   2038723  453763756.0    222.6    100.0          return zip(*random.sample(self.memory, batch_size))

Total time: 10268.3 s
File: <ipython-input-26-1b2c7e32e2ea>
Function: forward at line 18

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    18                                               def forward(self, x):
    19                                                   
    20                                                   #print('x shape {} and value:'.format(x.shape))
    21                                                   #print(x.detach().cpu())
    22                                                   
    23   8125689   11321227.0      1.4      0.1          if normalize_image:
    24   8125689  294763247.0     36.3      2.9              x=x/255.0
    25                                                   
    26   8125689 1464494292.0    180.2     14.3          output_conv = self.conv1(x)
    27   8125689  442608813.0     54.5      4.3          output_conv = self.activation(output_conv)
    28   8125689 1161723237.0    143.0     11.3          output_conv = self.conv2(output_conv)
    29   8125689  389468298.0     47.9      3.8          output_conv = self.activation(output_conv)
    30   8125689 1141873064.0    140.5     11.1          output_conv = self.conv3(output_conv)
    31   8125689  378711381.0     46.6      3.7          output_conv = self.activation(output_conv)
    32                                                   
    33   8125689  103391879.0     12.7      1.0          output_conv = output_conv.view(output_conv.shape[0],-1)
    34                                                   
    35   8125689 1052468155.0    129.5     10.2          output_advantage=self.advantage1(output_conv)
    36   8125689  390773167.0     48.1      3.8          output_advantage=self.activation(output_advantage)
    37   8125689  839254407.0    103.3      8.2          output_advantage=self.advantage2(output_advantage)
    38                                                   
    39   8125689  798099145.0     98.2      7.8          output_value=self.value1(output_conv)
    40   8125689  385152710.0     47.4      3.8          output_value=self.activation(output_value)
    41   8125689  717437725.0     88.3      7.0          output_value=self.value2(output_value)
    42                                                   
    43                                                   #print('output_advantage shape {} and value:'.format(output_advantage.shape))
    44                                                   #print(output_advantage.detach().cpu())
    45                                                   
    46                                                   #print('output_value shape {} and value:'.format(output_value.shape))
    47                                                   #print(output_value.detach().cpu())
    48                                                   
    49                                                   #print('output_advantage.mean shape {} and value:'.format(output_advantage.mean(dim=1,keepdim=True).shape))
    50                                                   #print(output_advantage.mean(dim=1,keepdim=True).detach().cpu())
    51                                                   
    52   8125689  686463429.0     84.5      6.7          output_final = output_value + output_advantage - output_advantage.mean()
    53                                                   
    54                                                   #output_final=output_value+output_advantage-output_advantage.mean(dim=1,keepdim=True)
    55                                                   
    56                                                   
    57   8125689   10264620.0      1.3      0.1          return output_final

Total time: 31716.1 s
File: <ipython-input-28-730e29fdb866>
Function: optimize at line 36

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    36                                               def optimize(self):
    37                                                   
    38   2038754   10100378.0      5.0      0.0          if len(memory)<batch_size:
    39        31         35.0      1.1      0.0              return
    40                                                   
    41   2038723  492840981.0    241.7      1.6          state, action, new_state, reward, done = memory.sample(batch_size)
    42                                                   
    43   2038723 4101715534.0   2011.9     12.9          state=[preprocess_frame(frame) for frame in state] 
    44   2038723  310339047.0    152.2      1.0          state=torch.cat(state)
    45                                                   
    46   2038723 3939632198.0   1932.4     12.4          new_state=[preprocess_frame(frame) for frame in new_state] 
    47   2038723  293870760.0    144.1      0.9          new_state=torch.cat(new_state)
    48                                                   
    49                                                   #print('state batch shape {}'.format(state.shape))
    50                                                   #print(state)
    51                                                   
    52                                                   #state=torch.Tensor(state).to(device)
    53                                                   #new_state=torch.Tensor(new_state).to(device)
    54                                                   
    55                                                   
    56   2038723  113386985.0     55.6      0.4          reward=torch.Tensor(reward).to(device)
    57                                                   
    58                                                   #the view call below is to transform into column vector
    59                                                   #so that it can be used in the gather call
    60                                                   #i.e. we will use it to pick out from the computed value
    61                                                   #tensor only values indexed by selected action
    62   2038723  118782884.0     58.3      0.4          action=(torch.Tensor(action).view(-1,1).long()).to(device)
    63                                                   #print('action: ')
    64                                                   #print(action)
    65                                                   #print('contiguous?', action.is_contiguous())
    66   2038723   71727883.0     35.2      0.2          done=torch.Tensor(done).to(device)
    67                                                   
    68                                                   #print('shape of: state, new state, reward, action, done:')
    69                                                   #print(state.shape)
    70                                                   #print(new_state.shape)
    71                                                   #print(reward.shape)
    72                                                   #print(action.shape)
    73                                                   #print(done.shape)
    74                                                   
    75                                                   
    76   2038723  315772675.0    154.9      1.0          self.nn.eval()
    77   2038723  258465789.0    126.8      0.8          self.target_nn.eval()
    78                                                       
    79                                                       
    80   2038723    4152364.0      2.0      0.0          if double_dqn:
    81                                                       #print('in double DQN')
    82   2038723 2947179455.0   1445.6      9.3              new_state_values_from_nn=self.nn(new_state).detach()
    83                                                       #print('new_state_values_from_nn shape {} and value:'.format(new_state_values_from_nn.shape))
    84                                                       #print(new_state_values_from_nn)
    85   2038723  101120057.0     49.6      0.3              max_new_state_indexes=torch.max(new_state_values_from_nn,dim=1)[1].view(-1,1)
    86                                                       #print('max_new_state_indexes shape {} and value:'.format(max_new_state_indexes.shape))
    87                                                       #print(max_new_state_indexes)
    88   2038723 2764614917.0   1356.1      8.7              new_state_values=self.target_nn(new_state).detach()
    89                                                       #print('new_state_values shape {} and value:'.format(new_state_values.shape))
    90                                                       #print(new_state_values)
    91   2038723   85193292.0     41.8      0.3              max_new_state_values=torch.gather(new_state_values,1,max_new_state_indexes).squeeze()
    92                                                       #print('max_new_state_values shape {} and value:'.format(max_new_state_values.shape))
    93                                                       #print(max_new_state_values)
    94                                                   else:
    95                                                       #print('in regular DQN')
    96                                                       new_state_values=self.target_nn(new_state).detach()
    97                                                       #print('new_state_values shape {} and value'.format(new_state_values.shape))
    98                                                       #print(new_state_values)
    99                                                   
   100                                                       max_new_state_values=torch.max(new_state_values,dim=1)[0]
   101                                                       #print('max_new_state_values shape {} and value'.format(max_new_state_values.shape))
   102                                                       #print(max_new_state_values)
   103                                                       
   104   2038723  222830207.0    109.3      0.7          target_value=(reward + (1-done)*gamma*max_new_state_values).view(-1,1)
   105                                                   
   106                                                   #print('shape of: target_value')
   107                                                   #print(target_value.shape)
   108   2038723  295619070.0    145.0      0.9          self.nn.train()
   109                                                   
   110                                                   #this will select only the values of the desired actions
   111   2038723 2738846536.0   1343.4      8.6          predicted_value=torch.gather(self.nn(state),1,action)
   112                                                   #print('shape of: predicted_value')
   113                                                   #print(predicted_value.shape)
   114                                                   
   115                                                   
   116   2038723  216815073.0    106.3      0.7          loss=self.loss_function(predicted_value,target_value)
   117   2038723  590904756.0    289.8      1.9          self.optimizer.zero_grad()
   118   2038723 5785097882.0   2837.6     18.2          loss.backward()
   119                                                   
   120   2038723   11070058.0      5.4      0.0          if clip_error:
   121  30580845  478848700.0     15.7      1.5              for param in self.nn.parameters():
   122  28542122  660805298.0     23.2      2.1                  param.grad.clamp_(-1.0,1.0)
   123                                                   
   124   2038723 4757841407.0   2333.7     15.0          self.optimizer.step()
   125                                                   
   126   2038723    8123945.0      4.0      0.0          if self.number_of_frames % update_target_frequency == 0:
   127                                                       #print("***********************")
   128                                                       #print("UPDATING TARGET NETWORK")
   129                                                       #print("update counter: {}".format(self.update_target_counter))
   130                                                       #print("***********************")
   131       408     424015.0   1039.3      0.0              self.target_nn.load_state_dict(self.nn.state_dict())
   132                                                       
   133   2038723    4456295.0      2.2      0.0          if self.number_of_frames % save_model_frequency ==0:
   134       204   10604068.0  51980.7      0.0              save_model(self.nn)
   135                                                   
   136   2038723    4950185.0      2.4      0.0          self.number_of_frames+=1

Total time: 39966.6 s
File: <ipython-input-32-c5a662e5eebe>
Function: train_agent at line 1

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     1                                           def train_agent():
     2         1         62.0     62.0      0.0      steps_total=np.full([num_episodes],-999,dtype=np.int32)
     3         1         10.0     10.0      0.0      reward_total=np.full([num_episodes],-999,dtype=np.int32)
     4                                           
     5         1          1.0      1.0      0.0      frames_total=0
     6                                           
     7         1          1.0      1.0      0.0      solved_after = 0
     8         1          0.0      0.0      0.0      solved = False
     9                                           
    10         1          2.0      2.0      0.0      start_time = time.time()
    11                                           
    12      1001       3744.0      3.7      0.0      for i_episode in range(num_episodes):
    13                                           
    14      1000   44117017.0  44117.0      0.1          state = env.reset()
    15                                                   #for step in range(100):
    16      1000       2711.0      2.7      0.0          step=0
    17      1000       3045.0      3.0      0.0          reward_total[i_episode]=0
    18                                           
    19      1000       1409.0      1.4      0.0          while True:
    20                                           
    21   2038754    3032462.0      1.5      0.0              step+=1
    22   2038754    2848497.0      1.4      0.0              frames_total += 1
    23                                           
    24   2038754   13011175.0      6.4      0.0              epsilon=calculate_epsilon(frames_total)
    25                                           
    26                                                       #action=env.action_space.sample()
    27   2038754 3245036851.0   1591.7      8.1              action=qnet_agent.select_action(state,epsilon)
    28                                           
    29   2038754 4455193594.0   2185.3     11.1              new_state, reward, done, info = env.step(action)
    30   2038754    5755660.0      2.8      0.0              memory.push(state, action, new_state,
    31   2038754   33355042.0     16.4      0.1                           reward, done)
    32                                           
    33   2038754   35450538.0     17.4      0.1              reward_total[i_episode]+=reward
    34                                           
    35   2038754 32095838496.0  15742.9     80.3              qnet_agent.optimize()
    36                                           
    37   2038754    3271971.0      1.6      0.0              state=new_state
    38                                           
    39                                           
    40   2038754    2713874.0      1.3      0.0              if done:
    41      1000      11010.0     11.0      0.0                  steps_total[i_episode]=step
    42                                           
    43      1000       1562.0      1.6      0.0                  if i_episode>100:
    44       899     104141.0    115.8      0.0                      mean_reward_100 = np.sum(reward_total[i_episode-100:i_episode])/100
    45                                           
    46                                           
    47       899       2824.0      3.1      0.0                      if (mean_reward_100 > score_to_solve and solved == False):
    48         1        530.0    530.0      0.0                          print("SOLVED! After %i episodes " % i_episode)
    49         1          2.0      2.0      0.0                          solved_after = i_episode
    50         1          1.0      1.0      0.0                          solved = True
    51                                           
    52      1000       1703.0      1.7      0.0                  if (i_episode % report_interval == 0 and i_episode>1):
    53                                           
    54        99   26708645.0 269784.3      0.1                      plot_results(reward_total, i_episode)
    55                                           
    56        99      45049.0    455.0      0.0                      print("**** Episode  {} **** ".format(i_episode))
    57        99       9936.0    100.4      0.0                      recent_avg_reward=np.average(reward_total[i_episode-report_interval:i_episode])
    58        99       6030.0     60.9      0.0                      print("Recent average reward: {}".format(recent_avg_reward))
    59        99        194.0      2.0      0.0                      if i_episode>100:
    60        89       6162.0     69.2      0.0                          print("Reward over last 100: {}".format(mean_reward_100))
    61        99       5046.0     51.0      0.0                      full_avg_so_far=np.average(reward_total[:i_episode])
    62        99       7151.0     72.2      0.0                      print("Average over all episodes so far: {}".format(full_avg_so_far))
    63        99       4181.0     42.2      0.0                      print("epsilon: {}".format(epsilon))
    64        99        385.0      3.9      0.0                      elapsed_time = time.time() - start_time
    65        99      11912.0    120.3      0.0                      print("Elapsed time: ", time.strftime("%H:%M:%S", time.gmtime(elapsed_time)))
    66                                                               #print("Episode {} finished after: {}".format(i_episode,step))
    67      1000       1483.0      1.5      0.0                  break
    68                                           
    69         1          2.0      2.0      0.0      if solved:
    70         1        495.0    495.0      0.0          print("Solved after %i episodes" % solved_after)