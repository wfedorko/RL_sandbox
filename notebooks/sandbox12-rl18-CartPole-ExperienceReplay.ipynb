{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, episode,step, info=\"\"):\n",
    "    plt.figure(99999,figsize=[8,6])\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"episode: {} step: {} \".format(episode,step))\n",
    "    #plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device=torch.device(\"cuda:4\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "num_episodes = 1000\n",
    "gamma=0.99\n",
    "#gamma=0.85\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.02\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval=10\n",
    "\n",
    "score_to_solve = 195\n",
    "\n",
    "hidden_layer_size=64\n",
    "\n",
    "replay_memory_size=50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.wrappers.Monitor(env, '../mp4/sandbox10',video_callable=lambda episode_id: True,force=True)\n",
    "#env = gym.wrappers.Monitor(env, '../mp4/cartpole-5',video_callable=lambda episode_id: episode_id%10==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs=env.observation_space.shape[0]\n",
    "number_of_outputs=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity=capacity\n",
    "        self.memory=[]\n",
    "        self.position=0\n",
    "        \n",
    "    \n",
    "    def push(self, state,\n",
    "             action, new_state,\n",
    "             reward, done):\n",
    "        pass\n",
    "    \n",
    "    def sample(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer_size)\n",
    "        self.linear2 = nn.Linear(hidden_layer_size,number_of_outputs)\n",
    "        \n",
    "        self.activation=nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_Agent():\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy=torch.rand(1).item()\n",
    "        \n",
    "        if random_for_egreedy>epsilon:\n",
    "            self.nn.eval()\n",
    "            with torch.no_grad():\n",
    "                state=torch.Tensor(state).to(device)\n",
    "                predicted_value_from_nn=self.nn(state)\n",
    "                action=torch.argmax(predicted_value_from_nn).item()\n",
    "        else:\n",
    "            action=env.action_space.sample()\n",
    "                \n",
    "                \n",
    "        return action\n",
    "    \n",
    "    def optimize(self, state, action, new_state, reward, done):\n",
    "        \n",
    "        state=torch.Tensor(state).to(device)\n",
    "        new_state=torch.Tensor(new_state).to(device)\n",
    "        reward=torch.Tensor([reward]).to(device)\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            target_value=reward\n",
    "        else:\n",
    "            self.nn.eval()\n",
    "            new_state_values=self.nn(new_state).detach()\n",
    "            max_new_state_values=torch.max(new_state_values)\n",
    "            target_value=reward + gamma*max_new_state_values\n",
    "        \n",
    "        self.nn.train()\n",
    "        #the view call needed to make it a tensor\n",
    "        predicted_value=self.nn(state)[action].view(-1)\n",
    "        \n",
    "        loss=self.loss_function(predicted_value,target_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ExperienceReplay(replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_agent=QNet_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value=23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  10 **** \n",
      "Recent average reward: 23.9\n",
      "Average over all episodes so far: 23.9\n",
      "epsilon: 0.5537469805471175\n",
      "**** Episode  20 **** \n",
      "Recent average reward: 14.0\n",
      "Average over all episodes so far: 18.95\n",
      "epsilon: 0.4185854512433708\n",
      "**** Episode  30 **** \n",
      "Recent average reward: 41.6\n",
      "Average over all episodes so far: 26.5\n",
      "epsilon: 0.17139948016428447\n",
      "**** Episode  40 **** \n",
      "Recent average reward: 42.9\n",
      "Average over all episodes so far: 30.6\n",
      "epsilon: 0.09369403852113245\n",
      "**** Episode  50 **** \n",
      "Recent average reward: 39.8\n",
      "Average over all episodes so far: 32.44\n",
      "epsilon: 0.05194200476696485\n",
      "**** Episode  60 **** \n",
      "Recent average reward: 58.6\n",
      "Average over all episodes so far: 36.8\n",
      "epsilon: 0.029874166552438394\n",
      "**** Episode  70 **** \n",
      "Recent average reward: 62.9\n",
      "Average over all episodes so far: 40.52857142857143\n",
      "epsilon: 0.022669582004333038\n",
      "**** Episode  80 **** \n",
      "Recent average reward: 119.6\n",
      "Average over all episodes so far: 50.4125\n",
      "epsilon: 0.02025459579921895\n",
      "**** Episode  90 **** \n",
      "Recent average reward: 57.7\n",
      "Average over all episodes so far: 51.22222222222222\n",
      "epsilon: 0.020072361702356125\n",
      "**** Episode  100 **** \n",
      "Recent average reward: 148.4\n",
      "Average over all episodes so far: 60.94\n",
      "epsilon: 0.02000337257715476\n",
      "**** Episode  110 **** \n",
      "Recent average reward: 229.9\n",
      "Reward over last 100: 81.54\n",
      "Average over all episodes so far: 76.3\n",
      "epsilon: 0.02000004292405818\n",
      "**** Episode  120 **** \n",
      "Recent average reward: 287.6\n",
      "Reward over last 100: 108.9\n",
      "Average over all episodes so far: 93.90833333333333\n",
      "epsilon: 0.020000000074083122\n",
      "**** Episode  130 **** \n",
      "Recent average reward: 209.1\n",
      "Reward over last 100: 125.65\n",
      "Average over all episodes so far: 102.76923076923077\n",
      "epsilon: 0.020000000001415084\n",
      "**** Episode  140 **** \n",
      "Recent average reward: 246.7\n",
      "Reward over last 100: 146.03\n",
      "Average over all episodes so far: 113.05\n",
      "epsilon: 0.02000000000001135\n",
      "**** Episode  150 **** \n",
      "Recent average reward: 191.9\n",
      "Reward over last 100: 161.24\n",
      "Average over all episodes so far: 118.30666666666667\n",
      "epsilon: 0.02000000000000027\n",
      "**** Episode  160 **** \n",
      "Recent average reward: 124.2\n",
      "Reward over last 100: 167.8\n",
      "Average over all episodes so far: 118.675\n",
      "epsilon: 0.02000000000000002\n",
      "**** Episode  170 **** \n",
      "Recent average reward: 183.5\n",
      "Reward over last 100: 179.86\n",
      "Average over all episodes so far: 122.48823529411764\n",
      "epsilon: 0.02\n",
      "SOLVED! After 178 episodes \n",
      "**** Episode  180 **** \n",
      "Recent average reward: 291.6\n",
      "Reward over last 100: 197.06\n",
      "Average over all episodes so far: 131.88333333333333\n",
      "epsilon: 0.02\n",
      "**** Episode  190 **** \n",
      "Recent average reward: 496.1\n",
      "Reward over last 100: 240.9\n",
      "Average over all episodes so far: 151.05263157894737\n",
      "epsilon: 0.02\n",
      "**** Episode  200 **** \n",
      "Recent average reward: 312.9\n",
      "Reward over last 100: 257.35\n",
      "Average over all episodes so far: 159.145\n",
      "epsilon: 0.02\n",
      "**** Episode  210 **** \n",
      "Recent average reward: 50.2\n",
      "Reward over last 100: 239.38\n",
      "Average over all episodes so far: 153.95714285714286\n",
      "epsilon: 0.02\n",
      "**** Episode  220 **** \n",
      "Recent average reward: 136.5\n",
      "Reward over last 100: 224.27\n",
      "Average over all episodes so far: 153.16363636363636\n",
      "epsilon: 0.02\n",
      "**** Episode  230 **** \n",
      "Recent average reward: 371.5\n",
      "Reward over last 100: 240.51\n",
      "Average over all episodes so far: 162.65652173913043\n",
      "epsilon: 0.02\n",
      "**** Episode  240 **** \n",
      "Recent average reward: 431.7\n",
      "Reward over last 100: 259.01\n",
      "Average over all episodes so far: 173.86666666666667\n",
      "epsilon: 0.02\n",
      "**** Episode  250 **** \n",
      "Recent average reward: 334.4\n",
      "Reward over last 100: 273.26\n",
      "Average over all episodes so far: 180.288\n",
      "epsilon: 0.02\n",
      "**** Episode  260 **** \n",
      "Recent average reward: 241.7\n",
      "Reward over last 100: 285.01\n",
      "Average over all episodes so far: 182.65\n",
      "epsilon: 0.02\n",
      "**** Episode  270 **** \n",
      "Recent average reward: 484.5\n",
      "Reward over last 100: 315.11\n",
      "Average over all episodes so far: 193.82962962962964\n",
      "epsilon: 0.02\n",
      "**** Episode  280 **** \n",
      "Recent average reward: 393.2\n",
      "Reward over last 100: 325.27\n",
      "Average over all episodes so far: 200.95\n",
      "epsilon: 0.02\n",
      "**** Episode  290 **** \n",
      "Recent average reward: 189.8\n",
      "Reward over last 100: 294.64\n",
      "Average over all episodes so far: 200.5655172413793\n",
      "epsilon: 0.02\n",
      "**** Episode  300 **** \n",
      "Recent average reward: 362.4\n",
      "Reward over last 100: 299.59\n",
      "Average over all episodes so far: 205.96\n",
      "epsilon: 0.02\n",
      "**** Episode  310 **** \n",
      "Recent average reward: 466.3\n",
      "Reward over last 100: 341.2\n",
      "Average over all episodes so far: 214.35806451612902\n",
      "epsilon: 0.02\n",
      "**** Episode  320 **** \n",
      "Recent average reward: 202.2\n",
      "Reward over last 100: 347.77\n",
      "Average over all episodes so far: 213.978125\n",
      "epsilon: 0.02\n",
      "**** Episode  330 **** \n",
      "Recent average reward: 155.7\n",
      "Reward over last 100: 326.19\n",
      "Average over all episodes so far: 212.21212121212122\n",
      "epsilon: 0.02\n",
      "**** Episode  340 **** \n",
      "Recent average reward: 136.6\n",
      "Reward over last 100: 296.68\n",
      "Average over all episodes so far: 209.98823529411766\n",
      "epsilon: 0.02\n",
      "**** Episode  350 **** \n",
      "Recent average reward: 262.9\n",
      "Reward over last 100: 289.53\n",
      "Average over all episodes so far: 211.5\n",
      "epsilon: 0.02\n",
      "**** Episode  360 **** \n",
      "Recent average reward: 500.0\n",
      "Reward over last 100: 315.36\n",
      "Average over all episodes so far: 219.51388888888889\n",
      "epsilon: 0.02\n",
      "**** Episode  370 **** \n",
      "Recent average reward: 140.5\n",
      "Reward over last 100: 280.96\n",
      "Average over all episodes so far: 217.3783783783784\n",
      "epsilon: 0.02\n",
      "**** Episode  380 **** \n",
      "Recent average reward: 159.3\n",
      "Reward over last 100: 257.57\n",
      "Average over all episodes so far: 215.85\n",
      "epsilon: 0.02\n",
      "**** Episode  390 **** \n",
      "Recent average reward: 177.6\n",
      "Reward over last 100: 256.35\n",
      "Average over all episodes so far: 214.86923076923077\n",
      "epsilon: 0.02\n",
      "**** Episode  400 **** \n",
      "Recent average reward: 334.5\n",
      "Reward over last 100: 253.56\n",
      "Average over all episodes so far: 217.86\n",
      "epsilon: 0.02\n",
      "**** Episode  410 **** \n",
      "Recent average reward: 491.7\n",
      "Reward over last 100: 256.1\n",
      "Average over all episodes so far: 224.5390243902439\n",
      "epsilon: 0.02\n",
      "**** Episode  420 **** \n",
      "Recent average reward: 392.2\n",
      "Reward over last 100: 275.1\n",
      "Average over all episodes so far: 228.53095238095239\n",
      "epsilon: 0.02\n",
      "**** Episode  430 **** \n",
      "Recent average reward: 360.9\n",
      "Reward over last 100: 295.62\n",
      "Average over all episodes so far: 231.60930232558138\n",
      "epsilon: 0.02\n",
      "**** Episode  440 **** \n",
      "Recent average reward: 474.5\n",
      "Reward over last 100: 329.41\n",
      "Average over all episodes so far: 237.12954545454545\n",
      "epsilon: 0.02\n",
      "**** Episode  450 **** \n",
      "Recent average reward: 496.7\n",
      "Reward over last 100: 352.79\n",
      "Average over all episodes so far: 242.89777777777778\n",
      "epsilon: 0.02\n",
      "**** Episode  460 **** \n",
      "Recent average reward: 269.8\n",
      "Reward over last 100: 329.77\n",
      "Average over all episodes so far: 243.48260869565217\n",
      "epsilon: 0.02\n",
      "**** Episode  470 **** \n",
      "Recent average reward: 392.5\n",
      "Reward over last 100: 354.97\n",
      "Average over all episodes so far: 246.6531914893617\n",
      "epsilon: 0.02\n",
      "**** Episode  480 **** \n",
      "Recent average reward: 405.8\n",
      "Reward over last 100: 379.62\n",
      "Average over all episodes so far: 249.96875\n",
      "epsilon: 0.02\n",
      "**** Episode  490 **** \n",
      "Recent average reward: 500.0\n",
      "Reward over last 100: 411.86\n",
      "Average over all episodes so far: 255.07142857142858\n",
      "epsilon: 0.02\n",
      "**** Episode  500 **** \n",
      "Recent average reward: 452.2\n",
      "Reward over last 100: 423.63\n",
      "Average over all episodes so far: 259.014\n",
      "epsilon: 0.02\n",
      "**** Episode  510 **** \n",
      "Recent average reward: 500.0\n",
      "Reward over last 100: 424.46\n",
      "Average over all episodes so far: 263.7392156862745\n",
      "epsilon: 0.02\n",
      "**** Episode  520 **** \n",
      "Recent average reward: 450.6\n",
      "Reward over last 100: 430.3\n",
      "Average over all episodes so far: 267.3326923076923\n",
      "epsilon: 0.02\n",
      "**** Episode  530 **** \n",
      "Recent average reward: 500.0\n",
      "Reward over last 100: 444.21\n",
      "Average over all episodes so far: 271.72264150943397\n",
      "epsilon: 0.02\n",
      "**** Episode  540 **** \n",
      "Recent average reward: 500.0\n",
      "Reward over last 100: 446.76\n",
      "Average over all episodes so far: 275.95\n",
      "epsilon: 0.02\n",
      "**** Episode  550 **** \n",
      "Recent average reward: 437.5\n",
      "Reward over last 100: 440.84\n",
      "Average over all episodes so far: 278.8872727272727\n",
      "epsilon: 0.02\n",
      "**** Episode  560 **** \n",
      "Recent average reward: 453.3\n",
      "Reward over last 100: 459.19\n",
      "Average over all episodes so far: 282.0017857142857\n",
      "epsilon: 0.02\n",
      "**** Episode  570 **** \n",
      "Recent average reward: 336.4\n",
      "Reward over last 100: 453.58\n",
      "Average over all episodes so far: 282.9561403508772\n",
      "epsilon: 0.02\n",
      "**** Episode  580 **** \n",
      "Recent average reward: 239.6\n",
      "Reward over last 100: 436.96\n",
      "Average over all episodes so far: 282.20862068965516\n",
      "epsilon: 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  590 **** \n",
      "Recent average reward: 246.9\n",
      "Reward over last 100: 411.65\n",
      "Average over all episodes so far: 281.6101694915254\n",
      "epsilon: 0.02\n",
      "**** Episode  600 **** \n",
      "Recent average reward: 313.2\n",
      "Reward over last 100: 397.75\n",
      "Average over all episodes so far: 282.13666666666666\n",
      "epsilon: 0.02\n",
      "**** Episode  610 **** \n",
      "Recent average reward: 483.5\n",
      "Reward over last 100: 396.1\n",
      "Average over all episodes so far: 285.4377049180328\n",
      "epsilon: 0.02\n",
      "**** Episode  620 **** \n",
      "Recent average reward: 396.2\n",
      "Reward over last 100: 390.66\n",
      "Average over all episodes so far: 287.2241935483871\n",
      "epsilon: 0.02\n",
      "**** Episode  630 **** \n",
      "Recent average reward: 61.8\n",
      "Reward over last 100: 346.84\n",
      "Average over all episodes so far: 283.6460317460317\n",
      "epsilon: 0.02\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_episodes=1000\n",
    "\n",
    "steps_total=np.full([num_episodes],-999,dtype=np.int32)\n",
    "\n",
    "frames_total=0\n",
    "\n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    #for step in range(100):\n",
    "    step=0\n",
    "    while True:\n",
    "        \n",
    "        step+=1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon=calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action=env.action_space.sample()\n",
    "        action=qnet_agent.select_action(state,epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        memeory.push(state, action, new_state,\n",
    "                     reward, done)\n",
    "        \n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state=new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total[i_episode]=step\n",
    "            \n",
    "            if i_episode>100:\n",
    "                mean_reward_100 = np.sum(steps_total[i_episode-100:i_episode])/100\n",
    "            \n",
    "                if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                    print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                    solved_after = i_episode\n",
    "                    solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0 and i_episode>1):\n",
    "                print(\"**** Episode  {} **** \".format(i_episode))\n",
    "                recent_avg_reward=np.average(steps_total[i_episode-report_interval:i_episode])\n",
    "                print(\"Recent average reward: {}\".format(recent_avg_reward))\n",
    "                if i_episode>100:\n",
    "                    print(\"Reward over last 100: {}\".format(mean_reward_100))\n",
    "                full_avg_so_far=np.average(steps_total[:i_episode])\n",
    "                print(\"Average over all episodes so far: {}\".format(full_avg_so_far))\n",
    "                print(\"epsilon: {}\".format(epsilon))\n",
    "            \n",
    "                #print(\"Episode {} finished after: {}\".format(i_episode,step))\n",
    "            break\n",
    "            \n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of steps: {}\". format(np.average(steps_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of steps=reward: {}\". format(np.average(steps_total)))\n",
    "print(\"Average number of steps=reward in last 100 episodes: {}\". format(np.average(steps_total[-100:])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(1,figsize=[12,5])\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total,alpha=0.6, color='green')\n",
    "#plt.plot(rewards_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2,figsize=[12,5])\n",
    "plt.title(\"Steps to finish episode\")\n",
    "plt.plot(steps_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
