This is with commit c68a535


Timer unit: 1e-06 s

Total time: 29156.6 s
File: ../agents/QNet_Agent.py
Function: optimize at line 63

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    63                                               def optimize(self):
    64                                                   
    65   1971004   11317910.0      5.7      0.0          if len(self.memory)<self.config.batch_size:
    66        31         54.0      1.7      0.0              return
    67                                                   
    68   1970973  451343191.0    229.0      1.5          state, action, new_state, reward, done = self.memory.sample(self.config.batch_size)
    69                                                   
    70   1970973 3779209555.0   1917.4     13.0          state=[preprocess_frame(frame, self.device) for frame in state] 
    71   1970973  280029642.0    142.1      1.0          state=torch.cat(state)
    72                                                   
    73   1970973 3633333027.0   1843.4     12.5          new_state=[preprocess_frame(frame, self.device) for frame in new_state] 
    74   1970973  263375519.0    133.6      0.9          new_state=torch.cat(new_state)
    75                                                   
    76                                                   #print('state batch shape {}'.format(state.shape))
    77                                                   #print(state)
    78                                                   
    79                                                   #state=torch.Tensor(state).to(device)
    80                                                   #new_state=torch.Tensor(new_state).to(device)
    81                                                   
    82                                                   
    83   1970973  108828743.0     55.2      0.4          reward=torch.Tensor(reward).to(self.device)
    84                                                   
    85                                                   #the view call below is to transform into column vector
    86                                                   #so that it can be used in the gather call
    87                                                   #i.e. we will use it to pick out from the computed value
    88                                                   #tensor only values indexed by selected action
    89   1970973  110419053.0     56.0      0.4          action=(torch.Tensor(action).view(-1,1).long()).to(self.device)
    90                                                   #print('action: ')
    91                                                   #print(action)
    92                                                   #print('contiguous?', action.is_contiguous())
    93   1970973   66392095.0     33.7      0.2          done=torch.Tensor(done).to(self.device)
    94                                                   
    95                                                   #print('shape of: state, new state, reward, action, done:')
    96                                                   #print(state.shape)
    97                                                   #print(new_state.shape)
    98                                                   #print(reward.shape)
    99                                                   #print(action.shape)
   100                                                   #print(done.shape)
   101                                                   
   102                                                   
   103   1970973  307731953.0    156.1      1.1          self.nn.eval()
   104   1970973  243302134.0    123.4      0.8          self.target_nn.eval()
   105                                                       
   106                                                       
   107   1970973    4758812.0      2.4      0.0          if self.config.double_dqn:
   108                                                       #print('in double DQN')
   109   1970973 2754021637.0   1397.3      9.4              new_state_values_from_nn=self.nn(new_state).detach()
   110                                                       #print('new_state_values_from_nn shape {} and value:'.format(new_state_values_from_nn.shape))
   111                                                       #print(new_state_values_from_nn)
   112   1970973   94608948.0     48.0      0.3              max_new_state_indexes=torch.max(new_state_values_from_nn,dim=1)[1].view(-1,1)
   113                                                       #print('max_new_state_indexes shape {} and value:'.format(max_new_state_indexes.shape))
   114                                                       #print(max_new_state_indexes)
   115   1970973 2588581799.0   1313.4      8.9              new_state_values=self.target_nn(new_state).detach()
   116                                                       #print('new_state_values shape {} and value:'.format(new_state_values.shape))
   117                                                       #print(new_state_values)
   118   1970973   77864348.0     39.5      0.3              max_new_state_values=torch.gather(new_state_values,1,max_new_state_indexes).squeeze()
   119                                                       #print('max_new_state_values shape {} and value:'.format(max_new_state_values.shape))
   120                                                       #print(max_new_state_values)
   121                                                   else:
   122                                                       #print('in regular DQN')
   123                                                       new_state_values=self.target_nn(new_state).detach()
   124                                                       #print('new_state_values shape {} and value'.format(new_state_values.shape))
   125                                                       #print(new_state_values)
   126                                                   
   127                                                       max_new_state_values=torch.max(new_state_values,dim=1)[0]
   128                                                       #print('max_new_state_values shape {} and value'.format(max_new_state_values.shape))
   129                                                       #print(max_new_state_values)
   130                                                       
   131   1970973  206950338.0    105.0      0.7          target_value=(reward + (1-done)*self.config.gamma*max_new_state_values).view(-1,1)
   132                                                   
   133                                                   #print('shape of: target_value')
   134                                                   #print(target_value.shape)
   135   1970973  285848795.0    145.0      1.0          self.nn.train()
   136                                                   
   137                                                   #this will select only the values of the desired actions
   138   1970973 2574007350.0   1306.0      8.8          predicted_value=torch.gather(self.nn(state),1,action)
   139                                                   #print('shape of: predicted_value')
   140                                                   #print(predicted_value.shape)
   141                                                   
   142                                                   
   143   1970973  199352091.0    101.1      0.7          loss=self.loss_function(predicted_value,target_value)
   144   1970973  532572955.0    270.2      1.8          self.optimizer.zero_grad()
   145   1970973 5201060663.0   2638.8     17.8          loss.backward()
   146                                                   
   147   1970973   12961545.0      6.6      0.0          if self.config.clip_error:
   148  29564595  466078257.0     15.8      1.6              for param in self.nn.parameters():
   149  27593622  593942716.0     21.5      2.0                  param.grad.clamp_(-1.0,1.0)
   150                                                   
   151   1970973 4278164587.0   2170.6     14.7          self.optimizer.step()
   152                                                   
   153   1970973    8970830.0      4.6      0.0          if self.number_of_frames % self.config.update_target_frequency == 0:
   154                                                       #print("***********************")
   155                                                       #print("UPDATING TARGET NETWORK")
   156                                                       #print("update counter: {}".format(self.update_target_counter))
   157                                                       #print("***********************")
   158       395     411547.0   1041.9      0.0              self.target_nn.load_state_dict(self.nn.state_dict())
   159                                                       
   160   1970973    4558905.0      2.3      0.0          if self.number_of_frames % self.config.save_model_frequency ==0:
   161       198   11505872.0  58110.5      0.0              self.save_model()
   162                                                   
   163   1970973    5115109.0      2.6      0.0          self.number_of_frames+=1

Total time: 9639.21 s
File: ../models/models.py
Function: forward at line 83

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    83                                               def forward(self, x):
    84                                                   
    85                                                   #print('x shape {} and value:'.format(x.shape))
    86                                                   #print(x.detach().cpu())
    87                                                   
    88   7855404   14363796.0      1.8      0.1          if self.config.normalize_image:
    89   7855404  277855777.0     35.4      2.9              x=x/255.0
    90                                                   
    91   7855404 1397649531.0    177.9     14.5          output_conv = self.conv1(x)
    92   7855404  413724268.0     52.7      4.3          output_conv = self.activation(output_conv)
    93   7855404 1084527687.0    138.1     11.3          output_conv = self.conv2(output_conv)
    94   7855404  362619397.0     46.2      3.8          output_conv = self.activation(output_conv)
    95   7855404 1066237789.0    135.7     11.1          output_conv = self.conv3(output_conv)
    96   7855404  353008028.0     44.9      3.7          output_conv = self.activation(output_conv)
    97                                                   
    98   7855404   92303824.0     11.8      1.0          output_conv = output_conv.view(output_conv.shape[0],-1)
    99                                                   
   100   7855404 1014514804.0    129.1     10.5          output_advantage=self.advantage1(output_conv)
   101   7855404  365730856.0     46.6      3.8          output_advantage=self.activation(output_advantage)
   102   7855404  785156381.0    100.0      8.1          output_advantage=self.advantage2(output_advantage)
   103                                                   
   104   7855404  742754459.0     94.6      7.7          output_value=self.value1(output_conv)
   105   7855404  355744121.0     45.3      3.7          output_value=self.activation(output_value)
   106   7855404  664592810.0     84.6      6.9          output_value=self.value2(output_value)
   107                                                   
   108                                                   #print('output_advantage shape {} and value:'.format(output_advantage.shape))
   109                                                   #print(output_advantage.detach().cpu())
   110                                                   
   111                                                   #print('output_value shape {} and value:'.format(output_value.shape))
   112                                                   #print(output_value.detach().cpu())
   113                                                   
   114                                                   #print('output_advantage.mean shape {} and value:'.format(output_advantage.mean(dim=1,keepdim=True).shape))
   115                                                   #print(output_advantage.mean(dim=1,keepdim=True).detach().cpu())
   116                                                   
   117   7855404  639154222.0     81.4      6.6          output_final = output_value + output_advantage - output_advantage.mean()
   118                                                   
   119                                                   #output_final=output_value+output_advantage-output_advantage.mean(dim=1,keepdim=True)
   120                                                   
   121                                                   
   122   7855404    9269703.0      1.2      0.1          return output_final

Total time: 21.1079 s
File: ../utils/ExperienceReplay.py
Function: push at line 13

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    13                                               def push(self, state,
    14                                                        action, new_state,
    15                                                        reward, done):
    16                                                   
    17   1971004    1936422.0      1.0      9.2              transition=(state,action,new_state,reward,done)
    18                                                       
    19   1971004    3511693.0      1.8     16.6              if self.position>=len(self.memory):
    20    100000     130539.0      1.3      0.6                  self.memory.append(transition)
    21                                                       else:
    22   1871004   12441632.0      6.6     58.9                  self.memory[self.position]=transition
    23                                                           
    24   1971004    3087633.0      1.6     14.6              self.position=(self.position+1)%self.capacity

Total time: 420.744 s
File: ../utils/ExperienceReplay.py
Function: sample at line 27

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    27                                               def sample(self,batch_size):
    28   1970973  420743687.0    213.5    100.0          return zip(*random.sample(self.memory, batch_size))

Total time: 37011.5 s
File: ../utils/engine.py
Function: train_agent at line 69

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    69                                               def train_agent(self):
    70                                                   
    71         1        280.0    280.0      0.0          steps_total=np.full([self.config.num_episodes],-999,dtype=np.int32)
    72         1         16.0     16.0      0.0          reward_total=np.full([self.config.num_episodes],-999,dtype=np.int32)
    73                                           
    74         1          1.0      1.0      0.0          frames_total=0
    75                                           
    76         1          1.0      1.0      0.0          solved_after = 0
    77         1          1.0      1.0      0.0          solved = False
    78                                           
    79         1          2.0      2.0      0.0          start_time = time.time()
    80                                           
    81      1001       3604.0      3.6      0.0          for i_episode in range(self.config.num_episodes):
    82                                           
    83      1000   40603875.0  40603.9      0.1              state = self.env.reset()
    84                                                       #for step in range(100):
    85      1000       2871.0      2.9      0.0              step=0
    86      1000       3312.0      3.3      0.0              reward_total[i_episode]=0
    87                                           
    88      1000       1476.0      1.5      0.0              while True:
    89                                           
    90   1971004    2955620.0      1.5      0.0                  step+=1
    91   1971004    2803908.0      1.4      0.0                  frames_total += 1
    92                                           
    93   1971004   15122068.0      7.7      0.0                  epsilon=calculate_epsilon(frames_total,self.config)
    94                                           
    95                                                           #action=env.action_space.sample()
    96   1971004 3061988499.0   1553.5      8.3                  action=self.qnet_agent.select_action(state,epsilon)
    97                                           
    98   1971004 4268765016.0   2165.8     11.5                  new_state, reward, done, info = self.env.step(action)
    99   1971004    5842022.0      3.0      0.0                  self.memory.push(state, action, new_state,
   100   1971004   33892447.0     17.2      0.1                                   reward, done)
   101                                           
   102   1971004   34217995.0     17.4      0.1                  reward_total[i_episode]+=reward
   103                                           
   104   1971004 29513256400.0  14973.7     79.7                  self.qnet_agent.optimize()
   105                                           
   106   1971004    3359383.0      1.7      0.0                  state=new_state
   107                                           
   108                                           
   109   1971004    2650607.0      1.3      0.0                  if done:
   110      1000       8740.0      8.7      0.0                      steps_total[i_episode]=step
   111                                           
   112      1000       1556.0      1.6      0.0                      if i_episode>100:
   113       899     100990.0    112.3      0.0                          mean_reward_100 = np.sum(reward_total[i_episode-100:i_episode])/100
   114                                           
   115                                           
   116       899       4019.0      4.5      0.0                          if (mean_reward_100 > self.config.score_to_solve and solved == False):
   117         1        468.0    468.0      0.0                              print("SOLVED! After %i episodes " % i_episode)
   118         1          1.0      1.0      0.0                              solved_after = i_episode
   119         1          1.0      1.0      0.0                              solved = True
   120                                           
   121      1000       2041.0      2.0      0.0                      if (i_episode % self.config.report_interval == 0 and i_episode>1):
   122                                           
   123        99   25837769.0 260987.6      0.1                          plot_results(reward_total, i_episode,self.config)
   124                                           
   125        99      42500.0    429.3      0.0                          print("**** Episode  {} **** ".format(i_episode))
   126        99      10440.0    105.5      0.0                          recent_avg_reward=np.average(reward_total[i_episode-self.config.report_interval:i_episode])
   127        99       7120.0     71.9      0.0                          print("Recent average reward: {}".format(recent_avg_reward))
   128        99        207.0      2.1      0.0                          if i_episode>100:
   129        89       4134.0     46.4      0.0                              print("Reward over last 100: {}".format(mean_reward_100))
   130        99       5287.0     53.4      0.0                          full_avg_so_far=np.average(reward_total[:i_episode])
   131        99       6573.0     66.4      0.0                          print("Average over all episodes so far: {}".format(full_avg_so_far))
   132        99      10656.0    107.6      0.0                          print("epsilon: {}".format(epsilon))
   133        99        383.0      3.9      0.0                          elapsed_time = time.time() - start_time
   134        99      13334.0    134.7      0.0                          print("Elapsed time: ", time.strftime("%H:%M:%S", time.gmtime(elapsed_time)))
   135                                           
   136                                                                   #print("Episode {} finished after: {}".format(i_episode,step))
   137      1000       2844.0      2.8      0.0                      break
   138                                           
   139         1          1.0      1.0      0.0          if solved:
   140         1       2384.0   2384.0      0.0              print("Solved after %i episodes" % solved_after)