{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, episode,step, info=\"\"):\n",
    "    plt.figure(99999,figsize=[8,6])\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"episode: {} step: {} \".format(episode,step))\n",
    "    #plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device=torch.device(\"cuda:4\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/lib/python3.6/site-packages/gym/__init__.py'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PARAMS ######\n",
    "learning_rate = 0.001\n",
    "num_episodes = 10000\n",
    "gamma=0.9999\n",
    "#gamma=0.85\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval=10\n",
    "\n",
    "score_to_solve = 195\n",
    "\n",
    "hidden_layer_size=64\n",
    "\n",
    "replay_memory_size=500000\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "update_target_frequency = 1500\n",
    "\n",
    "clip_error=False\n",
    "\n",
    "double_dqn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.wrappers.Monitor(env, '../mp4/sandbox10',video_callable=lambda episode_id: True,force=True)\n",
    "#env = gym.wrappers.Monitor(env, '../mp4/cartpole-5',video_callable=lambda episode_id: episode_id%10==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs=env.observation_space.shape[0]\n",
    "number_of_outputs=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=torch.arange(20,dtype=torch.float32).view(-1,4).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity=capacity\n",
    "        self.memory=[]\n",
    "        self.position=0\n",
    "        \n",
    "    \n",
    "    def push(self, state,\n",
    "             action, new_state,\n",
    "             reward, done):\n",
    "        \n",
    "            transition=(state,action,new_state,reward,done)\n",
    "            \n",
    "            if self.position>=len(self.memory):\n",
    "                self.memory.append(transition)\n",
    "            else:\n",
    "                self.memory[self.position]=transition\n",
    "                \n",
    "            self.position=(self.position+1)%self.capacity\n",
    "        \n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkDueling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkDueling, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer_size)\n",
    "        self.advantage = nn.Linear(hidden_layer_size,number_of_outputs)\n",
    "        self.value = nn.Linear(hidden_layer_size,1)\n",
    "        \n",
    "        self.activation=nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print('x shape {} and value:'.format(x.shape))\n",
    "        #print(x.detach().cpu())\n",
    "        \n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        \n",
    "        #print('output1 shape {} and value:'.format(output1.shape))\n",
    "        #print(output1.detach().cpu())\n",
    "        \n",
    "        output_advantage=self.advantage(output1)\n",
    "        output_value=self.value(output1)\n",
    "        \n",
    "        #print('output_advantage shape {} and value:'.format(output_advantage.shape))\n",
    "        #print(output_advantage.detach().cpu())\n",
    "        \n",
    "        #print('output_value shape {} and value:'.format(output_value.shape))\n",
    "        #print(output_value.detach().cpu())\n",
    "        \n",
    "        #print('output_advantage.mean shape {} and value:'.format(output_advantage.mean(dim=1,keepdim=True).shape))\n",
    "        #print(output_advantage.mean(dim=1,keepdim=True).detach().cpu())\n",
    "        \n",
    "        output_final=output_value+output_advantage-output_advantage.mean(dim=1,keepdim=True)\n",
    "        \n",
    "        \n",
    "        return output_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer_size)\n",
    "        self.linear2 = nn.Linear(hidden_layer_size,number_of_outputs)\n",
    "        \n",
    "        self.activation=nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_Agent():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.nn = NeuralNetworkDueling().to(device)\n",
    "        self.target_nn = NeuralNetworkDueling().to(device)\n",
    "        \n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy=torch.rand(1).item()\n",
    "        \n",
    "        if random_for_egreedy>epsilon:\n",
    "            self.nn.eval()\n",
    "            with torch.no_grad():\n",
    "                state=torch.Tensor(state).to(device)\n",
    "                predicted_value_from_nn=self.nn(state.unsqueeze(dim=0)).squeeze()\n",
    "                action=torch.argmax(predicted_value_from_nn).item()\n",
    "        else:\n",
    "            action=env.action_space.sample()\n",
    "                \n",
    "                \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if len(memory)<batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state=torch.Tensor(state).to(device)\n",
    "        new_state=torch.Tensor(new_state).to(device)\n",
    "        reward=torch.Tensor(reward).to(device)\n",
    "        \n",
    "        #the view call below is to transform into column vector\n",
    "        #so that it can be used in the gather call\n",
    "        #i.e. we will use it to pick out from the computed value\n",
    "        #tensor only values indexed by selected action\n",
    "        action=(torch.Tensor(action).view(-1,1).long()).to(device)\n",
    "        #print('action: ')\n",
    "        #print(action)\n",
    "        #print('contiguous?', action.is_contiguous())\n",
    "        done=torch.Tensor(done).to(device)\n",
    "        \n",
    "        #print('shape of: state, new state, reward, action, done:')\n",
    "        #print(state.shape)\n",
    "        #print(new_state.shape)\n",
    "        #print(reward.shape)\n",
    "        #print(action.shape)\n",
    "        #print(done.shape)\n",
    "        \n",
    "        \n",
    "        self.nn.eval()\n",
    "        self.target_nn.eval()\n",
    "            \n",
    "            \n",
    "        if double_dqn:\n",
    "            #print('in double DQN')\n",
    "            new_state_values_from_nn=self.nn(new_state).detach()\n",
    "            #print('new_state_values_from_nn shape {} and value:'.format(new_state_values_from_nn.shape))\n",
    "            #print(new_state_values_from_nn)\n",
    "            max_new_state_indexes=torch.max(new_state_values_from_nn,dim=1)[1].view(-1,1)\n",
    "            #print('max_new_state_indexes shape {} and value:'.format(max_new_state_indexes.shape))\n",
    "            #print(max_new_state_indexes)\n",
    "            new_state_values=self.target_nn(new_state).detach()\n",
    "            #print('new_state_values shape {} and value:'.format(new_state_values.shape))\n",
    "            #print(new_state_values)\n",
    "            max_new_state_values=torch.gather(new_state_values,1,max_new_state_indexes).squeeze()\n",
    "            #print('max_new_state_values shape {} and value:'.format(max_new_state_values.shape))\n",
    "            #print(max_new_state_values)\n",
    "        else:\n",
    "            #print('in regular DQN')\n",
    "            new_state_values=self.target_nn(new_state).detach()\n",
    "            #print('new_state_values shape {} and value'.format(new_state_values.shape))\n",
    "            #print(new_state_values)\n",
    "        \n",
    "            max_new_state_values=torch.max(new_state_values,dim=1)[0]\n",
    "            #print('max_new_state_values shape {} and value'.format(max_new_state_values.shape))\n",
    "            #print(max_new_state_values)\n",
    "            \n",
    "        target_value=(reward + (1-done)*gamma*max_new_state_values).view(-1,1)\n",
    "        \n",
    "        #print('shape of: target_value')\n",
    "        #print(target_value.shape)\n",
    "        self.nn.train()\n",
    "        \n",
    "        #this will select only the values of the desired actions\n",
    "        predicted_value=torch.gather(self.nn(state),1,action)\n",
    "        #print('shape of: predicted_value')\n",
    "        #print(predicted_value.shape)\n",
    "        \n",
    "        \n",
    "        loss=self.loss_function(predicted_value,target_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.clamp_(-1.0,1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            #print(\"***********************\")\n",
    "            #print(\"UPDATING TARGET NETWORK\")\n",
    "            #print(\"update counter: {}\".format(self.update_target_counter))\n",
    "            #print(\"***********************\")\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ExperienceReplay(replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_agent=QNet_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value=23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  10 **** \n",
      "Recent average reward: 18.3\n",
      "Average over all episodes so far: 18.3\n",
      "epsilon: 0.6077792046191888\n",
      "**** Episode  20 **** \n",
      "Recent average reward: 16.6\n",
      "Average over all episodes so far: 17.45\n",
      "epsilon: 0.4329362518006686\n",
      "**** Episode  30 **** \n",
      "Recent average reward: 12.5\n",
      "Average over all episodes so far: 15.8\n",
      "epsilon: 0.3460370637724424\n",
      "**** Episode  40 **** \n",
      "Recent average reward: 10.5\n",
      "Average over all episodes so far: 14.475\n",
      "epsilon: 0.28347807735500685\n",
      "**** Episode  50 **** \n",
      "Recent average reward: 12.7\n",
      "Average over all episodes so far: 14.12\n",
      "epsilon: 0.2217108572251238\n",
      "**** Episode  60 **** \n",
      "Recent average reward: 10.7\n",
      "Average over all episodes so far: 13.55\n",
      "epsilon: 0.18126660958666668\n",
      "**** Episode  70 **** \n",
      "Recent average reward: 10.3\n",
      "Average over all episodes so far: 13.085714285714285\n",
      "epsilon: 0.1499410780191286\n",
      "**** Episode  80 **** \n",
      "Recent average reward: 9.9\n",
      "Average over all episodes so far: 12.6875\n",
      "epsilon: 0.12480344162250226\n",
      "**** Episode  90 **** \n",
      "Recent average reward: 10.2\n",
      "Average over all episodes so far: 12.411111111111111\n",
      "epsilon: 0.10380530986005926\n",
      "**** Episode  100 **** \n",
      "Recent average reward: 9.2\n",
      "Average over all episodes so far: 12.09\n",
      "epsilon: 0.08788407184590553\n",
      "**** Episode  110 **** \n",
      "Recent average reward: 9.6\n",
      "Reward over last 100: 11.22\n",
      "Average over all episodes so far: 11.863636363636363\n",
      "epsilon: 0.07414983139250182\n",
      "**** Episode  120 **** \n",
      "Recent average reward: 11.8\n",
      "Reward over last 100: 10.74\n",
      "Average over all episodes so far: 11.858333333333333\n",
      "epsilon: 0.06086736011346172\n",
      "**** Episode  130 **** \n",
      "Recent average reward: 11.4\n",
      "Reward over last 100: 10.63\n",
      "Average over all episodes so far: 11.823076923076924\n",
      "epsilon: 0.050415826880783145\n",
      "**** Episode  140 **** \n",
      "Recent average reward: 13.8\n",
      "Reward over last 100: 10.96\n",
      "Average over all episodes so far: 11.964285714285714\n",
      "epsilon: 0.040606777235690714\n",
      "**** Episode  150 **** \n",
      "Recent average reward: 15.4\n",
      "Reward over last 100: 11.23\n",
      "Average over all episodes so far: 12.193333333333333\n",
      "epsilon: 0.03249338942540037\n",
      "**** Episode  160 **** \n",
      "Recent average reward: 10.8\n",
      "Reward over last 100: 11.24\n",
      "Average over all episodes so far: 12.10625\n",
      "epsilon: 0.02816000162628325\n",
      "**** Episode  170 **** \n",
      "Recent average reward: 13.9\n",
      "Reward over last 100: 11.6\n",
      "Average over all episodes so far: 12.211764705882352\n",
      "epsilon: 0.023670243492076304\n",
      "**** Episode  180 **** \n",
      "Recent average reward: 11.4\n",
      "Reward over last 100: 11.75\n",
      "Average over all episodes so far: 12.166666666666666\n",
      "epsilon: 0.020926832513730908\n",
      "**** Episode  190 **** \n",
      "Recent average reward: 10.1\n",
      "Reward over last 100: 11.74\n",
      "Average over all episodes so far: 12.057894736842105\n",
      "epsilon: 0.018910420751213415\n",
      "**** Episode  200 **** \n",
      "Recent average reward: 9.9\n",
      "Reward over last 100: 11.81\n",
      "Average over all episodes so far: 11.95\n",
      "epsilon: 0.01733913848209568\n",
      "**** Episode  210 **** \n",
      "Recent average reward: 13.5\n",
      "Reward over last 100: 12.2\n",
      "Average over all episodes so far: 12.023809523809524\n",
      "epsilon: 0.015591353925312995\n",
      "**** Episode  220 **** \n",
      "Recent average reward: 11.1\n",
      "Reward over last 100: 12.13\n",
      "Average over all episodes so far: 11.981818181818182\n",
      "epsilon: 0.014478201266216087\n",
      "**** Episode  230 **** \n",
      "Recent average reward: 9.7\n",
      "Reward over last 100: 11.96\n",
      "Average over all episodes so far: 11.882608695652173\n",
      "epsilon: 0.013703289440734122\n",
      "**** Episode  240 **** \n",
      "Recent average reward: 13.0\n",
      "Reward over last 100: 11.88\n",
      "Average over all episodes so far: 11.929166666666667\n",
      "epsilon: 0.012844028300173886\n",
      "**** Episode  250 **** \n",
      "Recent average reward: 11.9\n",
      "Reward over last 100: 11.53\n",
      "Average over all episodes so far: 11.928\n",
      "epsilon: 0.012246158587595295\n",
      "**** Episode  260 **** \n",
      "Recent average reward: 11.5\n",
      "Reward over last 100: 11.6\n",
      "Average over all episodes so far: 11.911538461538461\n",
      "epsilon: 0.011773972643070965\n",
      "**** Episode  270 **** \n",
      "Recent average reward: 9.8\n",
      "Reward over last 100: 11.19\n",
      "Average over all episodes so far: 11.833333333333334\n",
      "epsilon: 0.01146114658942162\n",
      "**** Episode  280 **** \n",
      "Recent average reward: 10.6\n",
      "Reward over last 100: 11.11\n",
      "Average over all episodes so far: 11.789285714285715\n",
      "epsilon: 0.011182016008811784\n",
      "**** Episode  290 **** \n",
      "Recent average reward: 12.2\n",
      "Reward over last 100: 11.32\n",
      "Average over all episodes so far: 11.803448275862069\n",
      "epsilon: 0.010926094926404775\n",
      "**** Episode  300 **** \n",
      "Recent average reward: 9.5\n",
      "Reward over last 100: 11.28\n",
      "Average over all episodes so far: 11.726666666666667\n",
      "epsilon: 0.010765842658289036\n",
      "**** Episode  310 **** \n",
      "Recent average reward: 12.0\n",
      "Reward over last 100: 11.13\n",
      "Average over all episodes so far: 11.735483870967743\n",
      "epsilon: 0.010606058636753912\n",
      "**** Episode  320 **** \n",
      "Recent average reward: 11.3\n",
      "Reward over last 100: 11.15\n",
      "Average over all episodes so far: 11.721875\n",
      "epsilon: 0.010481533951988345\n",
      "**** Episode  330 **** \n",
      "Recent average reward: 12.9\n",
      "Reward over last 100: 11.47\n",
      "Average over all episodes so far: 11.757575757575758\n",
      "epsilon: 0.010371287517298872\n",
      "**** Episode  340 **** \n",
      "Recent average reward: 9.5\n",
      "Reward over last 100: 11.12\n",
      "Average over all episodes so far: 11.691176470588236\n",
      "epsilon: 0.01030827022175965\n",
      "**** Episode  350 **** \n",
      "Recent average reward: 9.6\n",
      "Reward over last 100: 10.89\n",
      "Average over all episodes so far: 11.631428571428572\n",
      "epsilon: 0.010254417531369693\n",
      "**** Episode  360 **** \n",
      "Recent average reward: 10.9\n",
      "Reward over last 100: 10.83\n",
      "Average over all episodes so far: 11.61111111111111\n",
      "epsilon: 0.010203766909830082\n",
      "**** Episode  370 **** \n",
      "Recent average reward: 10.5\n",
      "Reward over last 100: 10.9\n",
      "Average over all episodes so far: 11.58108108108108\n",
      "epsilon: 0.01016583225107168\n",
      "**** Episode  380 **** \n",
      "Recent average reward: 9.7\n",
      "Reward over last 100: 10.81\n",
      "Average over all episodes so far: 11.531578947368422\n",
      "epsilon: 0.010136316139285147\n",
      "**** Episode  390 **** \n",
      "Recent average reward: 9.8\n",
      "Reward over last 100: 10.57\n",
      "Average over all episodes so far: 11.487179487179487\n",
      "epsilon: 0.010112053534276487\n",
      "**** Episode  400 **** \n",
      "Recent average reward: 12.3\n",
      "Reward over last 100: 10.85\n",
      "Average over all episodes so far: 11.5075\n",
      "epsilon: 0.010087968319300271\n",
      "**** Episode  410 **** \n",
      "Recent average reward: 18.2\n",
      "Reward over last 100: 11.47\n",
      "Average over all episodes so far: 11.670731707317072\n",
      "epsilon: 0.010060641334117368\n",
      "**** Episode  420 **** \n",
      "Recent average reward: 12.1\n",
      "Reward over last 100: 11.55\n",
      "Average over all episodes so far: 11.68095238095238\n",
      "epsilon: 0.010047797662742825\n",
      "**** Episode  430 **** \n",
      "Recent average reward: 10.8\n",
      "Reward over last 100: 11.34\n",
      "Average over all episodes so far: 11.66046511627907\n",
      "epsilon: 0.010038589365823278\n",
      "**** Episode  440 **** \n",
      "Recent average reward: 9.4\n",
      "Reward over last 100: 11.33\n",
      "Average over all episodes so far: 11.60909090909091\n",
      "epsilon: 0.010031911828540643\n",
      "**** Episode  450 **** \n",
      "Recent average reward: 10.4\n",
      "Reward over last 100: 11.41\n",
      "Average over all episodes so far: 11.582222222222223\n",
      "epsilon: 0.010025919011695054\n",
      "**** Episode  460 **** \n",
      "Recent average reward: 10.6\n",
      "Reward over last 100: 11.38\n",
      "Average over all episodes so far: 11.560869565217391\n",
      "epsilon: 0.010021009542551128\n",
      "**** Episode  470 **** \n",
      "Recent average reward: 11.4\n",
      "Reward over last 100: 11.47\n",
      "Average over all episodes so far: 11.557446808510639\n",
      "epsilon: 0.010016626149744396\n",
      "**** Episode  480 **** \n",
      "Recent average reward: 12.2\n",
      "Reward over last 100: 11.72\n",
      "Average over all episodes so far: 11.570833333333333\n",
      "epsilon: 0.01001305246156574\n",
      "**** Episode  490 **** \n",
      "Recent average reward: 12.9\n",
      "Reward over last 100: 12.03\n",
      "Average over all episodes so far: 11.597959183673469\n",
      "epsilon: 0.010010124687433071\n",
      "**** Episode  500 **** \n",
      "Recent average reward: 11.8\n",
      "Reward over last 100: 11.98\n",
      "Average over all episodes so far: 11.602\n",
      "epsilon: 0.010007932567404587\n",
      "**** Episode  510 **** \n",
      "Recent average reward: 10.0\n",
      "Reward over last 100: 11.16\n",
      "Average over all episodes so far: 11.570588235294117\n",
      "epsilon: 0.010006468710225348\n",
      "**** Episode  520 **** \n",
      "Recent average reward: 13.1\n",
      "Reward over last 100: 11.26\n",
      "Average over all episodes so far: 11.6\n",
      "epsilon: 0.010005017725491618\n",
      "**** Episode  530 **** \n",
      "Recent average reward: 13.1\n",
      "Reward over last 100: 11.49\n",
      "Average over all episodes so far: 11.628301886792453\n",
      "epsilon: 0.010003815137636131\n",
      "**** Episode  540 **** \n",
      "Recent average reward: 16.5\n",
      "Reward over last 100: 12.2\n",
      "Average over all episodes so far: 11.718518518518518\n",
      "epsilon: 0.01000278146214731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  550 **** \n",
      "Recent average reward: 12.5\n",
      "Reward over last 100: 12.41\n",
      "Average over all episodes so far: 11.732727272727272\n",
      "epsilon: 0.010002166204898409\n",
      "**** Episode  560 **** \n",
      "Recent average reward: 15.1\n",
      "Reward over last 100: 12.86\n",
      "Average over all episodes so far: 11.792857142857143\n",
      "epsilon: 0.01000152344884518\n",
      "**** Episode  570 **** \n",
      "Recent average reward: 15.0\n",
      "Reward over last 100: 13.22\n",
      "Average over all episodes so far: 11.849122807017544\n",
      "epsilon: 0.010001188838454411\n",
      "**** Episode  580 **** \n",
      "Recent average reward: 10.8\n",
      "Reward over last 100: 13.08\n",
      "Average over all episodes so far: 11.831034482758621\n",
      "epsilon: 0.010000918490921624\n",
      "**** Episode  590 **** \n",
      "Recent average reward: 12.6\n",
      "Reward over last 100: 13.05\n",
      "Average over all episodes so far: 11.844067796610169\n",
      "epsilon: 0.01000071818846501\n",
      "**** Episode  600 **** \n",
      "Recent average reward: 19.4\n",
      "Reward over last 100: 13.81\n",
      "Average over all episodes so far: 11.97\n",
      "epsilon: 0.01000049607742865\n",
      "**** Episode  610 **** \n",
      "Recent average reward: 15.4\n",
      "Reward over last 100: 14.35\n",
      "Average over all episodes so far: 12.026229508196721\n",
      "epsilon: 0.010000364574901234\n",
      "**** Episode  620 **** \n",
      "Recent average reward: 13.9\n",
      "Reward over last 100: 14.43\n",
      "Average over all episodes so far: 12.056451612903226\n",
      "epsilon: 0.010000266328902541\n",
      "**** Episode  630 **** \n",
      "Recent average reward: 41.1\n",
      "Reward over last 100: 17.23\n",
      "Average over all episodes so far: 12.517460317460317\n",
      "epsilon: 0.010000117065314274\n",
      "**** Episode  640 **** \n",
      "Recent average reward: 45.6\n",
      "Reward over last 100: 20.14\n",
      "Average over all episodes so far: 13.034375\n",
      "epsilon: 0.010000042382350792\n",
      "**** Episode  650 **** \n",
      "Recent average reward: 66.0\n",
      "Reward over last 100: 25.49\n",
      "Average over all episodes so far: 13.849230769230768\n",
      "epsilon: 0.010000011596832734\n",
      "**** Episode  660 **** \n",
      "Recent average reward: 75.1\n",
      "Reward over last 100: 31.49\n",
      "Average over all episodes so far: 14.777272727272727\n",
      "epsilon: 0.010000002427184688\n",
      "**** Episode  670 **** \n",
      "Recent average reward: 118.6\n",
      "Reward over last 100: 41.85\n",
      "Average over all episodes so far: 16.32686567164179\n",
      "epsilon: 0.010000000216694234\n",
      "**** Episode  680 **** \n",
      "Recent average reward: 112.5\n",
      "Reward over last 100: 52.02\n",
      "Average over all episodes so far: 17.741176470588236\n",
      "epsilon: 0.010000000024495448\n",
      "**** Episode  690 **** \n",
      "Recent average reward: 110.1\n",
      "Reward over last 100: 61.77\n",
      "Average over all episodes so far: 19.079710144927535\n",
      "epsilon: 0.010000000002195671\n",
      "**** Episode  700 **** \n",
      "Recent average reward: 158.1\n",
      "Reward over last 100: 75.64\n",
      "Average over all episodes so far: 21.065714285714286\n",
      "epsilon: 0.010000000000099112\n",
      "**** Episode  710 **** \n",
      "Recent average reward: 162.0\n",
      "Reward over last 100: 90.3\n",
      "Average over all episodes so far: 23.050704225352113\n",
      "epsilon: 0.010000000000003634\n",
      "**** Episode  720 **** \n",
      "Recent average reward: 177.5\n",
      "Reward over last 100: 106.66\n",
      "Average over all episodes so far: 25.195833333333333\n",
      "epsilon: 0.010000000000000113\n",
      "**** Episode  730 **** \n",
      "Recent average reward: 192.5\n",
      "Reward over last 100: 121.8\n",
      "Average over all episodes so far: 27.487671232876714\n",
      "epsilon: 0.010000000000000002\n",
      "**** Episode  740 **** \n",
      "Recent average reward: 169.2\n",
      "Reward over last 100: 134.16\n",
      "Average over all episodes so far: 29.4027027027027\n",
      "epsilon: 0.01\n",
      "**** Episode  750 **** \n",
      "Recent average reward: 176.1\n",
      "Reward over last 100: 145.17\n",
      "Average over all episodes so far: 31.358666666666668\n",
      "epsilon: 0.01\n",
      "**** Episode  760 **** \n",
      "Recent average reward: 195.1\n",
      "Reward over last 100: 157.17\n",
      "Average over all episodes so far: 33.51315789473684\n",
      "epsilon: 0.01\n",
      "**** Episode  770 **** \n",
      "Recent average reward: 193.3\n",
      "Reward over last 100: 164.64\n",
      "Average over all episodes so far: 35.58831168831169\n",
      "epsilon: 0.01\n",
      "**** Episode  780 **** \n",
      "Recent average reward: 185.8\n",
      "Reward over last 100: 171.97\n",
      "Average over all episodes so far: 37.514102564102565\n",
      "epsilon: 0.01\n",
      "**** Episode  790 **** \n",
      "Recent average reward: 180.5\n",
      "Reward over last 100: 179.01\n",
      "Average over all episodes so far: 39.324050632911394\n",
      "epsilon: 0.01\n",
      "**** Episode  800 **** \n",
      "Recent average reward: 186.2\n",
      "Reward over last 100: 181.82\n",
      "Average over all episodes so far: 41.16\n",
      "epsilon: 0.01\n",
      "**** Episode  810 **** \n",
      "Recent average reward: 185.3\n",
      "Reward over last 100: 184.15\n",
      "Average over all episodes so far: 42.93950617283951\n",
      "epsilon: 0.01\n",
      "**** Episode  820 **** \n",
      "Recent average reward: 186.7\n",
      "Reward over last 100: 185.07\n",
      "Average over all episodes so far: 44.69268292682927\n",
      "epsilon: 0.01\n",
      "**** Episode  830 **** \n",
      "Recent average reward: 180.7\n",
      "Reward over last 100: 183.89\n",
      "Average over all episodes so far: 46.33132530120482\n",
      "epsilon: 0.01\n",
      "**** Episode  840 **** \n",
      "Recent average reward: 178.3\n",
      "Reward over last 100: 184.8\n",
      "Average over all episodes so far: 47.90238095238095\n",
      "epsilon: 0.01\n",
      "**** Episode  850 **** \n",
      "Recent average reward: 165.7\n",
      "Reward over last 100: 183.76\n",
      "Average over all episodes so far: 49.28823529411765\n",
      "epsilon: 0.01\n",
      "**** Episode  860 **** \n",
      "Recent average reward: 179.5\n",
      "Reward over last 100: 182.2\n",
      "Average over all episodes so far: 50.80232558139535\n",
      "epsilon: 0.01\n",
      "**** Episode  870 **** \n",
      "Recent average reward: 170.2\n",
      "Reward over last 100: 179.89\n",
      "Average over all episodes so far: 52.17471264367816\n",
      "epsilon: 0.01\n",
      "**** Episode  880 **** \n",
      "Recent average reward: 184.1\n",
      "Reward over last 100: 179.72\n",
      "Average over all episodes so far: 53.673863636363635\n",
      "epsilon: 0.01\n",
      "**** Episode  890 **** \n",
      "Recent average reward: 171.4\n",
      "Reward over last 100: 178.81\n",
      "Average over all episodes so far: 54.996629213483146\n",
      "epsilon: 0.01\n",
      "**** Episode  900 **** \n",
      "Recent average reward: 174.3\n",
      "Reward over last 100: 177.62\n",
      "Average over all episodes so far: 56.32222222222222\n",
      "epsilon: 0.01\n",
      "**** Episode  910 **** \n",
      "Recent average reward: 177.9\n",
      "Reward over last 100: 176.88\n",
      "Average over all episodes so far: 57.65824175824176\n",
      "epsilon: 0.01\n",
      "**** Episode  920 **** \n",
      "Recent average reward: 178.7\n",
      "Reward over last 100: 176.08\n",
      "Average over all episodes so far: 58.97391304347826\n",
      "epsilon: 0.01\n",
      "**** Episode  930 **** \n",
      "Recent average reward: 169.6\n",
      "Reward over last 100: 174.97\n",
      "Average over all episodes so far: 60.163440860215054\n",
      "epsilon: 0.01\n",
      "**** Episode  940 **** \n",
      "Recent average reward: 181.7\n",
      "Reward over last 100: 175.31\n",
      "Average over all episodes so far: 61.456382978723404\n",
      "epsilon: 0.01\n",
      "**** Episode  950 **** \n",
      "Recent average reward: 168.6\n",
      "Reward over last 100: 175.6\n",
      "Average over all episodes so far: 62.584210526315786\n",
      "epsilon: 0.01\n",
      "**** Episode  960 **** \n",
      "Recent average reward: 173.7\n",
      "Reward over last 100: 175.02\n",
      "Average over all episodes so far: 63.74166666666667\n",
      "epsilon: 0.01\n",
      "**** Episode  970 **** \n",
      "Recent average reward: 174.8\n",
      "Reward over last 100: 175.48\n",
      "Average over all episodes so far: 64.88659793814433\n",
      "epsilon: 0.01\n",
      "**** Episode  980 **** \n",
      "Recent average reward: 175.0\n",
      "Reward over last 100: 174.57\n",
      "Average over all episodes so far: 66.01020408163265\n",
      "epsilon: 0.01\n",
      "**** Episode  990 **** \n",
      "Recent average reward: 167.5\n",
      "Reward over last 100: 174.18\n",
      "Average over all episodes so far: 67.03535353535354\n",
      "epsilon: 0.01\n",
      "**** Episode  1000 **** \n",
      "Recent average reward: 167.4\n",
      "Reward over last 100: 173.49\n",
      "Average over all episodes so far: 68.039\n",
      "epsilon: 0.01\n",
      "**** Episode  1010 **** \n",
      "Recent average reward: 164.5\n",
      "Reward over last 100: 172.15\n",
      "Average over all episodes so far: 68.99405940594059\n",
      "epsilon: 0.01\n",
      "**** Episode  1020 **** \n",
      "Recent average reward: 166.3\n",
      "Reward over last 100: 170.91\n",
      "Average over all episodes so far: 69.94803921568628\n",
      "epsilon: 0.01\n",
      "**** Episode  1030 **** \n",
      "Recent average reward: 165.0\n",
      "Reward over last 100: 170.45\n",
      "Average over all episodes so far: 70.87087378640777\n",
      "epsilon: 0.01\n",
      "**** Episode  1040 **** \n",
      "Recent average reward: 171.9\n",
      "Reward over last 100: 169.47\n",
      "Average over all episodes so far: 71.8423076923077\n",
      "epsilon: 0.01\n",
      "**** Episode  1050 **** \n",
      "Recent average reward: 169.2\n",
      "Reward over last 100: 169.53\n",
      "Average over all episodes so far: 72.7695238095238\n",
      "epsilon: 0.01\n",
      "**** Episode  1060 **** \n",
      "Recent average reward: 180.1\n",
      "Reward over last 100: 170.17\n",
      "Average over all episodes so far: 73.7820754716981\n",
      "epsilon: 0.01\n",
      "**** Episode  1070 **** \n",
      "Recent average reward: 170.7\n",
      "Reward over last 100: 169.76\n",
      "Average over all episodes so far: 74.68785046728972\n",
      "epsilon: 0.01\n",
      "**** Episode  1080 **** \n",
      "Recent average reward: 176.6\n",
      "Reward over last 100: 169.92\n",
      "Average over all episodes so far: 75.63148148148149\n",
      "epsilon: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  1090 **** \n",
      "Recent average reward: 175.0\n",
      "Reward over last 100: 170.67\n",
      "Average over all episodes so far: 76.54311926605504\n",
      "epsilon: 0.01\n",
      "**** Episode  1100 **** \n",
      "Recent average reward: 169.9\n",
      "Reward over last 100: 170.92\n",
      "Average over all episodes so far: 77.39181818181818\n",
      "epsilon: 0.01\n",
      "**** Episode  1110 **** \n",
      "Recent average reward: 167.4\n",
      "Reward over last 100: 171.21\n",
      "Average over all episodes so far: 78.20270270270271\n",
      "epsilon: 0.01\n",
      "**** Episode  1120 **** \n",
      "Recent average reward: 166.0\n",
      "Reward over last 100: 171.18\n",
      "Average over all episodes so far: 78.98660714285714\n",
      "epsilon: 0.01\n",
      "**** Episode  1130 **** \n",
      "Recent average reward: 168.8\n",
      "Reward over last 100: 171.56\n",
      "Average over all episodes so far: 79.78141592920353\n",
      "epsilon: 0.01\n",
      "**** Episode  1140 **** \n",
      "Recent average reward: 175.2\n",
      "Reward over last 100: 171.89\n",
      "Average over all episodes so far: 80.61842105263158\n",
      "epsilon: 0.01\n",
      "**** Episode  1150 **** \n",
      "Recent average reward: 175.3\n",
      "Reward over last 100: 172.5\n",
      "Average over all episodes so far: 81.44173913043478\n",
      "epsilon: 0.01\n",
      "**** Episode  1160 **** \n",
      "Recent average reward: 175.2\n",
      "Reward over last 100: 172.01\n",
      "Average over all episodes so far: 82.25\n",
      "epsilon: 0.01\n",
      "**** Episode  1170 **** \n",
      "Recent average reward: 173.9\n",
      "Reward over last 100: 172.33\n",
      "Average over all episodes so far: 83.03333333333333\n",
      "epsilon: 0.01\n",
      "**** Episode  1180 **** \n",
      "Recent average reward: 172.8\n",
      "Reward over last 100: 171.95\n",
      "Average over all episodes so far: 83.79406779661016\n",
      "epsilon: 0.01\n",
      "**** Episode  1190 **** \n",
      "Recent average reward: 174.9\n",
      "Reward over last 100: 171.94\n",
      "Average over all episodes so far: 84.55966386554621\n",
      "epsilon: 0.01\n",
      "**** Episode  1200 **** \n",
      "Recent average reward: 168.1\n",
      "Reward over last 100: 171.76\n",
      "Average over all episodes so far: 85.25583333333333\n",
      "epsilon: 0.01\n",
      "**** Episode  1210 **** \n",
      "Recent average reward: 169.0\n",
      "Reward over last 100: 171.92\n",
      "Average over all episodes so far: 85.94793388429753\n",
      "epsilon: 0.01\n",
      "**** Episode  1220 **** \n",
      "Recent average reward: 168.9\n",
      "Reward over last 100: 172.21\n",
      "Average over all episodes so far: 86.62786885245902\n",
      "epsilon: 0.01\n",
      "**** Episode  1230 **** \n",
      "Recent average reward: 169.8\n",
      "Reward over last 100: 172.31\n",
      "Average over all episodes so far: 87.3040650406504\n",
      "epsilon: 0.01\n",
      "**** Episode  1240 **** \n",
      "Recent average reward: 174.2\n",
      "Reward over last 100: 172.21\n",
      "Average over all episodes so far: 88.00483870967741\n",
      "epsilon: 0.01\n",
      "**** Episode  1250 **** \n",
      "Recent average reward: 169.2\n",
      "Reward over last 100: 171.6\n",
      "Average over all episodes so far: 88.6544\n",
      "epsilon: 0.01\n",
      "**** Episode  1260 **** \n",
      "Recent average reward: 176.0\n",
      "Reward over last 100: 171.68\n",
      "Average over all episodes so far: 89.34761904761905\n",
      "epsilon: 0.01\n",
      "**** Episode  1270 **** \n",
      "Recent average reward: 168.4\n",
      "Reward over last 100: 171.13\n",
      "Average over all episodes so far: 89.97007874015748\n",
      "epsilon: 0.01\n",
      "**** Episode  1280 **** \n",
      "Recent average reward: 179.0\n",
      "Reward over last 100: 171.75\n",
      "Average over all episodes so far: 90.665625\n",
      "epsilon: 0.01\n",
      "**** Episode  1290 **** \n",
      "Recent average reward: 172.6\n",
      "Reward over last 100: 171.52\n",
      "Average over all episodes so far: 91.30077519379844\n",
      "epsilon: 0.01\n",
      "**** Episode  1300 **** \n",
      "Recent average reward: 175.1\n",
      "Reward over last 100: 172.22\n",
      "Average over all episodes so far: 91.94538461538461\n",
      "epsilon: 0.01\n",
      "**** Episode  1310 **** \n",
      "Recent average reward: 178.6\n",
      "Reward over last 100: 173.18\n",
      "Average over all episodes so far: 92.60687022900764\n",
      "epsilon: 0.01\n",
      "**** Episode  1320 **** \n",
      "Recent average reward: 180.0\n",
      "Reward over last 100: 174.29\n",
      "Average over all episodes so far: 93.26893939393939\n",
      "epsilon: 0.01\n",
      "**** Episode  1330 **** \n",
      "Recent average reward: 173.3\n",
      "Reward over last 100: 174.64\n",
      "Average over all episodes so far: 93.87067669172933\n",
      "epsilon: 0.01\n",
      "**** Episode  1340 **** \n",
      "Recent average reward: 178.5\n",
      "Reward over last 100: 175.07\n",
      "Average over all episodes so far: 94.50223880597015\n",
      "epsilon: 0.01\n",
      "**** Episode  1350 **** \n",
      "Recent average reward: 178.3\n",
      "Reward over last 100: 175.98\n",
      "Average over all episodes so far: 95.12296296296296\n",
      "epsilon: 0.01\n",
      "**** Episode  1360 **** \n",
      "Recent average reward: 174.1\n",
      "Reward over last 100: 175.79\n",
      "Average over all episodes so far: 95.70367647058823\n",
      "epsilon: 0.01\n",
      "**** Episode  1370 **** \n",
      "Recent average reward: 173.4\n",
      "Reward over last 100: 176.29\n",
      "Average over all episodes so far: 96.27080291970803\n",
      "epsilon: 0.01\n",
      "**** Episode  1380 **** \n",
      "Recent average reward: 170.0\n",
      "Reward over last 100: 175.39\n",
      "Average over all episodes so far: 96.80507246376811\n",
      "epsilon: 0.01\n",
      "**** Episode  1390 **** \n",
      "Recent average reward: 171.3\n",
      "Reward over last 100: 175.26\n",
      "Average over all episodes so far: 97.34100719424461\n",
      "epsilon: 0.01\n",
      "**** Episode  1400 **** \n",
      "Recent average reward: 176.1\n",
      "Reward over last 100: 175.36\n",
      "Average over all episodes so far: 97.90357142857142\n",
      "epsilon: 0.01\n",
      "**** Episode  1410 **** \n",
      "Recent average reward: 173.8\n",
      "Reward over last 100: 174.88\n",
      "Average over all episodes so far: 98.4418439716312\n",
      "epsilon: 0.01\n",
      "**** Episode  1420 **** \n",
      "Recent average reward: 179.8\n",
      "Reward over last 100: 174.86\n",
      "Average over all episodes so far: 99.01478873239436\n",
      "epsilon: 0.01\n",
      "**** Episode  1430 **** \n",
      "Recent average reward: 179.6\n",
      "Reward over last 100: 175.49\n",
      "Average over all episodes so far: 99.57832167832167\n",
      "epsilon: 0.01\n",
      "**** Episode  1440 **** \n",
      "Recent average reward: 167.5\n",
      "Reward over last 100: 174.39\n",
      "Average over all episodes so far: 100.05\n",
      "epsilon: 0.01\n",
      "**** Episode  1450 **** \n",
      "Recent average reward: 168.5\n",
      "Reward over last 100: 173.41\n",
      "Average over all episodes so far: 100.52206896551724\n",
      "epsilon: 0.01\n",
      "**** Episode  1460 **** \n",
      "Recent average reward: 170.0\n",
      "Reward over last 100: 173.0\n",
      "Average over all episodes so far: 100.99794520547945\n",
      "epsilon: 0.01\n",
      "**** Episode  1470 **** \n",
      "Recent average reward: 171.9\n",
      "Reward over last 100: 172.85\n",
      "Average over all episodes so far: 101.48027210884354\n",
      "epsilon: 0.01\n",
      "**** Episode  1480 **** \n",
      "Recent average reward: 176.0\n",
      "Reward over last 100: 173.45\n",
      "Average over all episodes so far: 101.98378378378378\n",
      "epsilon: 0.01\n",
      "**** Episode  1490 **** \n",
      "Recent average reward: 169.1\n",
      "Reward over last 100: 173.23\n",
      "Average over all episodes so far: 102.43422818791946\n",
      "epsilon: 0.01\n",
      "**** Episode  1500 **** \n",
      "Recent average reward: 169.0\n",
      "Reward over last 100: 172.52\n",
      "Average over all episodes so far: 102.878\n",
      "epsilon: 0.01\n",
      "**** Episode  1510 **** \n",
      "Recent average reward: 173.5\n",
      "Reward over last 100: 172.49\n",
      "Average over all episodes so far: 103.34569536423841\n",
      "epsilon: 0.01\n",
      "**** Episode  1520 **** \n",
      "Recent average reward: 163.5\n",
      "Reward over last 100: 170.86\n",
      "Average over all episodes so far: 103.74144736842105\n",
      "epsilon: 0.01\n",
      "**** Episode  1530 **** \n",
      "Recent average reward: 165.9\n",
      "Reward over last 100: 169.49\n",
      "Average over all episodes so far: 104.14771241830066\n",
      "epsilon: 0.01\n",
      "**** Episode  1540 **** \n",
      "Recent average reward: 173.4\n",
      "Reward over last 100: 170.08\n",
      "Average over all episodes so far: 104.59740259740259\n",
      "epsilon: 0.01\n",
      "**** Episode  1550 **** \n",
      "Recent average reward: 175.3\n",
      "Reward over last 100: 170.76\n",
      "Average over all episodes so far: 105.05354838709677\n",
      "epsilon: 0.01\n",
      "**** Episode  1560 **** \n",
      "Recent average reward: 174.6\n",
      "Reward over last 100: 171.22\n",
      "Average over all episodes so far: 105.49935897435897\n",
      "epsilon: 0.01\n",
      "**** Episode  1570 **** \n",
      "Recent average reward: 165.6\n",
      "Reward over last 100: 170.59\n",
      "Average over all episodes so far: 105.88216560509554\n",
      "epsilon: 0.01\n",
      "**** Episode  1580 **** \n",
      "Recent average reward: 174.0\n",
      "Reward over last 100: 170.39\n",
      "Average over all episodes so far: 106.3132911392405\n",
      "epsilon: 0.01\n",
      "**** Episode  1590 **** \n",
      "Recent average reward: 178.6\n",
      "Reward over last 100: 171.34\n",
      "Average over all episodes so far: 106.76792452830189\n",
      "epsilon: 0.01\n",
      "**** Episode  1600 **** \n",
      "Recent average reward: 181.9\n",
      "Reward over last 100: 172.63\n",
      "Average over all episodes so far: 107.2375\n",
      "epsilon: 0.01\n",
      "**** Episode  1610 **** \n",
      "Recent average reward: 168.2\n",
      "Reward over last 100: 172.1\n",
      "Average over all episodes so far: 107.61614906832298\n",
      "epsilon: 0.01\n",
      "**** Episode  1620 **** \n",
      "Recent average reward: 179.1\n",
      "Reward over last 100: 173.66\n",
      "Average over all episodes so far: 108.05740740740741\n",
      "epsilon: 0.01\n",
      "**** Episode  1630 **** \n",
      "Recent average reward: 162.6\n",
      "Reward over last 100: 173.33\n",
      "Average over all episodes so far: 108.3920245398773\n",
      "epsilon: 0.01\n",
      "**** Episode  1640 **** \n",
      "Recent average reward: 181.4\n",
      "Reward over last 100: 174.13\n",
      "Average over all episodes so far: 108.83719512195123\n",
      "epsilon: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  1650 **** \n",
      "Recent average reward: 181.2\n",
      "Reward over last 100: 174.72\n",
      "Average over all episodes so far: 109.27575757575758\n",
      "epsilon: 0.01\n",
      "**** Episode  1660 **** \n",
      "Recent average reward: 173.6\n",
      "Reward over last 100: 174.62\n",
      "Average over all episodes so far: 109.66325301204819\n",
      "epsilon: 0.01\n",
      "**** Episode  1670 **** \n",
      "Recent average reward: 176.1\n",
      "Reward over last 100: 175.67\n",
      "Average over all episodes so far: 110.06107784431137\n",
      "epsilon: 0.01\n",
      "**** Episode  1680 **** \n",
      "Recent average reward: 171.2\n",
      "Reward over last 100: 175.39\n",
      "Average over all episodes so far: 110.425\n",
      "epsilon: 0.01\n",
      "**** Episode  1690 **** \n",
      "Recent average reward: 180.4\n",
      "Reward over last 100: 175.57\n",
      "Average over all episodes so far: 110.83905325443787\n",
      "epsilon: 0.01\n",
      "**** Episode  1700 **** \n",
      "Recent average reward: 186.7\n",
      "Reward over last 100: 176.05\n",
      "Average over all episodes so far: 111.28529411764706\n",
      "epsilon: 0.01\n",
      "**** Episode  1710 **** \n",
      "Recent average reward: 187.0\n",
      "Reward over last 100: 177.93\n",
      "Average over all episodes so far: 111.7280701754386\n",
      "epsilon: 0.01\n",
      "**** Episode  1720 **** \n",
      "Recent average reward: 188.7\n",
      "Reward over last 100: 178.89\n",
      "Average over all episodes so far: 112.17558139534884\n",
      "epsilon: 0.01\n",
      "**** Episode  1730 **** \n",
      "Recent average reward: 177.0\n",
      "Reward over last 100: 180.33\n",
      "Average over all episodes so far: 112.55028901734104\n",
      "epsilon: 0.01\n",
      "**** Episode  1740 **** \n",
      "Recent average reward: 179.4\n",
      "Reward over last 100: 180.13\n",
      "Average over all episodes so far: 112.93448275862069\n",
      "epsilon: 0.01\n",
      "**** Episode  1750 **** \n",
      "Recent average reward: 165.4\n",
      "Reward over last 100: 178.55\n",
      "Average over all episodes so far: 113.23428571428572\n",
      "epsilon: 0.01\n",
      "**** Episode  1760 **** \n",
      "Recent average reward: 181.7\n",
      "Reward over last 100: 179.36\n",
      "Average over all episodes so far: 113.62329545454546\n",
      "epsilon: 0.01\n",
      "**** Episode  1770 **** \n",
      "Recent average reward: 172.5\n",
      "Reward over last 100: 179.0\n",
      "Average over all episodes so far: 113.95593220338984\n",
      "epsilon: 0.01\n",
      "**** Episode  1780 **** \n",
      "Recent average reward: 184.8\n",
      "Reward over last 100: 180.36\n",
      "Average over all episodes so far: 114.35393258426966\n",
      "epsilon: 0.01\n",
      "**** Episode  1790 **** \n",
      "Recent average reward: 184.7\n",
      "Reward over last 100: 180.79\n",
      "Average over all episodes so far: 114.74692737430168\n",
      "epsilon: 0.01\n",
      "**** Episode  1800 **** \n",
      "Recent average reward: 184.7\n",
      "Reward over last 100: 180.59\n",
      "Average over all episodes so far: 115.13555555555556\n",
      "epsilon: 0.01\n",
      "**** Episode  1810 **** \n",
      "Recent average reward: 179.8\n",
      "Reward over last 100: 179.87\n",
      "Average over all episodes so far: 115.49281767955802\n",
      "epsilon: 0.01\n",
      "**** Episode  1820 **** \n",
      "Recent average reward: 172.0\n",
      "Reward over last 100: 178.2\n",
      "Average over all episodes so far: 115.80329670329671\n",
      "epsilon: 0.01\n",
      "**** Episode  1830 **** \n",
      "Recent average reward: 180.7\n",
      "Reward over last 100: 178.57\n",
      "Average over all episodes so far: 116.15792349726776\n",
      "epsilon: 0.01\n",
      "**** Episode  1840 **** \n",
      "Recent average reward: 175.0\n",
      "Reward over last 100: 178.13\n",
      "Average over all episodes so far: 116.47771739130435\n",
      "epsilon: 0.01\n",
      "**** Episode  1850 **** \n",
      "Recent average reward: 177.0\n",
      "Reward over last 100: 179.29\n",
      "Average over all episodes so far: 116.80486486486487\n",
      "epsilon: 0.01\n",
      "**** Episode  1860 **** \n",
      "Recent average reward: 181.1\n",
      "Reward over last 100: 179.23\n",
      "Average over all episodes so far: 117.15053763440861\n",
      "epsilon: 0.01\n",
      "**** Episode  1870 **** \n",
      "Recent average reward: 168.3\n",
      "Reward over last 100: 178.81\n",
      "Average over all episodes so far: 117.42406417112299\n",
      "epsilon: 0.01\n",
      "**** Episode  1880 **** \n",
      "Recent average reward: 174.5\n",
      "Reward over last 100: 177.78\n",
      "Average over all episodes so far: 117.72765957446809\n",
      "epsilon: 0.01\n",
      "**** Episode  1890 **** \n",
      "Recent average reward: 182.6\n",
      "Reward over last 100: 177.57\n",
      "Average over all episodes so far: 118.07089947089948\n",
      "epsilon: 0.01\n",
      "**** Episode  1900 **** \n",
      "Recent average reward: 176.1\n",
      "Reward over last 100: 176.71\n",
      "Average over all episodes so far: 118.37631578947368\n",
      "epsilon: 0.01\n",
      "**** Episode  1910 **** \n",
      "Recent average reward: 184.1\n",
      "Reward over last 100: 177.14\n",
      "Average over all episodes so far: 118.72041884816754\n",
      "epsilon: 0.01\n",
      "**** Episode  1920 **** \n",
      "Recent average reward: 188.8\n",
      "Reward over last 100: 178.82\n",
      "Average over all episodes so far: 119.08541666666666\n",
      "epsilon: 0.01\n",
      "**** Episode  1930 **** \n",
      "Recent average reward: 183.5\n",
      "Reward over last 100: 179.1\n",
      "Average over all episodes so far: 119.41917098445596\n",
      "epsilon: 0.01\n",
      "**** Episode  1940 **** \n",
      "Recent average reward: 185.7\n",
      "Reward over last 100: 180.17\n",
      "Average over all episodes so far: 119.76082474226804\n",
      "epsilon: 0.01\n",
      "**** Episode  1950 **** \n",
      "Recent average reward: 178.9\n",
      "Reward over last 100: 180.36\n",
      "Average over all episodes so far: 120.06410256410257\n",
      "epsilon: 0.01\n",
      "**** Episode  1960 **** \n",
      "Recent average reward: 176.9\n",
      "Reward over last 100: 179.94\n",
      "Average over all episodes so far: 120.35408163265306\n",
      "epsilon: 0.01\n",
      "**** Episode  1970 **** \n",
      "Recent average reward: 183.1\n",
      "Reward over last 100: 181.42\n",
      "Average over all episodes so far: 120.6725888324873\n",
      "epsilon: 0.01\n",
      "**** Episode  1980 **** \n",
      "Recent average reward: 178.0\n",
      "Reward over last 100: 181.77\n",
      "Average over all episodes so far: 120.96212121212122\n",
      "epsilon: 0.01\n",
      "**** Episode  1990 **** \n",
      "Recent average reward: 172.4\n",
      "Reward over last 100: 180.75\n",
      "Average over all episodes so far: 121.22060301507538\n",
      "epsilon: 0.01\n",
      "**** Episode  2000 **** \n",
      "Recent average reward: 179.9\n",
      "Reward over last 100: 181.13\n",
      "Average over all episodes so far: 121.514\n",
      "epsilon: 0.01\n",
      "**** Episode  2010 **** \n",
      "Recent average reward: 174.4\n",
      "Reward over last 100: 180.16\n",
      "Average over all episodes so far: 121.7771144278607\n",
      "epsilon: 0.01\n",
      "**** Episode  2020 **** \n",
      "Recent average reward: 160.1\n",
      "Reward over last 100: 177.29\n",
      "Average over all episodes so far: 121.96683168316832\n",
      "epsilon: 0.01\n",
      "**** Episode  2030 **** \n",
      "Recent average reward: 178.6\n",
      "Reward over last 100: 176.8\n",
      "Average over all episodes so far: 122.24581280788178\n",
      "epsilon: 0.01\n",
      "**** Episode  2040 **** \n",
      "Recent average reward: 177.3\n",
      "Reward over last 100: 175.96\n",
      "Average over all episodes so far: 122.5156862745098\n",
      "epsilon: 0.01\n",
      "**** Episode  2050 **** \n",
      "Recent average reward: 171.0\n",
      "Reward over last 100: 175.17\n",
      "Average over all episodes so far: 122.75219512195122\n",
      "epsilon: 0.01\n",
      "**** Episode  2060 **** \n",
      "Recent average reward: 182.3\n",
      "Reward over last 100: 175.71\n",
      "Average over all episodes so far: 123.04126213592232\n",
      "epsilon: 0.01\n",
      "**** Episode  2070 **** \n",
      "Recent average reward: 183.0\n",
      "Reward over last 100: 175.7\n",
      "Average over all episodes so far: 123.33091787439614\n",
      "epsilon: 0.01\n",
      "**** Episode  2080 **** \n",
      "Recent average reward: 189.9\n",
      "Reward over last 100: 176.89\n",
      "Average over all episodes so far: 123.65096153846154\n",
      "epsilon: 0.01\n",
      "**** Episode  2090 **** \n",
      "Recent average reward: 175.7\n",
      "Reward over last 100: 177.22\n",
      "Average over all episodes so far: 123.9\n",
      "epsilon: 0.01\n",
      "**** Episode  2100 **** \n",
      "Recent average reward: 180.7\n",
      "Reward over last 100: 177.3\n",
      "Average over all episodes so far: 124.1704761904762\n",
      "epsilon: 0.01\n",
      "**** Episode  2110 **** \n",
      "Recent average reward: 187.6\n",
      "Reward over last 100: 178.62\n",
      "Average over all episodes so far: 124.47109004739336\n",
      "epsilon: 0.01\n",
      "**** Episode  2120 **** \n",
      "Recent average reward: 174.5\n",
      "Reward over last 100: 180.06\n",
      "Average over all episodes so far: 124.70707547169812\n",
      "epsilon: 0.01\n",
      "**** Episode  2130 **** \n",
      "Recent average reward: 172.3\n",
      "Reward over last 100: 179.43\n",
      "Average over all episodes so far: 124.93051643192489\n",
      "epsilon: 0.01\n",
      "**** Episode  2140 **** \n",
      "Recent average reward: 178.7\n",
      "Reward over last 100: 179.57\n",
      "Average over all episodes so far: 125.18177570093458\n",
      "epsilon: 0.01\n",
      "**** Episode  2150 **** \n",
      "Recent average reward: 181.2\n",
      "Reward over last 100: 180.59\n",
      "Average over all episodes so far: 125.44232558139535\n",
      "epsilon: 0.01\n",
      "**** Episode  2160 **** \n",
      "Recent average reward: 176.2\n",
      "Reward over last 100: 179.98\n",
      "Average over all episodes so far: 125.67731481481482\n",
      "epsilon: 0.01\n",
      "**** Episode  2170 **** \n",
      "Recent average reward: 171.0\n",
      "Reward over last 100: 178.78\n",
      "Average over all episodes so far: 125.88617511520738\n",
      "epsilon: 0.01\n",
      "**** Episode  2180 **** \n",
      "Recent average reward: 179.2\n",
      "Reward over last 100: 177.71\n",
      "Average over all episodes so far: 126.13073394495413\n",
      "epsilon: 0.01\n",
      "**** Episode  2190 **** \n",
      "Recent average reward: 178.9\n",
      "Reward over last 100: 178.03\n",
      "Average over all episodes so far: 126.3716894977169\n",
      "epsilon: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  2200 **** \n",
      "Recent average reward: 183.2\n",
      "Reward over last 100: 178.28\n",
      "Average over all episodes so far: 126.63\n",
      "epsilon: 0.01\n",
      "**** Episode  2210 **** \n",
      "Recent average reward: 182.7\n",
      "Reward over last 100: 177.79\n",
      "Average over all episodes so far: 126.88371040723982\n",
      "epsilon: 0.01\n",
      "**** Episode  2220 **** \n",
      "Recent average reward: 169.0\n",
      "Reward over last 100: 177.24\n",
      "Average over all episodes so far: 127.07342342342342\n",
      "epsilon: 0.01\n",
      "**** Episode  2230 **** \n",
      "Recent average reward: 181.4\n",
      "Reward over last 100: 178.15\n",
      "Average over all episodes so far: 127.31704035874439\n",
      "epsilon: 0.01\n",
      "**** Episode  2240 **** \n",
      "Recent average reward: 166.4\n",
      "Reward over last 100: 176.92\n",
      "Average over all episodes so far: 127.49151785714285\n",
      "epsilon: 0.01\n",
      "**** Episode  2250 **** \n",
      "Recent average reward: 171.4\n",
      "Reward over last 100: 175.94\n",
      "Average over all episodes so far: 127.68666666666667\n",
      "epsilon: 0.01\n",
      "**** Episode  2260 **** \n",
      "Recent average reward: 166.7\n",
      "Reward over last 100: 174.99\n",
      "Average over all episodes so far: 127.85929203539823\n",
      "epsilon: 0.01\n",
      "**** Episode  2270 **** \n",
      "Recent average reward: 181.0\n",
      "Reward over last 100: 175.99\n",
      "Average over all episodes so far: 128.09339207048458\n",
      "epsilon: 0.01\n",
      "**** Episode  2280 **** \n",
      "Recent average reward: 180.6\n",
      "Reward over last 100: 176.13\n",
      "Average over all episodes so far: 128.32368421052632\n",
      "epsilon: 0.01\n",
      "**** Episode  2290 **** \n",
      "Recent average reward: 180.6\n",
      "Reward over last 100: 176.3\n",
      "Average over all episodes so far: 128.55196506550217\n",
      "epsilon: 0.01\n",
      "**** Episode  2300 **** \n",
      "Recent average reward: 187.5\n",
      "Reward over last 100: 176.73\n",
      "Average over all episodes so far: 128.80826086956523\n",
      "epsilon: 0.01\n",
      "**** Episode  2310 **** \n",
      "Recent average reward: 194.4\n",
      "Reward over last 100: 177.9\n",
      "Average over all episodes so far: 129.0922077922078\n",
      "epsilon: 0.01\n",
      "**** Episode  2320 **** \n",
      "Recent average reward: 189.7\n",
      "Reward over last 100: 179.97\n",
      "Average over all episodes so far: 129.35344827586206\n",
      "epsilon: 0.01\n",
      "**** Episode  2330 **** \n",
      "Recent average reward: 187.9\n",
      "Reward over last 100: 180.62\n",
      "Average over all episodes so far: 129.60472103004292\n",
      "epsilon: 0.01\n",
      "**** Episode  2340 **** \n",
      "Recent average reward: 183.1\n",
      "Reward over last 100: 182.29\n",
      "Average over all episodes so far: 129.83333333333334\n",
      "epsilon: 0.01\n",
      "**** Episode  2350 **** \n",
      "Recent average reward: 185.0\n",
      "Reward over last 100: 183.65\n",
      "Average over all episodes so far: 130.06808510638297\n",
      "epsilon: 0.01\n",
      "**** Episode  2360 **** \n",
      "Recent average reward: 181.7\n",
      "Reward over last 100: 185.15\n",
      "Average over all episodes so far: 130.28686440677967\n",
      "epsilon: 0.01\n",
      "**** Episode  2370 **** \n",
      "Recent average reward: 181.3\n",
      "Reward over last 100: 185.18\n",
      "Average over all episodes so far: 130.50210970464136\n",
      "epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "steps_total=np.full([num_episodes],-999,dtype=np.int32)\n",
    "\n",
    "frames_total=0\n",
    "\n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    #for step in range(100):\n",
    "    step=0\n",
    "    while True:\n",
    "        \n",
    "        step+=1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon=calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action=env.action_space.sample()\n",
    "        action=qnet_agent.select_action(state,epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        memory.push(state, action, new_state,\n",
    "                     reward, done)\n",
    "        \n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state=new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total[i_episode]=step\n",
    "            \n",
    "            if i_episode>100:\n",
    "                mean_reward_100 = np.sum(steps_total[i_episode-100:i_episode])/100\n",
    "            \n",
    "                if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                    print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                    solved_after = i_episode\n",
    "                    solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0 and i_episode>1):\n",
    "                print(\"**** Episode  {} **** \".format(i_episode))\n",
    "                recent_avg_reward=np.average(steps_total[i_episode-report_interval:i_episode])\n",
    "                print(\"Recent average reward: {}\".format(recent_avg_reward))\n",
    "                if i_episode>100:\n",
    "                    print(\"Reward over last 100: {}\".format(mean_reward_100))\n",
    "                full_avg_so_far=np.average(steps_total[:i_episode])\n",
    "                print(\"Average over all episodes so far: {}\".format(full_avg_so_far))\n",
    "                print(\"epsilon: {}\".format(epsilon))\n",
    "            \n",
    "                #print(\"Episode {} finished after: {}\".format(i_episode,step))\n",
    "            break\n",
    "            \n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(steps_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of steps: {}\". format(np.average(steps_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of steps=reward: {}\". format(np.average(steps_total)))\n",
    "print(\"Average number of steps=reward in last 100 episodes: {}\". format(np.average(steps_total[-100:])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(1,figsize=[12,5])\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total,alpha=0.6, color='green')\n",
    "#plt.plot(rewards_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2,figsize=[12,5])\n",
    "plt.title(\"Steps to finish episode\")\n",
    "plt.plot(steps_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
