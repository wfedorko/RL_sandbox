{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../baselines')\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-bright')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id=\"PongNoFrameskip-v4\"\n",
    "env=make_atari(env_id)\n",
    "env=wrap_deepmind(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, episode,step, info=\"\"):\n",
    "    plt.figure(99999,figsize=[8,6])\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"episode: {} step: {} \".format(episode,step))\n",
    "    #plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device=torch.device(\"cuda:4\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PARAMS ######\n",
    "learning_rate = 0.0001\n",
    "num_episodes = 1000\n",
    "gamma=0.99\n",
    "#gamma=0.85\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 10000\n",
    "\n",
    "report_interval=10\n",
    "\n",
    "score_to_solve = 18.0\n",
    "\n",
    "hidden_layer_size=512\n",
    "\n",
    "replay_memory_size=100000\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "update_target_frequency = 5000\n",
    "\n",
    "clip_error=True\n",
    "normalize_image=True\n",
    "\n",
    "double_dqn=True\n",
    "\n",
    "file2save = 'pong_save_funny_mean_t2.pth'\n",
    "\n",
    "save_model_frequency=10000\n",
    "\n",
    "resume_previous_training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.wrappers.Monitor(env, '../mp4/sandbox10',video_callable=lambda episode_id: True,force=True)\n",
    "env = gym.wrappers.Monitor(env, '../mp4/PongVideos_funny_mean',video_callable=lambda episode_id: episode_id%20==0, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs=env.observation_space.shape[0]\n",
    "number_of_outputs=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Discrete.contains of Discrete(6)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    return torch.load(file2save)\n",
    "\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), file2save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = frame.transpose((2,0,1))\n",
    "    frame = torch.from_numpy(frame)\n",
    "    frame = frame.to(device, dtype=torch.float32)\n",
    "    frame = frame.unsqueeze(0)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(reward_total, i_episode):\n",
    "    plt.figure(figsize=[12,5])\n",
    "    plt.title(\"Score at end of episode\")\n",
    "    plt.plot(reward_total[:i_episode],color='red')\n",
    "    plt.savefig(\"Pong-results-funny_mean_t.png\")\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19.]])\n"
     ]
    }
   ],
   "source": [
    "o=torch.arange(20,dtype=torch.float32).view(-1,5)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity=capacity\n",
    "        self.memory=[]\n",
    "        self.position=0\n",
    "        \n",
    "    \n",
    "    def push(self, state,\n",
    "             action, new_state,\n",
    "             reward, done):\n",
    "        \n",
    "            transition=(state,action,new_state,reward,done)\n",
    "            \n",
    "            if self.position>=len(self.memory):\n",
    "                self.memory.append(transition)\n",
    "            else:\n",
    "                self.memory[self.position]=transition\n",
    "                \n",
    "            self.position=(self.position+1)%self.capacity\n",
    "        \n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkDueling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkDueling, self).__init__()\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8 ,stride=4)\n",
    "        self.conv2=nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4 ,stride=2)\n",
    "        self.conv3=nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3 ,stride=1)\n",
    "        \n",
    "        self.advantage1 = nn.Linear(7*7*64, hidden_layer_size)\n",
    "        self.advantage2 = nn.Linear(hidden_layer_size, number_of_outputs)\n",
    "        \n",
    "        self.value1 = nn.Linear(7*7*64,hidden_layer_size)\n",
    "        self.value2 = nn.Linear(hidden_layer_size,1)\n",
    "        \n",
    "        #self.activation=nn.Tanh()\n",
    "        self.activation=nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print('x shape {} and value:'.format(x.shape))\n",
    "        #print(x.detach().cpu())\n",
    "        \n",
    "        if normalize_image:\n",
    "            x=x/255.0\n",
    "        \n",
    "        output_conv = self.conv1(x)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv2(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv3(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        \n",
    "        output_conv = output_conv.view(output_conv.shape[0],-1)\n",
    "        \n",
    "        output_advantage=self.advantage1(output_conv)\n",
    "        output_advantage=self.activation(output_advantage)\n",
    "        output_advantage=self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value=self.value1(output_conv)\n",
    "        output_value=self.activation(output_value)\n",
    "        output_value=self.value2(output_value)\n",
    "        \n",
    "        #print('output_advantage shape {} and value:'.format(output_advantage.shape))\n",
    "        #print(output_advantage.detach().cpu())\n",
    "        \n",
    "        #print('output_value shape {} and value:'.format(output_value.shape))\n",
    "        #print(output_value.detach().cpu())\n",
    "        \n",
    "        #print('output_advantage.mean shape {} and value:'.format(output_advantage.mean(dim=1,keepdim=True).shape))\n",
    "        #print(output_advantage.mean(dim=1,keepdim=True).detach().cpu())\n",
    "        \n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "        \n",
    "        #output_final=output_value+output_advantage-output_advantage.mean(dim=1,keepdim=True)\n",
    "        \n",
    "        \n",
    "        return output_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer_size)\n",
    "        self.linear2 = nn.Linear(hidden_layer_size,number_of_outputs)\n",
    "        \n",
    "        self.activation=nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_Agent():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.nn = NeuralNetworkDueling().to(device)\n",
    "        self.target_nn = NeuralNetworkDueling().to(device)\n",
    "        \n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.number_of_frames = 0\n",
    "        \n",
    "        if resume_previous_training and os.path.exists(file2save):\n",
    "            print('loading previous model')\n",
    "            self.nn.load_state_dict(load_model())\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy=torch.rand(1).item()\n",
    "        \n",
    "        if random_for_egreedy>epsilon:\n",
    "            self.nn.eval()\n",
    "            with torch.no_grad():\n",
    "                state=preprocess_frame(state)\n",
    "                #state=torch.Tensor(state).to(device)\n",
    "                predicted_value_from_nn=self.nn(state).squeeze()\n",
    "                #print('predicted value from nn:')\n",
    "                #print(predicted_value_from_nn)\n",
    "                action=torch.argmax(predicted_value_from_nn).item()\n",
    "                #print('action: {}'.format(action))\n",
    "        else:\n",
    "            action=env.action_space.sample()\n",
    "                \n",
    "                \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if len(memory)<batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state=[preprocess_frame(frame) for frame in state] \n",
    "        state=torch.cat(state)\n",
    "        \n",
    "        new_state=[preprocess_frame(frame) for frame in new_state] \n",
    "        new_state=torch.cat(new_state)\n",
    "        \n",
    "        #print('state batch shape {}'.format(state.shape))\n",
    "        #print(state)\n",
    "        \n",
    "        #state=torch.Tensor(state).to(device)\n",
    "        #new_state=torch.Tensor(new_state).to(device)\n",
    "        \n",
    "        \n",
    "        reward=torch.Tensor(reward).to(device)\n",
    "        \n",
    "        #the view call below is to transform into column vector\n",
    "        #so that it can be used in the gather call\n",
    "        #i.e. we will use it to pick out from the computed value\n",
    "        #tensor only values indexed by selected action\n",
    "        action=(torch.Tensor(action).view(-1,1).long()).to(device)\n",
    "        #print('action: ')\n",
    "        #print(action)\n",
    "        #print('contiguous?', action.is_contiguous())\n",
    "        done=torch.Tensor(done).to(device)\n",
    "        \n",
    "        #print('shape of: state, new state, reward, action, done:')\n",
    "        #print(state.shape)\n",
    "        #print(new_state.shape)\n",
    "        #print(reward.shape)\n",
    "        #print(action.shape)\n",
    "        #print(done.shape)\n",
    "        \n",
    "        \n",
    "        self.nn.eval()\n",
    "        self.target_nn.eval()\n",
    "            \n",
    "            \n",
    "        if double_dqn:\n",
    "            #print('in double DQN')\n",
    "            new_state_values_from_nn=self.nn(new_state).detach()\n",
    "            #print('new_state_values_from_nn shape {} and value:'.format(new_state_values_from_nn.shape))\n",
    "            #print(new_state_values_from_nn)\n",
    "            max_new_state_indexes=torch.max(new_state_values_from_nn,dim=1)[1].view(-1,1)\n",
    "            #print('max_new_state_indexes shape {} and value:'.format(max_new_state_indexes.shape))\n",
    "            #print(max_new_state_indexes)\n",
    "            new_state_values=self.target_nn(new_state).detach()\n",
    "            #print('new_state_values shape {} and value:'.format(new_state_values.shape))\n",
    "            #print(new_state_values)\n",
    "            max_new_state_values=torch.gather(new_state_values,1,max_new_state_indexes).squeeze()\n",
    "            #print('max_new_state_values shape {} and value:'.format(max_new_state_values.shape))\n",
    "            #print(max_new_state_values)\n",
    "        else:\n",
    "            #print('in regular DQN')\n",
    "            new_state_values=self.target_nn(new_state).detach()\n",
    "            #print('new_state_values shape {} and value'.format(new_state_values.shape))\n",
    "            #print(new_state_values)\n",
    "        \n",
    "            max_new_state_values=torch.max(new_state_values,dim=1)[0]\n",
    "            #print('max_new_state_values shape {} and value'.format(max_new_state_values.shape))\n",
    "            #print(max_new_state_values)\n",
    "            \n",
    "        target_value=(reward + (1-done)*gamma*max_new_state_values).view(-1,1)\n",
    "        \n",
    "        #print('shape of: target_value')\n",
    "        #print(target_value.shape)\n",
    "        self.nn.train()\n",
    "        \n",
    "        #this will select only the values of the desired actions\n",
    "        predicted_value=torch.gather(self.nn(state),1,action)\n",
    "        #print('shape of: predicted_value')\n",
    "        #print(predicted_value.shape)\n",
    "        \n",
    "        \n",
    "        loss=self.loss_function(predicted_value,target_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.clamp_(-1.0,1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.number_of_frames % update_target_frequency == 0:\n",
    "            #print(\"***********************\")\n",
    "            #print(\"UPDATING TARGET NETWORK\")\n",
    "            #print(\"update counter: {}\".format(self.update_target_counter))\n",
    "            #print(\"***********************\")\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "            \n",
    "        if self.number_of_frames % save_model_frequency ==0:\n",
    "            save_model(self.nn)\n",
    "        \n",
    "        self.number_of_frames+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ExperienceReplay(replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_agent=QNet_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value=23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_agent():\n",
    "    steps_total=np.full([num_episodes],-999,dtype=np.int32)\n",
    "    reward_total=np.full([num_episodes],-999,dtype=np.int32)\n",
    "\n",
    "    frames_total=0\n",
    "\n",
    "    solved_after = 0\n",
    "    solved = False\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "        #for step in range(100):\n",
    "        step=0\n",
    "        reward_total[i_episode]=0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            step+=1\n",
    "            frames_total += 1\n",
    "\n",
    "            epsilon=calculate_epsilon(frames_total)\n",
    "\n",
    "            #action=env.action_space.sample()\n",
    "            action=qnet_agent.select_action(state,epsilon)\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            memory.push(state, action, new_state,\n",
    "                         reward, done)\n",
    "\n",
    "            reward_total[i_episode]+=reward\n",
    "\n",
    "            qnet_agent.optimize()\n",
    "\n",
    "            state=new_state\n",
    "\n",
    "\n",
    "            if done:\n",
    "                steps_total[i_episode]=step\n",
    "\n",
    "                if i_episode>100:\n",
    "                    mean_reward_100 = np.sum(reward_total[i_episode-100:i_episode])/100\n",
    "\n",
    "\n",
    "                    if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                        print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                        solved_after = i_episode\n",
    "                        solved = True\n",
    "\n",
    "                if (i_episode % report_interval == 0 and i_episode>1):\n",
    "\n",
    "                    plot_results(reward_total, i_episode)\n",
    "\n",
    "                    print(\"**** Episode  {} **** \".format(i_episode))\n",
    "                    recent_avg_reward=np.average(reward_total[i_episode-report_interval:i_episode])\n",
    "                    print(\"Recent average reward: {}\".format(recent_avg_reward))\n",
    "                    if i_episode>100:\n",
    "                        print(\"Reward over last 100: {}\".format(mean_reward_100))\n",
    "                    full_avg_so_far=np.average(reward_total[:i_episode])\n",
    "                    print(\"Average over all episodes so far: {}\".format(full_avg_so_far))\n",
    "                    print(\"epsilon: {}\".format(epsilon))\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "                    #print(\"Episode {} finished after: {}\".format(i_episode,step))\n",
    "                break\n",
    "\n",
    "    if solved:\n",
    "        print(\"Solved after %i episodes\" % solved_after)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wfedorko/RL_sandbox/notebooks'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  10 **** \n",
      "Recent average reward: -20.1\n",
      "Average over all episodes so far: -20.1\n",
      "epsilon: 0.33386328365937423\n",
      "Elapsed time:  00:03:22\n",
      "**** Episode  20 **** \n",
      "Recent average reward: -20.2\n",
      "Average over all episodes so far: -20.15\n",
      "epsilon: 0.141475623620375\n",
      "Elapsed time:  00:06:12\n",
      "**** Episode  30 **** \n",
      "Recent average reward: -20.4\n",
      "Average over all episodes so far: -20.233333333333334\n",
      "epsilon: 0.06604872841973342\n",
      "Elapsed time:  00:08:48\n",
      "**** Episode  40 **** \n",
      "Recent average reward: -20.5\n",
      "Average over all episodes so far: -20.3\n",
      "epsilon: 0.03202595350279818\n",
      "Elapsed time:  00:11:52\n",
      "**** Episode  50 **** \n",
      "Recent average reward: -20.3\n",
      "Average over all episodes so far: -20.3\n",
      "epsilon: 0.017792965460446626\n",
      "Elapsed time:  00:15:13\n",
      "**** Episode  60 **** \n",
      "Recent average reward: -20.1\n",
      "Average over all episodes so far: -20.266666666666666\n",
      "epsilon: 0.012517890537627288\n",
      "Elapsed time:  00:18:54\n",
      "**** Episode  70 **** \n",
      "Recent average reward: -19.1\n",
      "Average over all episodes so far: -20.1\n",
      "epsilon: 0.010609949850631545\n",
      "Elapsed time:  00:23:34\n",
      "**** Episode  80 **** \n",
      "Recent average reward: -19.3\n",
      "Average over all episodes so far: -20.0\n",
      "epsilon: 0.010119638744737314\n",
      "Elapsed time:  00:28:55\n",
      "**** Episode  90 **** \n",
      "Recent average reward: -17.7\n",
      "Average over all episodes so far: -19.744444444444444\n",
      "epsilon: 0.010018398639868791\n",
      "Elapsed time:  00:35:01\n",
      "**** Episode  100 **** \n",
      "Recent average reward: -17.7\n",
      "Average over all episodes so far: -19.54\n",
      "epsilon: 0.01000213416771515\n",
      "Elapsed time:  00:41:58\n",
      "**** Episode  110 **** \n",
      "Recent average reward: -15.7\n",
      "Reward over last 100: -19.1\n",
      "Average over all episodes so far: -19.19090909090909\n",
      "epsilon: 0.01000027011073936\n",
      "Elapsed time:  00:48:40\n",
      "**** Episode  120 **** \n",
      "Recent average reward: -16.2\n",
      "Reward over last 100: -18.7\n",
      "Average over all episodes so far: -18.941666666666666\n",
      "epsilon: 0.010000019660902533\n",
      "Elapsed time:  00:57:07\n",
      "**** Episode  130 **** \n",
      "Recent average reward: -15.7\n",
      "Reward over last 100: -18.23\n",
      "Average over all episodes so far: -18.692307692307693\n",
      "epsilon: 0.010000001565541893\n",
      "Elapsed time:  01:05:16\n",
      "**** Episode  140 **** \n",
      "Recent average reward: -16.5\n",
      "Reward over last 100: -17.83\n",
      "Average over all episodes so far: -18.535714285714285\n",
      "epsilon: 0.010000000113668611\n",
      "Elapsed time:  01:13:46\n",
      "**** Episode  150 **** \n",
      "Recent average reward: -16.0\n",
      "Reward over last 100: -17.4\n",
      "Average over all episodes so far: -18.366666666666667\n",
      "epsilon: 0.010000000008892314\n",
      "Elapsed time:  01:22:01\n",
      "**** Episode  160 **** \n",
      "Recent average reward: -15.0\n",
      "Reward over last 100: -16.89\n",
      "Average over all episodes so far: -18.15625\n",
      "epsilon: 0.010000000000604645\n",
      "Elapsed time:  01:30:47\n",
      "**** Episode  170 **** \n",
      "Recent average reward: -14.7\n",
      "Reward over last 100: -16.45\n",
      "Average over all episodes so far: -17.95294117647059\n",
      "epsilon: 0.010000000000044721\n",
      "Elapsed time:  01:39:12\n",
      "**** Episode  180 **** \n",
      "Recent average reward: -14.2\n",
      "Reward over last 100: -15.94\n",
      "Average over all episodes so far: -17.744444444444444\n",
      "epsilon: 0.010000000000003065\n",
      "Elapsed time:  01:47:55\n",
      "**** Episode  190 **** \n",
      "Recent average reward: -14.0\n",
      "Reward over last 100: -15.57\n",
      "Average over all episodes so far: -17.54736842105263\n",
      "epsilon: 0.01000000000000019\n",
      "Elapsed time:  01:56:54\n",
      "**** Episode  200 **** \n",
      "Recent average reward: -14.9\n",
      "Reward over last 100: -15.29\n",
      "Average over all episodes so far: -17.415\n",
      "epsilon: 0.01000000000000001\n",
      "Elapsed time:  02:06:17\n",
      "**** Episode  210 **** \n",
      "Recent average reward: -14.0\n",
      "Reward over last 100: -15.12\n",
      "Average over all episodes so far: -17.252380952380953\n",
      "epsilon: 0.01\n",
      "Elapsed time:  02:15:06\n",
      "**** Episode  220 **** \n",
      "Recent average reward: -12.0\n",
      "Reward over last 100: -14.7\n",
      "Average over all episodes so far: -17.013636363636362\n",
      "epsilon: 0.01\n",
      "Elapsed time:  02:23:55\n",
      "**** Episode  230 **** \n",
      "Recent average reward: -11.4\n",
      "Reward over last 100: -14.27\n",
      "Average over all episodes so far: -16.769565217391303\n",
      "epsilon: 0.01\n",
      "Elapsed time:  02:32:07\n",
      "**** Episode  240 **** \n",
      "Recent average reward: -11.4\n",
      "Reward over last 100: -13.76\n",
      "Average over all episodes so far: -16.545833333333334\n",
      "epsilon: 0.01\n",
      "Elapsed time:  02:41:22\n",
      "**** Episode  250 **** \n",
      "Recent average reward: -10.3\n",
      "Reward over last 100: -13.19\n",
      "Average over all episodes so far: -16.296\n",
      "epsilon: 0.01\n",
      "Elapsed time:  02:49:48\n",
      "**** Episode  260 **** \n",
      "Recent average reward: -7.9\n",
      "Reward over last 100: -12.48\n",
      "Average over all episodes so far: -15.973076923076922\n",
      "epsilon: 0.01\n",
      "Elapsed time:  02:59:44\n",
      "**** Episode  270 **** \n",
      "Recent average reward: -7.8\n",
      "Reward over last 100: -11.79\n",
      "Average over all episodes so far: -15.670370370370371\n",
      "epsilon: 0.01\n",
      "Elapsed time:  03:09:32\n",
      "**** Episode  280 **** \n",
      "Recent average reward: -9.3\n",
      "Reward over last 100: -11.3\n",
      "Average over all episodes so far: -15.442857142857143\n",
      "epsilon: 0.01\n",
      "Elapsed time:  03:19:05\n",
      "**** Episode  290 **** \n",
      "Recent average reward: -6.5\n",
      "Reward over last 100: -10.55\n",
      "Average over all episodes so far: -15.13448275862069\n",
      "epsilon: 0.01\n",
      "Elapsed time:  03:29:12\n",
      "**** Episode  300 **** \n",
      "Recent average reward: -6.1\n",
      "Reward over last 100: -9.67\n",
      "Average over all episodes so far: -14.833333333333334\n",
      "epsilon: 0.01\n",
      "Elapsed time:  03:39:43\n",
      "**** Episode  310 **** \n",
      "Recent average reward: -0.7\n",
      "Reward over last 100: -8.34\n",
      "Average over all episodes so far: -14.37741935483871\n",
      "epsilon: 0.01\n",
      "Elapsed time:  03:51:06\n",
      "**** Episode  320 **** \n",
      "Recent average reward: 3.9\n",
      "Reward over last 100: -6.75\n",
      "Average over all episodes so far: -13.80625\n",
      "epsilon: 0.01\n",
      "Elapsed time:  04:02:23\n",
      "**** Episode  330 **** \n",
      "Recent average reward: 6.9\n",
      "Reward over last 100: -4.92\n",
      "Average over all episodes so far: -13.17878787878788\n",
      "epsilon: 0.01\n",
      "Elapsed time:  04:12:52\n",
      "**** Episode  340 **** \n",
      "Recent average reward: 5.9\n",
      "Reward over last 100: -3.19\n",
      "Average over all episodes so far: -12.617647058823529\n",
      "epsilon: 0.01\n",
      "Elapsed time:  04:23:11\n",
      "**** Episode  350 **** \n",
      "Recent average reward: 11.9\n",
      "Reward over last 100: -0.97\n",
      "Average over all episodes so far: -11.917142857142856\n",
      "epsilon: 0.01\n",
      "Elapsed time:  04:32:29\n",
      "**** Episode  360 **** \n",
      "Recent average reward: 11.5\n",
      "Reward over last 100: 0.97\n",
      "Average over all episodes so far: -11.266666666666667\n",
      "epsilon: 0.01\n",
      "Elapsed time:  04:41:14\n",
      "**** Episode  370 **** \n",
      "Recent average reward: 13.3\n",
      "Reward over last 100: 3.08\n",
      "Average over all episodes so far: -10.602702702702702\n",
      "epsilon: 0.01\n",
      "Elapsed time:  04:49:10\n",
      "**** Episode  380 **** \n",
      "Recent average reward: 17.2\n",
      "Reward over last 100: 5.73\n",
      "Average over all episodes so far: -9.871052631578948\n",
      "epsilon: 0.01\n",
      "Elapsed time:  04:56:50\n",
      "**** Episode  390 **** \n",
      "Recent average reward: 17.3\n",
      "Reward over last 100: 8.11\n",
      "Average over all episodes so far: -9.174358974358974\n",
      "epsilon: 0.01\n",
      "Elapsed time:  05:03:33\n",
      "**** Episode  400 **** \n",
      "Recent average reward: 15.9\n",
      "Reward over last 100: 10.31\n",
      "Average over all episodes so far: -8.5475\n",
      "epsilon: 0.01\n",
      "Elapsed time:  05:11:08\n",
      "**** Episode  410 **** \n",
      "Recent average reward: 16.3\n",
      "Reward over last 100: 12.01\n",
      "Average over all episodes so far: -7.941463414634146\n",
      "epsilon: 0.01\n",
      "Elapsed time:  05:18:24\n",
      "**** Episode  420 **** \n",
      "Recent average reward: 17.5\n",
      "Reward over last 100: 13.37\n",
      "Average over all episodes so far: -7.335714285714285\n",
      "epsilon: 0.01\n",
      "Elapsed time:  05:26:03\n",
      "**** Episode  430 **** \n",
      "Recent average reward: 17.6\n",
      "Reward over last 100: 14.44\n",
      "Average over all episodes so far: -6.755813953488372\n",
      "epsilon: 0.01\n",
      "Elapsed time:  05:32:56\n",
      "**** Episode  440 **** \n",
      "Recent average reward: 18.9\n",
      "Reward over last 100: 15.74\n",
      "Average over all episodes so far: -6.172727272727273\n",
      "epsilon: 0.01\n",
      "Elapsed time:  05:39:10\n",
      "**** Episode  450 **** \n",
      "Recent average reward: 18.8\n",
      "Reward over last 100: 16.43\n",
      "Average over all episodes so far: -5.6177777777777775\n",
      "epsilon: 0.01\n",
      "Elapsed time:  05:45:32\n",
      "**** Episode  460 **** \n",
      "Recent average reward: 19.4\n",
      "Reward over last 100: 17.22\n",
      "Average over all episodes so far: -5.073913043478261\n",
      "epsilon: 0.01\n",
      "Elapsed time:  05:51:52\n",
      "**** Episode  470 **** \n",
      "Recent average reward: 17.2\n",
      "Reward over last 100: 17.61\n",
      "Average over all episodes so far: -4.6\n",
      "epsilon: 0.01\n",
      "Elapsed time:  05:58:40\n",
      "**** Episode  480 **** \n",
      "Recent average reward: 18.1\n",
      "Reward over last 100: 17.7\n",
      "Average over all episodes so far: -4.127083333333333\n",
      "epsilon: 0.01\n",
      "Elapsed time:  06:05:30\n",
      "**** Episode  490 **** \n",
      "Recent average reward: 17.8\n",
      "Reward over last 100: 17.75\n",
      "Average over all episodes so far: -3.679591836734694\n",
      "epsilon: 0.01\n",
      "Elapsed time:  06:12:09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLVED! After 497 episodes \n",
      "**** Episode  500 **** \n",
      "Recent average reward: 18.0\n",
      "Reward over last 100: 17.96\n",
      "Average over all episodes so far: -3.246\n",
      "epsilon: 0.01\n",
      "Elapsed time:  06:18:38\n",
      "**** Episode  510 **** \n",
      "Recent average reward: 19.3\n",
      "Reward over last 100: 18.26\n",
      "Average over all episodes so far: -2.803921568627451\n",
      "epsilon: 0.01\n",
      "Elapsed time:  06:24:55\n",
      "**** Episode  520 **** \n",
      "Recent average reward: 20.0\n",
      "Reward over last 100: 18.51\n",
      "Average over all episodes so far: -2.3653846153846154\n",
      "epsilon: 0.01\n",
      "Elapsed time:  06:31:02\n",
      "**** Episode  530 **** \n",
      "Recent average reward: 20.0\n",
      "Reward over last 100: 18.75\n",
      "Average over all episodes so far: -1.9433962264150944\n",
      "epsilon: 0.01\n",
      "Elapsed time:  06:36:36\n",
      "**** Episode  540 **** \n",
      "Recent average reward: 19.6\n",
      "Reward over last 100: 18.82\n",
      "Average over all episodes so far: -1.5444444444444445\n",
      "epsilon: 0.01\n",
      "Elapsed time:  06:42:30\n",
      "**** Episode  550 **** \n",
      "Recent average reward: 19.1\n",
      "Reward over last 100: 18.85\n",
      "Average over all episodes so far: -1.1690909090909092\n",
      "epsilon: 0.01\n",
      "Elapsed time:  06:48:09\n",
      "**** Episode  560 **** \n",
      "Recent average reward: 18.2\n",
      "Reward over last 100: 18.73\n",
      "Average over all episodes so far: -0.8232142857142857\n",
      "epsilon: 0.01\n",
      "Elapsed time:  06:54:12\n",
      "**** Episode  570 **** \n",
      "Recent average reward: 18.6\n",
      "Reward over last 100: 18.87\n",
      "Average over all episodes so far: -0.4824561403508772\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:00:18\n",
      "**** Episode  580 **** \n",
      "Recent average reward: 19.3\n",
      "Reward over last 100: 18.99\n",
      "Average over all episodes so far: -0.1413793103448276\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:06:11\n",
      "**** Episode  590 **** \n",
      "Recent average reward: 18.7\n",
      "Reward over last 100: 19.08\n",
      "Average over all episodes so far: 0.17796610169491525\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:12:04\n",
      "**** Episode  600 **** \n",
      "Recent average reward: 18.4\n",
      "Reward over last 100: 19.12\n",
      "Average over all episodes so far: 0.4816666666666667\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:17:48\n",
      "**** Episode  610 **** \n",
      "Recent average reward: 19.5\n",
      "Reward over last 100: 19.14\n",
      "Average over all episodes so far: 0.7934426229508197\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:23:21\n",
      "**** Episode  620 **** \n",
      "Recent average reward: 19.0\n",
      "Reward over last 100: 19.04\n",
      "Average over all episodes so far: 1.0870967741935484\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:28:53\n",
      "**** Episode  630 **** \n",
      "Recent average reward: 17.2\n",
      "Reward over last 100: 18.76\n",
      "Average over all episodes so far: 1.3428571428571427\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:34:57\n",
      "**** Episode  640 **** \n",
      "Recent average reward: 18.0\n",
      "Reward over last 100: 18.6\n",
      "Average over all episodes so far: 1.603125\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:40:47\n",
      "**** Episode  650 **** \n",
      "Recent average reward: 19.2\n",
      "Reward over last 100: 18.61\n",
      "Average over all episodes so far: 1.873846153846154\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:46:34\n",
      "**** Episode  660 **** \n",
      "Recent average reward: 19.4\n",
      "Reward over last 100: 18.73\n",
      "Average over all episodes so far: 2.139393939393939\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:52:18\n",
      "**** Episode  670 **** \n",
      "Recent average reward: 19.6\n",
      "Reward over last 100: 18.83\n",
      "Average over all episodes so far: 2.4\n",
      "epsilon: 0.01\n",
      "Elapsed time:  07:58:07\n",
      "**** Episode  680 **** \n",
      "Recent average reward: 19.3\n",
      "Reward over last 100: 18.83\n",
      "Average over all episodes so far: 2.648529411764706\n",
      "epsilon: 0.01\n",
      "Elapsed time:  08:03:39\n",
      "**** Episode  690 **** \n",
      "Recent average reward: 19.7\n",
      "Reward over last 100: 18.93\n",
      "Average over all episodes so far: 2.8956521739130436\n",
      "epsilon: 0.01\n",
      "Elapsed time:  08:09:21\n",
      "**** Episode  700 **** \n",
      "Recent average reward: 19.5\n",
      "Reward over last 100: 19.04\n",
      "Average over all episodes so far: 3.132857142857143\n",
      "epsilon: 0.01\n",
      "Elapsed time:  08:15:02\n",
      "**** Episode  710 **** \n",
      "Recent average reward: 18.5\n",
      "Reward over last 100: 18.94\n",
      "Average over all episodes so far: 3.3492957746478873\n",
      "epsilon: 0.01\n",
      "Elapsed time:  08:20:47\n",
      "**** Episode  720 **** \n",
      "Recent average reward: 19.2\n",
      "Reward over last 100: 18.96\n",
      "Average over all episodes so far: 3.5694444444444446\n",
      "epsilon: 0.01\n",
      "Elapsed time:  08:26:42\n",
      "**** Episode  730 **** \n",
      "Recent average reward: 18.1\n",
      "Reward over last 100: 19.05\n",
      "Average over all episodes so far: 3.7684931506849315\n",
      "epsilon: 0.01\n",
      "Elapsed time:  08:32:48\n",
      "**** Episode  740 **** \n",
      "Recent average reward: 19.0\n",
      "Reward over last 100: 19.15\n",
      "Average over all episodes so far: 3.9743243243243245\n",
      "epsilon: 0.01\n",
      "Elapsed time:  08:38:42\n",
      "**** Episode  750 **** \n",
      "Recent average reward: 19.4\n",
      "Reward over last 100: 19.17\n",
      "Average over all episodes so far: 4.18\n",
      "epsilon: 0.01\n",
      "Elapsed time:  08:44:32\n",
      "**** Episode  760 **** \n",
      "Recent average reward: 18.3\n",
      "Reward over last 100: 19.06\n",
      "Average over all episodes so far: 4.36578947368421\n",
      "epsilon: 0.01\n",
      "Elapsed time:  08:50:28\n",
      "**** Episode  770 **** \n",
      "Recent average reward: 19.1\n",
      "Reward over last 100: 19.01\n",
      "Average over all episodes so far: 4.557142857142857\n",
      "epsilon: 0.01\n",
      "Elapsed time:  08:56:20\n",
      "**** Episode  780 **** \n",
      "Recent average reward: 19.4\n",
      "Reward over last 100: 19.02\n",
      "Average over all episodes so far: 4.7474358974358974\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:02:06\n",
      "**** Episode  790 **** \n",
      "Recent average reward: 19.1\n",
      "Reward over last 100: 18.96\n",
      "Average over all episodes so far: 4.9291139240506325\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:07:51\n",
      "**** Episode  800 **** \n",
      "Recent average reward: 19.7\n",
      "Reward over last 100: 18.98\n",
      "Average over all episodes so far: 5.11375\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:13:22\n",
      "**** Episode  810 **** \n",
      "Recent average reward: 20.0\n",
      "Reward over last 100: 19.13\n",
      "Average over all episodes so far: 5.2975308641975305\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:18:58\n",
      "**** Episode  820 **** \n",
      "Recent average reward: 17.9\n",
      "Reward over last 100: 19.0\n",
      "Average over all episodes so far: 5.451219512195122\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:25:04\n",
      "**** Episode  830 **** \n",
      "Recent average reward: 19.3\n",
      "Reward over last 100: 19.12\n",
      "Average over all episodes so far: 5.618072289156626\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:30:57\n",
      "**** Episode  840 **** \n",
      "Recent average reward: 19.5\n",
      "Reward over last 100: 19.17\n",
      "Average over all episodes so far: 5.783333333333333\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:36:54\n",
      "**** Episode  850 **** \n",
      "Recent average reward: 17.8\n",
      "Reward over last 100: 19.01\n",
      "Average over all episodes so far: 5.924705882352941\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:42:24\n",
      "**** Episode  860 **** \n",
      "Recent average reward: 18.3\n",
      "Reward over last 100: 19.01\n",
      "Average over all episodes so far: 6.068604651162791\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:48:09\n",
      "**** Episode  870 **** \n",
      "Recent average reward: 18.9\n",
      "Reward over last 100: 18.99\n",
      "Average over all episodes so far: 6.216091954022988\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:53:52\n",
      "**** Episode  880 **** \n",
      "Recent average reward: 18.5\n",
      "Reward over last 100: 18.9\n",
      "Average over all episodes so far: 6.355681818181818\n",
      "epsilon: 0.01\n",
      "Elapsed time:  09:59:41\n",
      "**** Episode  890 **** \n",
      "Recent average reward: 19.5\n",
      "Reward over last 100: 18.94\n",
      "Average over all episodes so far: 6.503370786516854\n",
      "epsilon: 0.01\n",
      "Elapsed time:  10:05:16\n",
      "**** Episode  900 **** \n",
      "Recent average reward: 19.8\n",
      "Reward over last 100: 18.95\n",
      "Average over all episodes so far: 6.651111111111111\n",
      "epsilon: 0.01\n",
      "Elapsed time:  10:10:57\n",
      "**** Episode  910 **** \n",
      "Recent average reward: 19.3\n",
      "Reward over last 100: 18.88\n",
      "Average over all episodes so far: 6.79010989010989\n",
      "epsilon: 0.01\n",
      "Elapsed time:  10:16:52\n",
      "**** Episode  920 **** \n",
      "Recent average reward: 19.6\n",
      "Reward over last 100: 19.05\n",
      "Average over all episodes so far: 6.929347826086956\n",
      "epsilon: 0.01\n",
      "Elapsed time:  10:22:30\n",
      "**** Episode  930 **** \n",
      "Recent average reward: 18.6\n",
      "Reward over last 100: 18.98\n",
      "Average over all episodes so far: 7.054838709677419\n",
      "epsilon: 0.01\n",
      "Elapsed time:  10:28:22\n",
      "**** Episode  940 **** \n",
      "Recent average reward: 19.6\n",
      "Reward over last 100: 18.99\n",
      "Average over all episodes so far: 7.188297872340425\n",
      "epsilon: 0.01\n",
      "Elapsed time:  10:33:52\n",
      "**** Episode  950 **** \n",
      "Recent average reward: 20.2\n",
      "Reward over last 100: 19.23\n",
      "Average over all episodes so far: 7.325263157894737\n",
      "epsilon: 0.01\n",
      "Elapsed time:  10:39:19\n",
      "**** Episode  960 **** \n",
      "Recent average reward: 20.0\n",
      "Reward over last 100: 19.4\n",
      "Average over all episodes so far: 7.457291666666666\n",
      "epsilon: 0.01\n",
      "Elapsed time:  10:44:47\n",
      "**** Episode  970 **** \n",
      "Recent average reward: 20.5\n",
      "Reward over last 100: 19.56\n",
      "Average over all episodes so far: 7.591752577319587\n",
      "epsilon: 0.01\n",
      "Elapsed time:  10:50:21\n",
      "**** Episode  980 **** \n",
      "Recent average reward: 19.0\n",
      "Reward over last 100: 19.61\n",
      "Average over all episodes so far: 7.708163265306123\n",
      "epsilon: 0.01\n",
      "Elapsed time:  10:55:57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  990 **** \n",
      "Recent average reward: 19.6\n",
      "Reward over last 100: 19.62\n",
      "Average over all episodes so far: 7.828282828282828\n",
      "epsilon: 0.01\n",
      "Elapsed time:  11:01:47\n",
      "Solved after 497 episodes\n"
     ]
    }
   ],
   "source": [
    "%lprun -f train_agent -f QNet_Agent.optimize -f NeuralNetworkDueling.forward -f ExperienceReplay.push -f ExperienceReplay.sample train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward_total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-45819bca371e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score at end of episode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reward_total' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2,figsize=[12,5])\n",
    "plt.title(\"Score at end of episode\")\n",
    "plt.plot(reward_total[:i_episode])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of steps: {}\". format(np.average(steps_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of steps: {}\". format(np.average(steps_total)))\n",
    "print(\"Average number of steps in last 100 episodes: {}\". format(np.average(steps_total[-100:])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(1,figsize=[12,5])\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total,alpha=0.6, color='green')\n",
    "#plt.plot(rewards_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2,figsize=[12,5])\n",
    "plt.title(\"Steps to finish episode\")\n",
    "plt.plot(:i_episode)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
