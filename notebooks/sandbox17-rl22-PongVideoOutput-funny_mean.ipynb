{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../baselines')\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id=\"PongNoFrameskip-v4\"\n",
    "env=make_atari(env_id)\n",
    "env=wrap_deepmind(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, episode,step, info=\"\"):\n",
    "    plt.figure(99999,figsize=[8,6])\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"episode: {} step: {} \".format(episode,step))\n",
    "    #plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device=torch.device(\"cuda:4\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PARAMS ######\n",
    "learning_rate = 0.0001\n",
    "num_episodes = 10000\n",
    "gamma=0.99\n",
    "#gamma=0.85\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 10000\n",
    "\n",
    "report_interval=10\n",
    "\n",
    "score_to_solve = 18.0\n",
    "\n",
    "hidden_layer_size=512\n",
    "\n",
    "replay_memory_size=100000\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "update_target_frequency = 5000\n",
    "\n",
    "clip_error=True\n",
    "normalize_image=True\n",
    "\n",
    "double_dqn=True\n",
    "\n",
    "file2save = 'pong_save_funny_mean.pth'\n",
    "\n",
    "save_model_frequency=10000\n",
    "\n",
    "resume_previous_training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.wrappers.Monitor(env, '../mp4/sandbox10',video_callable=lambda episode_id: True,force=True)\n",
    "env = gym.wrappers.Monitor(env, '../mp4/PongVideos_funny_mean',video_callable=lambda episode_id: episode_id%20==0, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_inputs=env.observation_space.shape[0]\n",
    "number_of_outputs=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Discrete.contains of Discrete(6)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    return torch.load(file2save)\n",
    "\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), file2save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = frame.transpose((2,0,1))\n",
    "    frame = torch.from_numpy(frame)\n",
    "    frame = frame.to(device, dtype=torch.float32)\n",
    "    frame = frame.unsqueeze(0)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results():\n",
    "    plt.figure(figsize=[12,5])\n",
    "    plt.title(\"Score at end of episode\")\n",
    "    plt.plot(reward_total[:i_episode],color='red')\n",
    "    plt.savefig(\"Pong-results-funny_mean.png\")\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19.]])\n"
     ]
    }
   ],
   "source": [
    "o=torch.arange(20,dtype=torch.float32).view(-1,5)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity=capacity\n",
    "        self.memory=[]\n",
    "        self.position=0\n",
    "        \n",
    "    \n",
    "    def push(self, state,\n",
    "             action, new_state,\n",
    "             reward, done):\n",
    "        \n",
    "            transition=(state,action,new_state,reward,done)\n",
    "            \n",
    "            if self.position>=len(self.memory):\n",
    "                self.memory.append(transition)\n",
    "            else:\n",
    "                self.memory[self.position]=transition\n",
    "                \n",
    "            self.position=(self.position+1)%self.capacity\n",
    "        \n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkDueling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkDueling, self).__init__()\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8 ,stride=4)\n",
    "        self.conv2=nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4 ,stride=2)\n",
    "        self.conv3=nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3 ,stride=1)\n",
    "        \n",
    "        self.advantage1 = nn.Linear(7*7*64, hidden_layer_size)\n",
    "        self.advantage2 = nn.Linear(hidden_layer_size, number_of_outputs)\n",
    "        \n",
    "        self.value1 = nn.Linear(7*7*64,hidden_layer_size)\n",
    "        self.value2 = nn.Linear(hidden_layer_size,1)\n",
    "        \n",
    "        #self.activation=nn.Tanh()\n",
    "        self.activation=nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print('x shape {} and value:'.format(x.shape))\n",
    "        #print(x.detach().cpu())\n",
    "        \n",
    "        if normalize_image:\n",
    "            x=x/255.0\n",
    "        \n",
    "        output_conv = self.conv1(x)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv2(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        output_conv = self.conv3(output_conv)\n",
    "        output_conv = self.activation(output_conv)\n",
    "        \n",
    "        output_conv = output_conv.view(output_conv.shape[0],-1)\n",
    "        \n",
    "        output_advantage=self.advantage1(output_conv)\n",
    "        output_advantage=self.activation(output_advantage)\n",
    "        output_advantage=self.advantage2(output_advantage)\n",
    "        \n",
    "        output_value=self.value1(output_conv)\n",
    "        output_value=self.activation(output_value)\n",
    "        output_value=self.value2(output_value)\n",
    "        \n",
    "        #print('output_advantage shape {} and value:'.format(output_advantage.shape))\n",
    "        #print(output_advantage.detach().cpu())\n",
    "        \n",
    "        #print('output_value shape {} and value:'.format(output_value.shape))\n",
    "        #print(output_value.detach().cpu())\n",
    "        \n",
    "        #print('output_advantage.mean shape {} and value:'.format(output_advantage.mean(dim=1,keepdim=True).shape))\n",
    "        #print(output_advantage.mean(dim=1,keepdim=True).detach().cpu())\n",
    "        \n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "        \n",
    "        #output_final=output_value+output_advantage-output_advantage.mean(dim=1,keepdim=True)\n",
    "        \n",
    "        \n",
    "        return output_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer_size)\n",
    "        self.linear2 = nn.Linear(hidden_layer_size,number_of_outputs)\n",
    "        \n",
    "        self.activation=nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet_Agent():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.nn = NeuralNetworkDueling().to(device)\n",
    "        self.target_nn = NeuralNetworkDueling().to(device)\n",
    "        \n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.number_of_frames = 0\n",
    "        \n",
    "        if resume_previous_training and os.path.exists(file2save):\n",
    "            print('loading previous model')\n",
    "            self.nn.load_state_dict(load_model())\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy=torch.rand(1).item()\n",
    "        \n",
    "        if random_for_egreedy>epsilon:\n",
    "            self.nn.eval()\n",
    "            with torch.no_grad():\n",
    "                state=preprocess_frame(state)\n",
    "                #state=torch.Tensor(state).to(device)\n",
    "                predicted_value_from_nn=self.nn(state).squeeze()\n",
    "                #print('predicted value from nn:')\n",
    "                #print(predicted_value_from_nn)\n",
    "                action=torch.argmax(predicted_value_from_nn).item()\n",
    "                #print('action: {}'.format(action))\n",
    "        else:\n",
    "            action=env.action_space.sample()\n",
    "                \n",
    "                \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if len(memory)<batch_size:\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state=[preprocess_frame(frame) for frame in state] \n",
    "        state=torch.cat(state)\n",
    "        \n",
    "        new_state=[preprocess_frame(frame) for frame in new_state] \n",
    "        new_state=torch.cat(new_state)\n",
    "        \n",
    "        #print('state batch shape {}'.format(state.shape))\n",
    "        #print(state)\n",
    "        \n",
    "        #state=torch.Tensor(state).to(device)\n",
    "        #new_state=torch.Tensor(new_state).to(device)\n",
    "        \n",
    "        \n",
    "        reward=torch.Tensor(reward).to(device)\n",
    "        \n",
    "        #the view call below is to transform into column vector\n",
    "        #so that it can be used in the gather call\n",
    "        #i.e. we will use it to pick out from the computed value\n",
    "        #tensor only values indexed by selected action\n",
    "        action=(torch.Tensor(action).view(-1,1).long()).to(device)\n",
    "        #print('action: ')\n",
    "        #print(action)\n",
    "        #print('contiguous?', action.is_contiguous())\n",
    "        done=torch.Tensor(done).to(device)\n",
    "        \n",
    "        #print('shape of: state, new state, reward, action, done:')\n",
    "        #print(state.shape)\n",
    "        #print(new_state.shape)\n",
    "        #print(reward.shape)\n",
    "        #print(action.shape)\n",
    "        #print(done.shape)\n",
    "        \n",
    "        \n",
    "        self.nn.eval()\n",
    "        self.target_nn.eval()\n",
    "            \n",
    "            \n",
    "        if double_dqn:\n",
    "            #print('in double DQN')\n",
    "            new_state_values_from_nn=self.nn(new_state).detach()\n",
    "            #print('new_state_values_from_nn shape {} and value:'.format(new_state_values_from_nn.shape))\n",
    "            #print(new_state_values_from_nn)\n",
    "            max_new_state_indexes=torch.max(new_state_values_from_nn,dim=1)[1].view(-1,1)\n",
    "            #print('max_new_state_indexes shape {} and value:'.format(max_new_state_indexes.shape))\n",
    "            #print(max_new_state_indexes)\n",
    "            new_state_values=self.target_nn(new_state).detach()\n",
    "            #print('new_state_values shape {} and value:'.format(new_state_values.shape))\n",
    "            #print(new_state_values)\n",
    "            max_new_state_values=torch.gather(new_state_values,1,max_new_state_indexes).squeeze()\n",
    "            #print('max_new_state_values shape {} and value:'.format(max_new_state_values.shape))\n",
    "            #print(max_new_state_values)\n",
    "        else:\n",
    "            #print('in regular DQN')\n",
    "            new_state_values=self.target_nn(new_state).detach()\n",
    "            #print('new_state_values shape {} and value'.format(new_state_values.shape))\n",
    "            #print(new_state_values)\n",
    "        \n",
    "            max_new_state_values=torch.max(new_state_values,dim=1)[0]\n",
    "            #print('max_new_state_values shape {} and value'.format(max_new_state_values.shape))\n",
    "            #print(max_new_state_values)\n",
    "            \n",
    "        target_value=(reward + (1-done)*gamma*max_new_state_values).view(-1,1)\n",
    "        \n",
    "        #print('shape of: target_value')\n",
    "        #print(target_value.shape)\n",
    "        self.nn.train()\n",
    "        \n",
    "        #this will select only the values of the desired actions\n",
    "        predicted_value=torch.gather(self.nn(state),1,action)\n",
    "        #print('shape of: predicted_value')\n",
    "        #print(predicted_value.shape)\n",
    "        \n",
    "        \n",
    "        loss=self.loss_function(predicted_value,target_value)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.clamp_(-1.0,1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.number_of_frames % update_target_frequency == 0:\n",
    "            #print(\"***********************\")\n",
    "            #print(\"UPDATING TARGET NETWORK\")\n",
    "            #print(\"update counter: {}\".format(self.update_target_counter))\n",
    "            #print(\"***********************\")\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "            \n",
    "        if self.number_of_frames % save_model_frequency ==0:\n",
    "            save_model(self.nn)\n",
    "        \n",
    "        self.number_of_frames+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ExperienceReplay(replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet_agent=QNet_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value=23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  10 **** \n",
      "Recent average reward: -20.4\n",
      "Average over all episodes so far: -20.4\n",
      "epsilon: 0.3348688176110784\n",
      "**** Episode  20 **** \n",
      "Recent average reward: -20.2\n",
      "Average over all episodes so far: -20.3\n",
      "epsilon: 0.1433158950452933\n",
      "**** Episode  30 **** \n",
      "Recent average reward: -19.5\n",
      "Average over all episodes so far: -20.033333333333335\n",
      "epsilon: 0.06028573621951299\n",
      "**** Episode  40 **** \n",
      "Recent average reward: -20.2\n",
      "Average over all episodes so far: -20.075\n",
      "epsilon: 0.030922423168563633\n",
      "**** Episode  50 **** \n",
      "Recent average reward: -19.9\n",
      "Average over all episodes so far: -20.04\n",
      "epsilon: 0.018594491396395088\n",
      "**** Episode  60 **** \n",
      "Recent average reward: -20.0\n",
      "Average over all episodes so far: -20.033333333333335\n",
      "epsilon: 0.013352885983985294\n",
      "**** Episode  70 **** \n",
      "Recent average reward: -20.0\n",
      "Average over all episodes so far: -20.02857142857143\n",
      "epsilon: 0.011185330291466842\n",
      "**** Episode  80 **** \n",
      "Recent average reward: -16.9\n",
      "Average over all episodes so far: -19.6375\n",
      "epsilon: 0.010271178355209926\n",
      "**** Episode  90 **** \n",
      "Recent average reward: -17.4\n",
      "Average over all episodes so far: -19.38888888888889\n",
      "epsilon: 0.010060538331426462\n",
      "**** Episode  100 **** \n",
      "Recent average reward: -16.7\n",
      "Average over all episodes so far: -19.12\n",
      "epsilon: 0.010011924281003013\n",
      "**** Episode  110 **** \n",
      "Recent average reward: -16.7\n",
      "Reward over last 100: -18.75\n",
      "Average over all episodes so far: -18.9\n",
      "epsilon: 0.010002158636433804\n",
      "**** Episode  120 **** \n",
      "Recent average reward: -15.2\n",
      "Reward over last 100: -18.25\n",
      "Average over all episodes so far: -18.591666666666665\n",
      "epsilon: 0.010000345617274819\n",
      "**** Episode  130 **** \n",
      "Recent average reward: -15.7\n",
      "Reward over last 100: -17.87\n",
      "Average over all episodes so far: -18.369230769230768\n",
      "epsilon: 0.010000051869555597\n",
      "**** Episode  140 **** \n",
      "Recent average reward: -14.5\n",
      "Reward over last 100: -17.3\n",
      "Average over all episodes so far: -18.09285714285714\n",
      "epsilon: 0.010000008981186682\n",
      "**** Episode  150 **** \n",
      "Recent average reward: -13.9\n",
      "Reward over last 100: -16.7\n",
      "Average over all episodes so far: -17.813333333333333\n",
      "epsilon: 0.010000001324628471\n",
      "**** Episode  160 **** \n",
      "Recent average reward: -15.0\n",
      "Reward over last 100: -16.2\n",
      "Average over all episodes so far: -17.6375\n",
      "epsilon: 0.010000000247420001\n",
      "**** Episode  170 **** \n",
      "Recent average reward: -13.4\n",
      "Reward over last 100: -15.54\n",
      "Average over all episodes so far: -17.388235294117646\n",
      "epsilon: 0.010000000029635924\n",
      "**** Episode  180 **** \n",
      "Recent average reward: -11.7\n",
      "Reward over last 100: -15.02\n",
      "Average over all episodes so far: -17.072222222222223\n",
      "epsilon: 0.010000000002702528\n",
      "**** Episode  190 **** \n",
      "Recent average reward: -10.7\n",
      "Reward over last 100: -14.35\n",
      "Average over all episodes so far: -16.736842105263158\n",
      "epsilon: 0.010000000000244752\n",
      "**** Episode  200 **** \n",
      "Recent average reward: -10.4\n",
      "Reward over last 100: -13.72\n",
      "Average over all episodes so far: -16.42\n",
      "epsilon: 0.010000000000017726\n",
      "**** Episode  210 **** \n",
      "Recent average reward: -9.9\n",
      "Reward over last 100: -13.04\n",
      "Average over all episodes so far: -16.10952380952381\n",
      "epsilon: 0.01000000000000113\n",
      "**** Episode  220 **** \n",
      "Recent average reward: -8.9\n",
      "Reward over last 100: -12.41\n",
      "Average over all episodes so far: -15.781818181818181\n",
      "epsilon: 0.010000000000000057\n",
      "**** Episode  230 **** \n",
      "Recent average reward: -9.2\n",
      "Reward over last 100: -11.76\n",
      "Average over all episodes so far: -15.495652173913044\n",
      "epsilon: 0.010000000000000004\n",
      "**** Episode  240 **** \n",
      "Recent average reward: -7.3\n",
      "Reward over last 100: -11.04\n",
      "Average over all episodes so far: -15.154166666666667\n",
      "epsilon: 0.01\n",
      "**** Episode  250 **** \n",
      "Recent average reward: -2.9\n",
      "Reward over last 100: -9.94\n",
      "Average over all episodes so far: -14.664\n",
      "epsilon: 0.01\n",
      "**** Episode  260 **** \n",
      "Recent average reward: -1.8\n",
      "Reward over last 100: -8.62\n",
      "Average over all episodes so far: -14.169230769230769\n",
      "epsilon: 0.01\n",
      "**** Episode  270 **** \n",
      "Recent average reward: -1.4\n",
      "Reward over last 100: -7.42\n",
      "Average over all episodes so far: -13.696296296296296\n",
      "epsilon: 0.01\n",
      "**** Episode  280 **** \n",
      "Recent average reward: 3.0\n",
      "Reward over last 100: -5.95\n",
      "Average over all episodes so far: -13.1\n",
      "epsilon: 0.01\n",
      "**** Episode  290 **** \n",
      "Recent average reward: 5.1\n",
      "Reward over last 100: -4.37\n",
      "Average over all episodes so far: -12.472413793103449\n",
      "epsilon: 0.01\n",
      "**** Episode  300 **** \n",
      "Recent average reward: 8.8\n",
      "Reward over last 100: -2.45\n",
      "Average over all episodes so far: -11.763333333333334\n",
      "epsilon: 0.01\n",
      "**** Episode  310 **** \n",
      "Recent average reward: 13.2\n",
      "Reward over last 100: -0.14\n",
      "Average over all episodes so far: -10.958064516129033\n",
      "epsilon: 0.01\n",
      "**** Episode  320 **** \n",
      "Recent average reward: 14.5\n",
      "Reward over last 100: 2.2\n",
      "Average over all episodes so far: -10.1625\n",
      "epsilon: 0.01\n",
      "**** Episode  330 **** \n",
      "Recent average reward: 15.0\n",
      "Reward over last 100: 4.62\n",
      "Average over all episodes so far: -9.4\n",
      "epsilon: 0.01\n",
      "**** Episode  340 **** \n",
      "Recent average reward: 15.6\n",
      "Reward over last 100: 6.91\n",
      "Average over all episodes so far: -8.66470588235294\n",
      "epsilon: 0.01\n",
      "**** Episode  350 **** \n",
      "Recent average reward: 17.8\n",
      "Reward over last 100: 8.98\n",
      "Average over all episodes so far: -7.908571428571428\n",
      "epsilon: 0.01\n",
      "**** Episode  360 **** \n",
      "Recent average reward: 17.4\n",
      "Reward over last 100: 10.9\n",
      "Average over all episodes so far: -7.205555555555556\n",
      "epsilon: 0.01\n",
      "**** Episode  370 **** \n",
      "Recent average reward: 18.8\n",
      "Reward over last 100: 12.92\n",
      "Average over all episodes so far: -6.5027027027027025\n",
      "epsilon: 0.01\n",
      "**** Episode  380 **** \n",
      "Recent average reward: 19.2\n",
      "Reward over last 100: 14.54\n",
      "Average over all episodes so far: -5.826315789473684\n",
      "epsilon: 0.01\n",
      "**** Episode  390 **** \n",
      "Recent average reward: 17.7\n",
      "Reward over last 100: 15.8\n",
      "Average over all episodes so far: -5.223076923076923\n",
      "epsilon: 0.01\n",
      "**** Episode  400 **** \n",
      "Recent average reward: 19.0\n",
      "Reward over last 100: 16.82\n",
      "Average over all episodes so far: -4.6175\n",
      "epsilon: 0.01\n",
      "**** Episode  410 **** \n",
      "Recent average reward: 16.0\n",
      "Reward over last 100: 17.1\n",
      "Average over all episodes so far: -4.114634146341463\n",
      "epsilon: 0.01\n",
      "**** Episode  420 **** \n",
      "Recent average reward: 18.9\n",
      "Reward over last 100: 17.54\n",
      "Average over all episodes so far: -3.566666666666667\n",
      "epsilon: 0.01\n",
      "**** Episode  430 **** \n",
      "Recent average reward: 18.4\n",
      "Reward over last 100: 17.88\n",
      "Average over all episodes so far: -3.055813953488372\n",
      "epsilon: 0.01\n",
      "SOLVED! After 433 episodes \n",
      "**** Episode  440 **** \n",
      "Recent average reward: 19.0\n",
      "Reward over last 100: 18.22\n",
      "Average over all episodes so far: -2.5545454545454547\n",
      "epsilon: 0.01\n",
      "**** Episode  450 **** \n",
      "Recent average reward: 18.5\n",
      "Reward over last 100: 18.29\n",
      "Average over all episodes so far: -2.0866666666666664\n",
      "epsilon: 0.01\n",
      "**** Episode  460 **** \n",
      "Recent average reward: 18.4\n",
      "Reward over last 100: 18.39\n",
      "Average over all episodes so far: -1.641304347826087\n",
      "epsilon: 0.01\n",
      "**** Episode  470 **** \n",
      "Recent average reward: 20.0\n",
      "Reward over last 100: 18.51\n",
      "Average over all episodes so far: -1.1808510638297873\n",
      "epsilon: 0.01\n",
      "**** Episode  480 **** \n",
      "Recent average reward: 18.8\n",
      "Reward over last 100: 18.47\n",
      "Average over all episodes so far: -0.7645833333333333\n",
      "epsilon: 0.01\n",
      "**** Episode  490 **** \n",
      "Recent average reward: 18.5\n",
      "Reward over last 100: 18.55\n",
      "Average over all episodes so far: -0.37142857142857144\n",
      "epsilon: 0.01\n",
      "**** Episode  500 **** \n",
      "Recent average reward: 18.8\n",
      "Reward over last 100: 18.53\n",
      "Average over all episodes so far: 0.012\n",
      "epsilon: 0.01\n",
      "**** Episode  510 **** \n",
      "Recent average reward: 19.0\n",
      "Reward over last 100: 18.83\n",
      "Average over all episodes so far: 0.3843137254901961\n",
      "epsilon: 0.01\n",
      "**** Episode  520 **** \n",
      "Recent average reward: 18.2\n",
      "Reward over last 100: 18.76\n",
      "Average over all episodes so far: 0.7269230769230769\n",
      "epsilon: 0.01\n",
      "**** Episode  530 **** \n",
      "Recent average reward: 17.6\n",
      "Reward over last 100: 18.68\n",
      "Average over all episodes so far: 1.0452830188679245\n",
      "epsilon: 0.01\n",
      "**** Episode  540 **** \n",
      "Recent average reward: 18.5\n",
      "Reward over last 100: 18.63\n",
      "Average over all episodes so far: 1.3685185185185185\n",
      "epsilon: 0.01\n",
      "**** Episode  550 **** \n",
      "Recent average reward: 19.4\n",
      "Reward over last 100: 18.72\n",
      "Average over all episodes so far: 1.6963636363636363\n",
      "epsilon: 0.01\n",
      "**** Episode  560 **** \n",
      "Recent average reward: 19.1\n",
      "Reward over last 100: 18.79\n",
      "Average over all episodes so far: 2.007142857142857\n",
      "epsilon: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Episode  570 **** \n",
      "Recent average reward: 19.6\n",
      "Reward over last 100: 18.75\n",
      "Average over all episodes so far: 2.3157894736842106\n",
      "epsilon: 0.01\n",
      "**** Episode  580 **** \n",
      "Recent average reward: 17.2\n",
      "Reward over last 100: 18.59\n",
      "Average over all episodes so far: 2.5724137931034483\n",
      "epsilon: 0.01\n",
      "**** Episode  590 **** \n",
      "Recent average reward: 20.1\n",
      "Reward over last 100: 18.75\n",
      "Average over all episodes so far: 2.869491525423729\n",
      "epsilon: 0.01\n",
      "**** Episode  600 **** \n",
      "Recent average reward: 18.7\n",
      "Reward over last 100: 18.74\n",
      "Average over all episodes so far: 3.1333333333333333\n",
      "epsilon: 0.01\n",
      "**** Episode  610 **** \n",
      "Recent average reward: 17.8\n",
      "Reward over last 100: 18.62\n",
      "Average over all episodes so far: 3.3737704918032785\n",
      "epsilon: 0.01\n",
      "**** Episode  620 **** \n",
      "Recent average reward: 19.2\n",
      "Reward over last 100: 18.72\n",
      "Average over all episodes so far: 3.629032258064516\n",
      "epsilon: 0.01\n",
      "**** Episode  630 **** \n",
      "Recent average reward: 18.1\n",
      "Reward over last 100: 18.77\n",
      "Average over all episodes so far: 3.858730158730159\n",
      "epsilon: 0.01\n",
      "**** Episode  640 **** \n",
      "Recent average reward: 18.4\n",
      "Reward over last 100: 18.76\n",
      "Average over all episodes so far: 4.0859375\n",
      "epsilon: 0.01\n",
      "**** Episode  650 **** \n",
      "Recent average reward: 19.1\n",
      "Reward over last 100: 18.73\n",
      "Average over all episodes so far: 4.316923076923077\n",
      "epsilon: 0.01\n",
      "**** Episode  660 **** \n",
      "Recent average reward: 19.2\n",
      "Reward over last 100: 18.74\n",
      "Average over all episodes so far: 4.542424242424242\n",
      "epsilon: 0.01\n",
      "**** Episode  670 **** \n",
      "Recent average reward: 18.9\n",
      "Reward over last 100: 18.67\n",
      "Average over all episodes so far: 4.7567164179104475\n",
      "epsilon: 0.01\n",
      "**** Episode  680 **** \n",
      "Recent average reward: 19.0\n",
      "Reward over last 100: 18.85\n",
      "Average over all episodes so far: 4.966176470588235\n",
      "epsilon: 0.01\n",
      "**** Episode  690 **** \n",
      "Recent average reward: 19.6\n",
      "Reward over last 100: 18.8\n",
      "Average over all episodes so far: 5.178260869565217\n",
      "epsilon: 0.01\n",
      "**** Episode  700 **** \n",
      "Recent average reward: 19.3\n",
      "Reward over last 100: 18.86\n",
      "Average over all episodes so far: 5.38\n",
      "epsilon: 0.01\n",
      "**** Episode  710 **** \n",
      "Recent average reward: 16.5\n",
      "Reward over last 100: 18.73\n",
      "Average over all episodes so far: 5.5366197183098596\n",
      "epsilon: 0.01\n",
      "**** Episode  720 **** \n",
      "Recent average reward: 18.9\n",
      "Reward over last 100: 18.7\n",
      "Average over all episodes so far: 5.722222222222222\n",
      "epsilon: 0.01\n",
      "**** Episode  730 **** \n",
      "Recent average reward: 18.4\n",
      "Reward over last 100: 18.73\n",
      "Average over all episodes so far: 5.895890410958904\n",
      "epsilon: 0.01\n",
      "**** Episode  740 **** \n",
      "Recent average reward: 18.2\n",
      "Reward over last 100: 18.71\n",
      "Average over all episodes so far: 6.062162162162162\n",
      "epsilon: 0.01\n",
      "**** Episode  750 **** \n",
      "Recent average reward: 18.8\n",
      "Reward over last 100: 18.68\n",
      "Average over all episodes so far: 6.232\n",
      "epsilon: 0.01\n",
      "**** Episode  760 **** \n",
      "Recent average reward: 19.9\n",
      "Reward over last 100: 18.75\n",
      "Average over all episodes so far: 6.411842105263158\n",
      "epsilon: 0.01\n",
      "**** Episode  770 **** \n",
      "Recent average reward: 19.1\n",
      "Reward over last 100: 18.77\n",
      "Average over all episodes so far: 6.576623376623377\n",
      "epsilon: 0.01\n",
      "**** Episode  780 **** \n",
      "Recent average reward: 19.6\n",
      "Reward over last 100: 18.83\n",
      "Average over all episodes so far: 6.743589743589744\n",
      "epsilon: 0.01\n",
      "**** Episode  790 **** \n",
      "Recent average reward: 18.9\n",
      "Reward over last 100: 18.76\n",
      "Average over all episodes so far: 6.89746835443038\n",
      "epsilon: 0.01\n",
      "**** Episode  800 **** \n",
      "Recent average reward: 18.9\n",
      "Reward over last 100: 18.72\n",
      "Average over all episodes so far: 7.0475\n",
      "epsilon: 0.01\n",
      "**** Episode  810 **** \n",
      "Recent average reward: 19.4\n",
      "Reward over last 100: 19.01\n",
      "Average over all episodes so far: 7.2\n",
      "epsilon: 0.01\n",
      "**** Episode  820 **** \n",
      "Recent average reward: 19.0\n",
      "Reward over last 100: 19.02\n",
      "Average over all episodes so far: 7.34390243902439\n",
      "epsilon: 0.01\n",
      "**** Episode  830 **** \n",
      "Recent average reward: 19.6\n",
      "Reward over last 100: 19.14\n",
      "Average over all episodes so far: 7.491566265060241\n",
      "epsilon: 0.01\n",
      "**** Episode  840 **** \n",
      "Recent average reward: 20.4\n",
      "Reward over last 100: 19.36\n",
      "Average over all episodes so far: 7.645238095238096\n",
      "epsilon: 0.01\n",
      "**** Episode  850 **** \n",
      "Recent average reward: 19.8\n",
      "Reward over last 100: 19.46\n",
      "Average over all episodes so far: 7.788235294117647\n",
      "epsilon: 0.01\n",
      "**** Episode  860 **** \n",
      "Recent average reward: 19.7\n",
      "Reward over last 100: 19.44\n",
      "Average over all episodes so far: 7.926744186046512\n",
      "epsilon: 0.01\n",
      "**** Episode  870 **** \n",
      "Recent average reward: 19.7\n",
      "Reward over last 100: 19.5\n",
      "Average over all episodes so far: 8.062068965517241\n",
      "epsilon: 0.01\n",
      "**** Episode  880 **** \n",
      "Recent average reward: 20.5\n",
      "Reward over last 100: 19.59\n",
      "Average over all episodes so far: 8.20340909090909\n",
      "epsilon: 0.01\n",
      "**** Episode  890 **** \n",
      "Recent average reward: 20.6\n",
      "Reward over last 100: 19.76\n",
      "Average over all episodes so far: 8.342696629213483\n",
      "epsilon: 0.01\n",
      "**** Episode  900 **** \n",
      "Recent average reward: 19.4\n",
      "Reward over last 100: 19.81\n",
      "Average over all episodes so far: 8.465555555555556\n",
      "epsilon: 0.01\n",
      "**** Episode  910 **** \n",
      "Recent average reward: 19.2\n",
      "Reward over last 100: 19.79\n",
      "Average over all episodes so far: 8.583516483516483\n",
      "epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "steps_total=np.full([num_episodes],-999,dtype=np.int32)\n",
    "reward_total=np.full([num_episodes],-999,dtype=np.int32)\n",
    "\n",
    "frames_total=0\n",
    "\n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    #for step in range(100):\n",
    "    step=0\n",
    "    reward_total[i_episode]=0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        step+=1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon=calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action=env.action_space.sample()\n",
    "        action=qnet_agent.select_action(state,epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        memory.push(state, action, new_state,\n",
    "                     reward, done)\n",
    "        \n",
    "        reward_total[i_episode]+=reward\n",
    "        \n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state=new_state\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            steps_total[i_episode]=step\n",
    "            \n",
    "            if i_episode>100:\n",
    "                mean_reward_100 = np.sum(reward_total[i_episode-100:i_episode])/100\n",
    "                \n",
    "            \n",
    "                if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                    print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                    solved_after = i_episode\n",
    "                    solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0 and i_episode>1):\n",
    "                \n",
    "                plot_results()\n",
    "                \n",
    "                print(\"**** Episode  {} **** \".format(i_episode))\n",
    "                recent_avg_reward=np.average(reward_total[i_episode-report_interval:i_episode])\n",
    "                print(\"Recent average reward: {}\".format(recent_avg_reward))\n",
    "                if i_episode>100:\n",
    "                    print(\"Reward over last 100: {}\".format(mean_reward_100))\n",
    "                full_avg_so_far=np.average(reward_total[:i_episode])\n",
    "                print(\"Average over all episodes so far: {}\".format(full_avg_so_far))\n",
    "                print(\"epsilon: {}\".format(epsilon))\n",
    "            \n",
    "                #print(\"Episode {} finished after: {}\".format(i_episode,step))\n",
    "            break\n",
    "            \n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(2,figsize=[12,5])\n",
    "plt.title(\"Score at end of episode\")\n",
    "plt.plot(reward_total[:i_episode])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of steps: {}\". format(np.average(steps_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of steps: {}\". format(np.average(steps_total)))\n",
    "print(\"Average number of steps in last 100 episodes: {}\". format(np.average(steps_total[-100:])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(1,figsize=[12,5])\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(rewards_total)), rewards_total,alpha=0.6, color='green')\n",
    "#plt.plot(rewards_total)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2,figsize=[12,5])\n",
    "plt.title(\"Steps to finish episode\")\n",
    "plt.plot(:i_episode)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
